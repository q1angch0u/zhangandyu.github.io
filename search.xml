<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[关于工作和成长的建议（节选）]]></title>
    <url>%2F2019%2F02%2F20%2F%E5%85%B3%E4%BA%8E%E4%BA%BA%E7%94%9F%E5%92%8C%E5%B7%A5%E4%BD%9C%E7%9A%84%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[面对批评 面对批评，得体的第一反应是“不急于解释， 不反唇相讥”。 很多批评的发生时是因为误会。所以，首先建立情感层面的信任，其次澄清事实，然后才是消除误会。这三步能解决大部分因批评带来的关系问题。 被批评时，思考的重点应该是“我应该从哪里入手解决问题”，而不是“对不对”。 对“评论”不理不睬，对“批评”高度重视。 如果对批评有不同意见或者不明之处，直接问当事人的完整意见。别回避。 关于单身生活 单身过得不愉快，有伴侣之后也不会愉快。 单身且愉快，一个重要前提是有较高收入且未来可期。 单身时最值得花时间去做的是投资一切长本事长能耐的事儿：学习、进修、放纵好奇心。因为有伴侣以后很难再有大块时间可以自由支配。 拥有联系非常紧密的至交好友，且能够共同成长，两个条件缺一不可。 不够紧密则对彼此生活缺少实际支持，不能同步成长则无法长期维持关系。 建立自己的生活秩序， 但要有弹性。 弹性是指随时可以接纳一个因伴侣而带来的新秩序。很多人被动长期单身的原因是因为沉浸在自己的世界里、过度缺乏弹性以至于无法接纳别人。 积极参加有意思的社交活动，认识陌生人，多交朋友。 选择偏保守的理财方式，给自己买足额的大病保险，小心对待自己的钱。 和婚姻生活质量较高的人交朋友，人在面临重大选择时需要榜样和示范。 不要上来就用长期关系的标准要求对方，关系是递进的：可见、可约、可睡、可长期交往，然后才是可固定长期关系。在不同阶段，标准不一样。 主动点。对一切事。 关于个人形象管理 对普通人来说，“好看”的意思其实等同于“体面”。做到体面，完全是任何人能力范围内的事情。并非苛求。 清洁是最低标准。对于现代化城市工作生活的职场人来说，清洁的定义就是每天洗澡每天换衣服搽皮鞋。 你看起来像什么地位，你就是什么地位。 把衣服的数量最小化，然后把单价提高到力所能及范围内的最高。 尽快找出自己的“基本形象”：任何人都会有一个最佳穿衣模式，固定下来。 健身可以帮助你更好的认知自己的身体，再贵的衣服也纠正不了驼背弯腰的仪态。 不存在减不掉的体重，只是有没有纪律的问题。 一个人从里到外让人有“看起来好干净”的感觉，是一种极高的形象水平。这一条对老头儿都适用。 那些质疑个人形象管理重要性的人，从来都不会知道因为糟糕的形象而损失了什么。 关于重大选择 尽自己最大努力做到有钱、好看、有本事、受欢迎。手里的牌多一些，做选择的主动性就高一些。 选择大于能力。有人管这个叫“命运”。其实是一种长期被忽略的能力：关键时刻快速做出最优选择的能力。 最优选择就是对个人远期价值最大化的选择。过于关注当下利益往往是人生的大坑。那些抱着要给未来的孩子找个好爸爸的女生，在婚姻生活中往往过得比寻找最佳男朋友的女生要好，就是这个原理。 成大事者不纠结。一旦决定，全力以赴。 掌握“概率权”。 两张牌，一张掀起来保证给你一百万美金，另一张有百分之五十的概率有1个亿美金，或者为零。想清楚，选哪张？怎么选？ 可以征求别人的意见。但是这个“别人”是特指那些你由衷佩服和学习的对象。 无论别人给你什么意见，都要记住决定是你自己的。不要依赖别人，更不要把后果归罪于外。 远离安全的舒适区。 远离颠倒梦想，乐于动手做具体事，少想，多干。 做选择时不要只看自己，要看如果做了一个决定之后，你会跟什么样的人混在一起。事业选择这一点尤为重要。 关于人际关系 良性的人际关系只有一种， 叫做独立自主、强强联合。从来就没有抱团取暖这回事。 做到对别人有价值， 是建立良性人际关系的前提。 哪怕是暂时只创造了微小的价值，也是价值。 所有关系中最多正向循环最少事后负担的， 是交易关系。市场最残酷但也最善意。 所有关系中最危险最有破坏力的， 是纯感情关系。 所以， 一段关系想要良性发展，要有能力从纯感情喂养，发展成“复合材料”： 比如就婚姻而言，激情不长久，但是双方可以成为共同成长的伙伴，或者某个具体目标的合作者。甚至就亲子关系而言，也可以发展出协同学习的伙伴关系。 不成为别人的负担。 这包活了不成为别人的心理和时间负担，接受别人对自己的不接受。 对自己负责。这包括了对自己的选择和决定负责，承受由之而来的任何后果。 定期梳理和剖析自己的原生家庭和亲密关系。 适度学习一点帮助自我认知的方法，清楚认识并且正面接受自己在人际关系中的短板和问题。大量的人际关系能力缺陷是在原生家庭中已经形成，无需自卑或者自责，因为正面接受自己是战胜这一问题的开始。而且大部分人的大部分心理障碍是可以被消除的。 第一反应是选择信任别人， 但是同时保持独立思考的能力。 人际关系不仅有交互频率这一个维度，还有交互深度。 熟人未必是知己。 远离巨婴、远离不具有建设性能力的人。 人际关系是一个人真实自我的外在镜像。 关于命中贵人 贵人是那些在关键时刻给出关键点拨的人。 他们有能力呈现世界的本来面目。路原来就在那里，但是没有他们的指路你就看不见。 他们之所有原因帮你是因为他们的修行，而不是因为我的能耐和好处。 对他们最好的回报是努力成为和他们一样的人。 贵人不是等到的，是寻到的和求到的。前提是对他们而言至少你不是减分项。 以求道之心与人交往。 举手之劳、锦上添花，才是良性关系。不要求人雪中送炭。跟世态炎凉没关系，有则感谢珍惜，无则检讨自己。 务必定期让人看到帮助你的结果，分享喜悦和成果。 逢年过节快递两盒点心，送一辈子，也不算有礼。若想要感谢对方，花心思观察，送出终身难忘的礼物。 不要黏着对方，不要成为别人的负担。要让对方感觉掌握关系的主动权。人家帮过你，但最怕被要求帮你一辈子。 有机会帮别人的时候，姿态放低再放低，尽量不让对方有心理压力。帮完之后，对方不提，自己不提。 也许终有一天你会超过那些你生命中曾经那么重要的人，往前看，别害怕。 既往不恋。 关于压力管理 压力是公平的。 真正在入世过活的人，没有人能够置身事外。坚信这一点，不易起怨懑(men)之心。 职场压力和生活压力无法互相消解。因此不要把压力释放错了地方。 做一个乐观的悲观主义者。因上努力，果上随缘。 充分想象最坏的结果，如果认为是无法承受的后果，一定源自自身的贪婪，果断踩刹车。君子不立危墙之下。 压力无法被替代，注意力可以被转移。到难以承受之际，刷一小时消消乐，剧烈运动两小时，都可以回血。问题依然还在，但不妨缓口气再来。 有无话不说且旗鼓相当的朋友。说出来可能是最有效的减压之道，并非有人可以安慰你，而是因为站在别人的视角重新看一遍问题会简单很多。 做不到上一条，就试着把压力和问题写下来，只给自己看。然后会发现其实没有那么复杂。 永远用最直接的方式面对压力源。拖延和迂回只会让压力变大。 永远不做任何不能让别人知道的决定和交易， 无论好处有多大。 把压力想象成一个具体的形象，比如一只怪兽，每处理一步，就在脑海里给丫一拳。 关于工作习惯 自己最受益的工作习惯是做笔记。 最佩服的是会面后最先共享笔记的人。 当日事当日毕能提升工作中的幸福感。 有关人的问题，都不可拖延，不要心存侥幸，认为可以避免直面冲突或对方可以心领神会。越晚着手，问题会恶化的越严重。难听的话必须当面说、尽早说、直接说。 个人行动养成彩排的习惯。打腹稿、做预案。认真准备，就能发现达到一个目标的N种途径，行动中灵活不纠结。 团队作业养成复盘的习惯。不追究具体人责任，着眼于我们学到了什么。 每天琢磨核心数据。问出好问题。 与人面对面交流时不刷手机。 开会时敢于并善于终结无实质意义的对话，直切主题。致力于达成行动共识。 准时参加与别人的约会，不迟到。 每天早中晚三次集中时间段处理社交媒体、邮件等方面的信息。不要随刷随到。这是最节省时间的办法。 购置最好的工作装备。 睁大双眼找个好搭档，和聪明人一起工作。 关于自我成长 早晚你会知道，这个世界上没有别人。你所看到的都是，你自己的认知模式创造的镜像。 与此同时你会知道，这个世界上全是别人。所谓反复追寻的“真我”实在是小到不能再小的东西。 同时知道并接受上述两点，自我成长这件事才真正开始。 最难的在于不断建立更高的标准。为了解决这一问题最有效的方法是，不断结识更好的榜样。 在人际关系中做一个能力型“势利眼”， 向上看、向前看，与比自己优秀的人交往。 既往不恋，不回头，不怀旧，不惋惜。 以真实的、认识的人为榜样，而不是“传说”中的。因为真人会在每天的交往中给你真实的压力。 慎独。守心如镜。 每天独处时问问自己，今天有什么是比前一天做的好的。哪怕是对快递员更认真的说谢谢。 不做负面表达，负面包括讽刺、抱怨、指责、争论、批评、牢骚、大话、评价议论。凡事从建设性出发。 认真记录并揣摩与别人的交往。 尽可能扩大自己的阅读面。 文科生多阅读点科学著作，理科生不妨研究点文学艺术。阅读是为了理解人。 带徒弟是逼迫个人成长的绝招。 不相信“适当的年龄”这件事。除了生孩子，想做任何事情都不会被年龄限制。 关于机会和陷阱 世界上最大的陷阱，叫做“机会型陷阱”。 评估一下这件事对多少人有利？越是多赢的局面，越可能是个机会，反之如果只有你占尽便宜，那肯定不是机会。 想想这件事是不是很容易做到？有两种事。一种是机灵事：开头就炸，但是越来越没劲。另一种是苦逼事：开头特难，但是越干门槛越高。前者是陷阱，后者是机会。结硬寨、打呆仗是不变的真理，很容易实现的事情，是技巧，不是机会。 看忽悠你干这个事的人跟这事的关系。如果对他在精力上只是业务兼职，在财富上只是锦上添花，而你要付出全部精力，搭上全部身家。那么，要警惕是个陷阱。 周边人赞成与否不重要，但是否出手帮你很重要。一旦开始干，帮你的人越来越多的事，机会。反之，陷阱。 因资源而启动的事，容易翻转为陷阱。因顺势而发生的事，容易找到机会。 推动社会新分工的事，也就是越来越多的人靠这事吃饭养活一家老小的事，机会。在各种裂缝中套利，除了你谁都没好处的事，陷阱。 做成之后，容易被巨无霸们摘果子的事，陷阱。做成之后，自成体系的事，机会。换句话说，南瓜不会结在树上。有命长出来，没命hold住。 就算做成了，也有后遗症的事，陷阱。就算做不成，也长本事长江湖地位的事，机会。 摊在桌上打明牌也能干的事儿，机会。必须遮遮掩掩唯恐别人知道的事儿，陷阱。 你孩子长大了为你骄傲的事，机会。 反之，陷阱。]]></content>
      <categories>
        <category>个人系统</category>
        <category>逻辑</category>
      </categories>
      <tags>
        <tag>转载</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2019%2F02%2F20%2Flinux%E5%AD%A6%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[linux学习 linux本身只操作系统的内核。 ctrl + d: 键盘输入结束或退出终端ctrl + s: 暂停当前程序， 暂停后按下任意键恢复运营ctrl + z: 将当前程序放到后台运行， 恢复到前台为命令 fgctrl + a: 将光标移到输入行头， 相当于home键ctrl + e: 将光标移到输入行未，相当于end键ctrk + k: 删除从光标所在位置到行末alt + backspace : 向前删除一个单词shift + paup : 将终端显示向上滚动]]></content>
  </entry>
  <entry>
    <title><![CDATA[《刻意学习》读书笔记]]></title>
    <url>%2F2019%2F02%2F17%2F%E3%80%8A%E5%88%BB%E6%84%8F%E5%AD%A6%E4%B9%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%20%2F</url>
    <content type="text"><![CDATA[你必须先遵守才能谈自由与灵活。 每个人都是自己生活轨迹的制定者，每个人成长的主旋律都在自己手中，要做的事情必须独立承担，任何评价都无法代替我们要面对的事情。 1-300天厘清当前的处境。感受到“我要做的”和“我能做的”之间的差距 300-600天缺什么补什么，有困难解决困难，直接硬上， 用时间换空间，用耐心换进步。抵御“放弃”的诱惑，有时候你会感到无趣，但拼的就是坚持。 加载任务的标准： 在原有任务能保持每天做到位并有所进步的基础上，仍然有足够多的心智和力量完成更多的任务。 对于一本书， 你能消化多少，不是由采取的方法决定，而是由你自身“消化系统”的能力决定，而这个“消化系统”， 就是学以致用的能力。 习惯就是每天都做，如何你哪一天不做这件事了， 就不再是习惯了。 对于写作的一个观点： 我写我心，我写我情，我写我世界。 对我而言，写作是用来整理自己的世界，当你把自己梳理好以后，你写的文章也会变得清晰明了，容易理解。当我们关注自己内心的时候，其实更容易处理好与外界的关系。 而我们的写作，也就顺理成章地编程了一种对我们内心世界的推演。于是你会得到一些结论，而这些结论正是你身体力行、复盘总结、升华提炼后的结果。你把文字写出来的时候，也许你会想，某本书中早就写了这些。你写的意义又在哪里？意义在于，你自己想明白的，就是你的。书上的只是书上的，终究不是你的。自己想明白的，是从你的体系中萌芽生长出来的；而从书中看到的，非常容易停留在做个笔记画个线，自以为懂了的层面。 只有从原理级别、行动层面、复盘角度综合学习并且全面吸收而掌握的知识、技能，才能真正成为我们所需要的武器。 现在我们每个人的环境都承载了所有前辈的过去，包括出生的家庭环境、所处的城市，都是前辈逐渐积攒的结果。这是我们无法选择的“默认参数”，我们一出生，身上就带了这些参数，需要背着它不断去优化，走完此生。 对现状感到不满，对未来感到迷茫，压力很大。而且我们往往会把这些现状当成原因，来指导我们的行动。我迷茫，所以我要如何。 但在某种程度上，更多的是结果而不是原因。我们只是在还过去欠下的债而已。过去的某个时刻我们没有全力投入，没有付出更多的心思，没有做更多的事情，尝试更多的可能性。于是，我们就按照默认的配置，来到当下。 当我们一无所有的时候，能选择的也就是每天都行动了。 这是我们唯一能够把握的， 因为唯一相对公平的筹码就是每个人的时间，而我们能做的就是把自己有限的时间投入到某个方面，不断积累，提升形成优势，然后去市场上做交换。我们要把时间持续的、长期稳定地专注在一个点上，并且在这一点上形成自己的优势资源。这其实是一种通过持续行动为自己的成长“复利”的行为。 核心技能是没发通过强及时反馈来构建的，也就是你要做好坐冷板凳的准备。而且你也要相信，在这个过程中是不可能天天有强及时反馈的。如何你能守住这种寂寞，就能拨云见日；受不住的话，就是低水平重复建设了。 反馈从来都是漫长的，成长从来都不是一蹴而就的。你不要想太多，也不要想太远，只要今天写明天写，不要停，慢慢就可以做到了。 他最想要的是什么，你能给吗？你最想要的是什么，她能给吗？ 把这两个问题想明白，然后就去追吧。]]></content>
      <categories>
        <category>个人系统</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[业务分析——工作报表、临时需求、异常数据排查]]></title>
    <url>%2F2019%2F02%2F17%2F%E6%8A%A5%E8%A1%A8-%E4%B8%B4%E6%97%B6%E9%9C%80%E6%B1%82-%E5%BC%82%E5%B8%B8%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[一、 日报、周报、月报 日报 日报每天都要看。编写数据脚本，实现日报半自动化。 为什么每天要看日报： 了解公司业务现状 当公司同事询问数据时，能够做到对基础的业务数据脱口而出,一定要对公司的核心指标数据熟记于心 培养个人数据敏感性 当每天看数据时，都能够看到数据的波动变化。比如每日客户端活跃用户数都是在550-600万用户之间，突然一天增长到605万，能够做到有所怀疑，并进一步寻找原因。这就是数据敏感性。 提供业务发展建议 通过日报的数据观察，能够知道当产品运营做出哪些优化时， 数据会出现涨跌情况，给业务运营提供发展建议。 周报 看短期趋势。 周报作用： 新版本发布往往一到两周，通过一周的时间长短， 可以分析APP新版本的改版效果。 一周的数据更加稳定， 有说服力。 月报 给出建议。 月报内容： 通过月报对公司业务进行梳理，月报一定要有给业务运营的建议，不能只是罗列数据。 当通过数据变化发现某些活动效果好时， 一定要把分析原因放到月报里去展现，让所有管理层知道，从而推送业务的发展。 二、临时数据需求 管理层需求： 优先级最高 了解需求背景，思考为何要这个数据， 通过这个数据可以进行哪些决策 一定要进行核对，不能出错，只能给一遍。 必须时候找上级进行询问 业务需求： 询问为何要这个数据 建立该业务类的分析框架 只给业务方最核心的需求，其他延伸需求让自己去取数 坚决不做提数机 例如： 优惠券的使用情况如何，如何优化？建立分析框架： 优惠券的下发人数 优惠券的点击人数 优惠券的使用人数 优惠券的使用金额 用户消费优惠券的频次 优惠券的消费时段 三、数据异常排查清楚以下三点： 业务理解 指标口径 当前数据产出过程 数据异常原因分析： 数据有问题 将时间轴拉长，看数据是近期异常还是历史异常，对比近三个月数据。 查看和该指标关联的其他核心指标是否也异常，如果异常，也要一并查看。 核查埋点是否有问题， 数据是否存在多发情况。 业务口径是否有问题， 取的数据是不是真正需要的数据。 写的sql逻辑是否有误， 或者android 和 ios 数据没有相加。 业务发生了变化 同口径下，同比环比数据是否异常， 在长时间轴的条件下进行对比 进行细拆， 看到底是哪个指标的数据出现了异常， 将该指标和相关的指标一起进行对比 与负责此指标的负责人进行沟通， 询问他们近期是否做了推广活动。 产品最近是否发布了新版本，或者某个功能改版存在缺陷问题。 其他因素影响 假期效应： 开学季， 暑期， 四大节日，当地节日 热点事件：常规热点如运动会，世界杯，其他热点事件 活动影响： 双11， 618， 支付宝送红包 政策影响： 金融监管 底层系统故障： 服务器迁移导致数据丢失 存在外部刷量作弊情况 统计口径： 业务逻辑进行了更改， 之前某业务有统计， 但现在不统计了。 数据计算方式改变 要对异常排查进行闭环 持续跟踪后期数据是否再次异常比如7.0版本数据存在多发现象， 但7.1版本开发说已更改， 当7.1版本发布之后， 要及时去查看数据是否再次异常。 对数据异常一定要记录， 对异常口径要留文档。 邮件化， 当数据异常排查原因查明， 并且确认更改之后， 发邮件给相关方， 描述数据异常的影响范围和主要结论。 作为一名数据分析师， 一定要经常去看这三种报表，培养自己的数据敏感度，了解业务的各种核心指标。]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>分析流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[运营笔记]]></title>
    <url>%2F2019%2F02%2F17%2F%E8%BF%90%E8%90%A5%2F</url>
    <content type="text"></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>运营</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用户路径分析]]></title>
    <url>%2F2019%2F01%2F22%2F%E8%B7%AF%E5%BE%84%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[玩转用户行为路径分析，3种方法就够了 http://www.sylan215.com/upload-to-qiniu.html]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>路径分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何整理大脑思绪]]></title>
    <url>%2F2019%2F01%2F20%2F%E3%80%8A%E9%9B%B6%E7%A7%92%E6%80%9D%E8%80%83%E3%80%8B-%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[自己时常有这种感觉，总觉得自己的大脑反应很迟钝，思绪比较混乱，看一个问题想法很模糊，有时心情急躁却找不到问题的解决方法。工作中和别人沟通时，有时词不达意，表达不够清晰，导致工作中出现很多问题。自己也明白自己需要更加深入的思考，但总是没有找到好的办法。 最近在看《零秒思考》这本书，书中作者提供了一个解决此类问题的方法：时刻写下自己的想法。 我们每天会接受不同的信息，在脑海中会产生大量的想法与感觉。很多时候，这些想法还没有转换成语言之前，便在含混朦胧、内心纠结的状态下消失了。虽然想法会暂时忘记，但是那种纠结的感觉并没有消解，会导致自己的心情消极的，产生精神上的压力，进而让大脑变得迟钝。 时刻记录自己想法的好处是，写下来能够将纠结于心的情绪发泄出来，在写的过程中对大脑的思绪进行整理，也能更准确的表达自己的想法。这个方法的关键是：在1分钟的时间限制内，迅速写出大量自己的原始感受。 具体的做法是这样： 写标题： 写出有关大脑中思绪的一个疑问句。 写内容：写4-6行文字来写下自己的原始想法。 字数限制：每行文字字数在10-15字之间。 时间限制：在2分钟之内完成。 数量限制：每天写10条。 每个做法对应的原理是这样： 标题，用疑问句可以让自己更有写下去的冲动。 内容，写4-6行文字，能够将自己大脑中浮现的想法基本都写下来，而不至于重复。 字数，10-15字，让自己不至于写的太短而不能充分表达想法，也不会字数太长在规定时间内写不完。 时间，限制时间，避免大脑受环境和周围状况的影响。 数量，每天写10条，不会过多占用时间，更容易坚持。 书中作者要求在A4纸上写，个人感觉不是很方便，自己目前习惯于通过手机自带的【闪念胶囊】软件来进行记录。记录想法的方法与写反思日记有些相似，两者都要求把自己的想法写下来，不同之处在于，反思日记是对自己这一天做的事情进行反思记录，而记笔记是对你时刻产生的想法进行记录。 目前自己按照这个方法写了10天左右，感觉自己一个很大的变化就是下班回家走在路上，可以通过自问自答的方式来对一个问题进行深入的思考。虽然有时也想着想着就跑偏了，但是自己对于思考这件事情，不再有抵触的情绪。 书中作者还提供了通过回顾记录来挖掘价值的方法： 回顾自己的笔记， 然后再把笔记的内容当做标题， 每个标题再写4-6行。 这样自己对这类问题的思考会更加的深入。 多角度的写一个标题。 让自己对带有个人情感的内容作出更冷静的判断， 能够站在别人的角度去看问题。 将笔记按照不同领域来进行分类整理。 每三个月回顾一次笔记，了解自己面临的问题，探寻自己的成长轨迹。 最后，想说的是，看到一个方法论，我们常常会怀疑这样的方法真的有用吗，但问题本质是看你选择先相信再看见，还是先看见再相信。 我相信时刻记录想法是一个看似简单却对个人成长大有裨益的方法，所以准备践行100天再看看效果。]]></content>
      <categories>
        <category>个人系统</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>读书笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[反思日记]]></title>
    <url>%2F2019%2F01%2F13%2F%E5%8F%8D%E6%80%9D%E6%97%A5%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[“我应该不会拒绝从头到尾把生活再过一遍，只是希望能够获得唯有作家才有的特权——在‘再版’的生活中修正‘初版’的错误，生活的悲哀之处在于我们总是老得太快而又聪明得太晚，等你不再修正的时候，你也就不再了” ——西塞罗《论老年》 在18年下半年，自己有一种感觉特别的强烈，就是总感觉时间过得飞快，而自己还什么都没有做，一周就完了，甚至自己想不到自己上周主要做了哪些事情。于是，自己想了一个办法，来抵抗这种感觉，就是每天写日记。 开始的时候，自己主要是记录自己这一天都干了什么，写了一个月之后，自己总感觉自己是在记流水账，刚好那时候，在看一本书《好好学习：个人知识精力管理指南》，里面提到了通过写反思日记的方法来掌握知识，自己按照里面的方法写了 100 多天，感觉还是非常有用的。 反思日记主要分为两个部分，反思与日记。反思就是对自己做的事进行思考，对产生结果的原因进行分析。日记则是要每天都要去写，每天都要对自己的生活进行记录。自己现在更加深刻的认识到， 一个人的变化不是突然发生的， 而是发生在每一天做的事情中的。 根据自己的实际情况，我给自己制定了反思模板，每天日记的任务主要是回答自己这7个问题。 今天自己做的不好的事情是什么？ 自己当时是怎么想的， 身体是如何反应的 如果自己再重新来一次， 自己会如何做 自己今天做的很好的事情是什么 自己目前最主要的目标是什么 自己今天任务的完成情况。 自己明天的计划是什么 这7个问题背后的原理是这样的： 问题1-3，是对自己思考方式的反思。我们平时做一件事情，是基于 假设-行动-结果 这样的过程。 而反思，就是通过 观察结果-研究原来假设-反思校正假设 这样的顺序对自己思考的再思考。 问题4， 是为了提高自己的自信心。个人认为自己在生活中不够自信，通过每天记录自己做的事情，来让自己增加自信心。 问题5，提醒自己时刻盯住自己的目标，为了自己的目标而努力。 问题6，对比昨天的计划，监督自己今天任务的完成情况。 问题7，是为明天的事情，做出一个良好的计划。 有时候，翻看自己之前的记录，会发现自己当时会面临这样的问题，回过头来再看也是比较有意思的事情，比如, 翻看自己18年10月22号的日记，发现自己是这样想的： 通过写反思日记，自己发现了一些自己反复会犯的问题。比如： 自己下班一回到房子就什么也不想干，总是在刷微博、看美剧，但是自己在看完之后，自己并没有产生放松的愉悦感，在写反思日记的时候，自己总是懊恼自己为什么这样浪费时间，这实际上是自己的精力管理方面出了问题，认识到这个问题之后，现在自己也在尝试各种方式来恢复自己的精力。 个人认为培养出记反思日记的习惯还是很有必要的，通过记录自己的生活并不断反思， 能够让我更清醒的认识到自己的不足，从而尝试做出改变。 作者在《好好学习-个人知识精力管理指南》这本书中还提到了通过写反思日记来进行对标管理，把一本书中的知识进行每日的反思等方法。大家可以根据书中的内容，指定自己的个人反思模板。]]></content>
      <categories>
        <category>个人系统</category>
        <category>写作</category>
      </categories>
      <tags>
        <tag>周报</tag>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hive 查询进阶]]></title>
    <url>%2F2019%2F01%2F06%2FHive%E8%BF%9B%E9%98%B6%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[Hive 查询进阶笔记 总结一下最近查询用到的Hive查询。 目录： Hive 查询性能优化 求两组数据的并集、交集、差集 Hive 中查询留存率 Hive 中的窗口函数 Hive查询性能优化 什么是数据倾斜当我们在Hive上进行查询时，因为数据的分散度不够， 导致大量数据集中在一台或者几台服务器上， 导致数据的计算速度远远低于平均计算速度， 计算过程特别耗时。 数据倾斜的表现任务进度长时间维持在99%，查看任务监控页面，发现只有少量子任务未完成。 如何避免数据倾斜 sql优化 业务逻辑优化 当数据量特别大时，用 group by 代替 count(distinct) 123456789101112131415161718192021# 求客户端每日的去重uvwith a1 as ( select hit_date, user_account from android_data where hit_date between &apos;2018-10-01&apos; and&apos;2018-10-03&apos; group by hit_date, user_account)select hit_date, count(user_account) as uv from a1 group by hit_dateorder by hit_date join 优化 将条目少的表/子查询放在 Join 操作符的左边。原因是在 Join 操作的 Reduce 阶段，位于 Join 操作符左边的表的内容会被加载进内存，将条目少的表放在左边，可以有效减少发生错误的几率。 1234567# a, b表中， b表最小select a.price_close, b.price_closefrom b join a on b.ymd = a.ymd AND b.symbol = a.symbolwhere a.symbol = &apos;apple&apos; 避免 union all 子查询中使用 group by 【替换 count(distinct) 除外】、count(distinct)、max、min等。 12345678910111213141516171819202122232425262728use computer_view;with a1 as ( select user_account, hit_date from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-13&apos; and nbtn_name like &quot;%支付宝%&quot; union all select user_account, hit_date from client_ios_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-13&apos; and nbtn_name like &quot;%支付宝%&quot;)select hit_date, count(user_account) as pvfrom a1group by hit_date 避免不同数据类型进行关联 使用CAST函数对数据类型进行转换，语法为cast(value AS TYPE)123456789select a.price_close, b.price_closefrom a join b on a.user_id = cast(b.user_id as string)where hit_date between &apos;2018-11-01&apos; and &apos;2018-11-02&apos; and a.symbol = &apos;apple&apos; 无效ID在关联时的数据倾斜问题 把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的 reduce 上 ,解决数据倾斜问题。需要用到Case When … Else…End语法 写法1：123456789101112131415Select *From a Join bOn a.user_id is not nullAnd a.user_id = b.user_idUnion allSelect * from awhere a.user_id is null 写法2：1234567891011Select *From a left out Join bOn Case when a.user_id is null then concat(‘dp_hive’,rand() ) else a.user_id = b.user_id end; 在查询中， 避免使用 select *, 使用条件限制取需要的列。 在使用 Join 进行外关联时， 将副表的过滤条件写在 where 后面，会先全表关联， 再进行过滤， 这样会耗费资源。 123456SELECT a.price_close, b.price_closeFROM b JOIN a ON b.ymd = a.ymd AND b.symbol = a.symbolWHERE s.symbol = &apos;APPLE&apos; 正确的写法是将 where 条件卸载 on 后面 1234SELECT a.price_close, b.price_closeFROM b JOIN a ON ( b.ymd = a.ymd AND b.symbol = a.symbol and s.symbol = &apos;APPLE&apos;) 求两组数据的交集， 并集， 差集 并集 union 与 union all union, 结果包含所有行， 并删除重复行unoin all, 结果包含所有行， 但不删除重复行 写法1：1234567891011121314151617181920212223use computer_view;with a1 as ( select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-02&apos; and nbtn_name like &quot;%支付宝%&quot; union select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-02&apos; and nbtn_name like &quot;%手淘%&quot;)select count(user_account) as pvfrom a1 点击支付宝或者手淘活动的人数总共有 435499 人 写法2：1234567891011121314151617181920212223use computer_view;with a1 as ( select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-02&apos; and nbtn_name like &quot;%支付宝%&quot; union all select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-02&apos; and nbtn_name like &quot;%手淘%&quot;)select count(user_account) as pvfrom a1 点击支付宝或者手淘活动的次数为 665935 交集-intersect函数 写法1：1234567891011121314151617181920212223use computer_view;with a1 as ( select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-02&apos; and nbtn_name like &quot;%支付宝%&quot; intersect select user_account from client_ios_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-02&apos; and nbtn_name like &quot;%手淘%&quot;)select count(user_account) as pvfrom a1 点击支付宝又点击手淘活动的人数为 66174 差集-except 函数 与 join写法 写法1：1234567891011121314151617181920212223use computer_view;with a1 as ( select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-25&apos; and nbtn_name like &quot;%支付宝%&quot; except select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-25&apos; and nbtn_name like &quot;%手淘%&quot;)select count(user_account) as pvfrom a1 写法2：12345678910111213141516171819202122232425use computer_view;with a1 as ( select user_account from client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-25&apos; and nbtn_name like &quot;%支付宝%&quot;),a2 as ( select user_account from client_android_log_view where hit_date between &apos;2018-12-20&apos; and &apos;2018-12-25&apos; and nbtn_name like &quot;%支付宝%&quot;)select count(distinct a1.user_account) as pvfrom a1 left outer join a2 on a1.user_account = a2.user_account and a2.user_account is null 只参加支付宝活动， 没有参加手淘活动的人数为 369325在求差集时， 需要注意前后顺序， 否则会出现逻辑错误可以发现， 差集 + 交集 =并集， 369325 + 66174 = 435499 HIVE中查询留存率 求11月10-15号每天的1、3、7日留存率 方法1. 统计每天的uv 统计上一天与本天uv的交集， 需要每一天做交集 算出留存率 方法2： 统计每天的uv 使用date_add 函数， 一次性求出10-15号每一天的次1、3、7日留存 算出留存率 123456789101112# 统计10-15号每天uvSELECT hit_date, count(distinct user_account) as uvFROM computer_view.client_android_log_viewWHERE hit_date between &apos;2018-11-10&apos; and &apos;2018-11-15&apos;group BY hit_dateorder BY hit_date 123456789101112131415161718192021222324252627282930# 统计10-15号每天的次日留存数， 统计次3、7日留存只需将1换为3、7with a1 as ( select user_account, hit_date from computer_view.client_android_log_view where hit_date between &apos;2018-11-10&apos; and &apos;2018-11-15&apos;),a2 as ( select user_account, hit_date from computer_view.client_android_log_view where hit_date between &apos;2018-11-10&apos; and &apos;2018-11-25&apos;)select a1.hit_date, count(distinct a1.user_account) as uvfrom a1 join a2 on a1.user_account = a2.user_accountWHERE a2.hit_date = date_add(a1.hit_date, 1) group by a1.hit_dateorder BY a1.hit_date HIVE中的窗口函数 over 函数 语法： over(partition by ….)作用： 与聚合函数sum(), count(), avg()等结合使用， 实现分组聚合的功能 123456789# 根据日期 和 mac_id 进行分组求每组的数量和， 并按日期排序select hit_date, mac_id, mac_color, day_num, sum(day_num) over(partition by hit_date, mac_id order by hit_date) as sum_numfrom test.datas hit_date mac_id mac_color day_num sum_num 20171011 1292 金色 11 89 20171011 1292 黑色 19 89 20171011 1292 粉金 58 89 20171011 1292 金色 1 89 20171011 2013 金色 9 22 20171011 2013 金色 3 22 20171012 1292 金色 5 18 20171012 1292 粉金 1 18 20171012 2013 粉金 1 7 20171012 2013 金色 6 7 20171013 1292 黑色 1 1 20171013 2013 粉金 2 2 123456789101112# group by 语句select hit_date, mac_id, sum(day_num) from test.datagroup by hit_date, mac_idorder by hit_date day_id mac_id sum_num 20171011 124609 1 20171011 20130 22 20171011 12922 89 20171012 12922 18 20171012 20130 7 20171013 12922 1 20171013 20130 2 over(partition by) 与 group by 的区别grou by 字段只能显示与分组聚合相关的字段， 而 over(partition by)可以显示所有字段 LAG 和 LEAD 函数 语法： LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值;LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值 123456789101112131415161718192021# 计算11月1-10号， 不同日期同一用户登陆客户端 pv 量对比with a1 as (select user_account, count(user_account) as pv, hit_datefrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-11-01&apos; and&apos;2018-11-10&apos;group by user_account, hit_date)select user_account, a1.hit_date, a1.pv, lag(a1.pv, 1) over (partition by user_account order by user_account, a1.hit_date) as pv1, lead(a1.pv, 1) over(partition by user_account order by user_account, a1.hit_date) as pv2from a1limit 100 first_value() 和 last_value() 函数 = 语法:first_value() ：比较每个用户浏览次数与第一天浏览次数进行比较，查询返回当前浏览次数以及第一天浏览次数last_value() ： 比较每个用户浏览次数与最新一天浏览次数进行比较，查询返回当前浏览次数以及最新一天浏览次数 12345678910111213141516171819with a1 as (select distinct user_account, count(user_account) as pv, hit_datefrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-11-01&apos; and&apos;2018-11-10&apos;group by user_account, hit_date)select distinct user_account, a1.hit_date, a1.pv, first_value(a1.pv) over (partition by user_account order by user_account, a1.hit_date) as pv1, last_value(a1.pv) over(partition by user_account order by user_account, a1.hit_date) as pv2from a1limit 100 rank、dense_rank、 row_number 排序函数 说明： rank函数， 返回数据项在分组中的排名， 排名相等的会留下空位， 如1、2、2、4dense_rank函数， 返回数据项在分组中的排名， 排名相等的不会留下空位， 如1、2、2、3row_number函数， 返回数据项在分组中的排名， 排名不管数据是否相等， 如1、2、3、4 1234567select a, row_number() over(order by b) row_number, rank() over(order by b) rank, dense_rank() over(order by b) dense_rank from lijie.test_rank a row_number rank dense_rank A 1 1 1 C 2 2 2 D 3 3 3 B 4 3 3 E 5 5 4 F 6 6 5 G 7 7 6 业务问题 求点击【确认充值】按钮的上一步点击的名称 1234567891011121314151617181920212223242526use default;with a as (select user_account, btn_name, lag(btn_name, 1) over (partition by user_account order by create_timestamp) as previous_btn_namefrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-11-01&apos; and&apos;2018-11-01&apos; and btn_name is not nullhaving btn_name like &apos;确认支付&apos;)select previous_btn_name, count(distinct user_account) as cfrom agroup by previous_btn_nameorder by c desclimit 1000 上一步点击的名称我已经知道了， 现在要想 之前通过上一步点击这些条件之后， 再点击【确认支付】按钮的 去重uv 1234567891011121314151617181920212223242526272829use default;with a as (select user_account, btn_name, lag(btn_name, 1) over (partition by user_account order by create_timestamp) as previous_btn_namefrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-11-01&apos; and&apos;2018-11-01&apos; and btn_name is not nullhaving btn_name like &apos;确认支付&apos;)select count(distinct user_account) as cfrom awhere (previous_btn_name like &quot;%10元%&quot; or previous_btn_name like &quot;%30元%&quot; or previous_btn_name like &quot;%50元%&quot; or previous_btn_name like &quot;%10元%&quot; or previous_btn_name like &quot;%30元%&quot; or previous_btn_name like &quot;%50元%&quot; or previous_btn_name like &quot;%100元%&quot; or previous_btn_name like &quot;%200元%&quot; or previous_btn_name like &quot;%300元%&quot; ) 125752 参考：过往记忆——hive ‘SELECT hit_date, btn_navigation ,SUM(uv), SUM(pv) \FROM ( \select \ btn_navigation, \ hit_date, \ count(distinct user_account) uv, \ count(user_account) pv \from \ computer_view.client_android_log_view \where \ hit_date between “{}” and “{}” \ and \ app_version = “7.1.0” \group by \ btn_navigation, hit_date \order by \ btn_navigation, hit_date \) cb_view \GROUP BY hit_date, btn_navigation]]></content>
      <categories>
        <category>编程语言</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
        <tag>周报</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人情绪控制]]></title>
    <url>%2F2019%2F01%2F06%2F%E8%87%AA%E6%88%91%E6%95%91%E8%B5%8E%2F</url>
    <content type="text"><![CDATA[自我救赎 童年遭受过负面的人，自控力和延迟自我满足的能力较弱。 对生活在不可预测的环境中的孩子来说，未来是不可信赖的，不值得等待的，更不值得放弃眼前就能得到的快乐。 匮乏感让人不愿意接受他人的帮助。 一个在资源充足的环境中长大的人，反而更容易要求或者接受别人的帮助。 但对于匮乏感强烈的孩子来说，接受他人的帮助，就会提醒他们再次想起那种匮乏感， 以及那种匮乏感所带来的羞耻感、面对他人的自卑感、没有力量的虚弱感。 痛楚感能够帮助我们对自我的情绪更有洞察力。在痛楚感发生时，人们为了减轻这种不适感，被迫去不断思考，探寻思考这种痛楚从何而来？自己究竟为什么感到痛苦？痛楚感是我们自我分析、理解他人、思考世界的最重要的原动力。 成长过程中有很多痛楚感的孩子，在长大后成为那些更深刻、更敏锐、更能接受和处理人和生活本质的复杂性的人。 以更开放的视角去看待不幸每天花时间，写下你人生中的痛苦给你带来的积极和消极的影响。]]></content>
      <categories>
        <category>个人系统</category>
        <category>阅读</category>
      </categories>
      <tags>
        <tag>情绪</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[业务分析——建立业务数据指标]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%BB%BA%E7%AB%8B%E4%B8%9A%E5%8A%A1%E6%95%B0%E6%8D%AE%E6%8C%87%E6%A0%87%2F</url>
    <content type="text"><![CDATA[如何建立业务数据指标 数据君 指标体系不重要，重要的是指标之间的关系 假设进入一个企业， 要从0-1去搭建指标体系。 调研 一方面是业务的调研，比如我们现在有哪些业务？这些业务在行业有没有一些标准的指标？ 另一方面使需求调研，比如哪个部门处于我们业务流程的哪个阶段，他们关注什么指标？ 这两点很重要，因为我们后面做的分析都是要通过指标去看业务的发展，而每个业务发展的阶段都是某个或者某些部门一起来完成，这就会埋下一个很深的伏笔，问题出现了，谁来背锅。 找到适合自己企业的方法论比如互联网企业都在用AARRR，整体的拆分逻辑是【获取-活跃-留存-营收-传播】， 这个模型在各个公司都会被改良，因为太笼统不够细化，但它给了我们一种关键行为的思路。 比如有的分为好几个阶段： 认知、注册、行为、互动、交易、售后、客服等等，你需要对每一个类型进行进一步的拆分。 比如： 认知阶段，也就是你和用户第一次见面在哪里？可以分为线上广告，品牌类推广、SEM\SEO、CPS、CPC等等， 线下的灯箱广告、楼宇、公交广告、地铁等，这样去制定每个渠道的关键性考核指标，比如线下可能多是曝光，不懂可以去进行搜索，这种方法可以依次拆分注册、行为、互动等等。 技巧自己要画几个方框，在方法论走通的前提下，每一个业务动作都话一个方框，注明什么动作，再补充这个动作需要哪些指标，到最后一目了然，领导看后肯定喜悦，这就是指标体系与业务之间的商业画布。 落实在方法论和商业画布的基础上，要对应到部门，比如认知阶段，很多都是市场部、品牌部、渠道从部负责，那么这样指标有问题，你就知道找谁背锅了。 这里面需要注意的一点，现在很多部门的指标比较单一，比如渠道部只负责拉点击，不负责注册，那么你就要通过指标的表现和系统性，考虑跨部门背指标，这一点比较难，但还是要提出来，或者用权重的方式去解决，让领导去拍板，一般指标体系老板都要产于，因为他们想要找个方式来量化一切。 最后以上的部分只是让你从业务和企业的角度梳理出来适合自己的衣服，但是指标既然存在就要有量化之后的数据，最难的是数据之间的关系，所以这里面也要指出每个阶段，每个部门更关注的一些指标，也被称为第一关键指标或者北极星指标，这样会让自己更清晰，而不是简单的对指标进行归类，一个是自上而下，一个是自下而上，思路是不一样的。 核心最核心的一点， 指标体系的搭建其实很简单，主要是如何从你的报告体系中反应出来，比如日报应该反应什么，周报应该反应什么，月报应该反应什么？这就很烧脑了。]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>分析流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[面试问题]]></title>
    <url>%2F2018%2F12%2F22%2F%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[面试问题 工作中， 提数数据时，有没有遇到过数据倾斜问题， 什么是数据倾斜， 如何避免数据倾斜问题?]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>面试问题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[业务分析-数据分析师岗位介绍]]></title>
    <url>%2F2018%2F12%2F22%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90-%E5%B2%97%E4%BD%8D%2F</url>
    <content type="text"><![CDATA[1. 数据分析师岗位介绍 什么是数据分析师？ 数据分析师， 就是专门从事数据收集、研究分析，并依据数据来指导业务决策的人员。 岗位分类 其中的数据收集、研究分析、业务指导刚好可以将数据分析的岗位分为三个大的部分： 数据收集——数据研发工程师 主要职能是搭建数据处理的基础设置，让大数据的存储、处理、计算能按要求完成，包括数据仓库搭建、数据存储、计算处理、报表开发等。 数据挖掘、算法工程师 主要是应用机器学习和数据挖掘算法，进行用户行为分析和用户属性挖掘，建立模型，预测、用户画像等为业务场景提供支持。 业务数据分析师 分析业务数据， 发现问题，分析问题，得出分析结论，为决策提供支持，主要支持市场运营部门。 如需详细了解数据分析师的岗位划分，可以参考秦路老师的文章：数据分析的职业规划 业务数据分析师的日常工作有哪些? 业务前期： 建立业务数据指标， 梳理业务数据口径， 确定数据埋点方案， 进行埋点测试，确保数据采集的准确性。 日常工作： 公司的日报、周报、月报数据支持，业务方临时性数据支持， 数据异常原因分析， 业务专题分析。 业务数据分析师的技能要求 数据分析思维和对公司业务的理解程度， 是业务数据分析的主要核心技能。 必备技能有： Excel, Hive/Sql, 统计学，PPT 软技能有： 逻辑思维能力，分析方法论， 数据敏感度， 沟通协调能力 加分技能： Python, R, 机器学习算法 业务数据分析的发展方向 业务数据分析是入门相对容易， 但要精通比较困难， 在公司属于比较基础的岗位。如果专精于业务方向， 可以往管理方面发展， 如数据运营经理/总监。也可往数据挖掘工程师方面发展， 需要进一步掌握Python和机器学习算法等知识，后面也可以往数据产品经理的方向发展。 对于我自己来说，目前的职业发展方向是业务数据分析师， 主要任务是不断学习和完善业务数据分析的所有技能， 加强互联网运营的业务理解能力，打好职业发展的基础，未来的期望是往数据挖掘方面发展。 业务数据分析师的考量标准 业务数据分析师的考核标准产品理解能力： 熟悉业务的各种核心数据，明白用户从哪里来，进来之后做了什么，了解用户反馈最多的问题是什么。 了解产品功能渗透率和关键路径，再以这些数据为切入点，思考当前产品有哪些问题，并与产品经理沟通如何优化，同时看竞品数据和行业数据，深入了解业务数据。 深入了解： 要有自己的洞见，对于整个行业，各个不同阶段的领头羊是谁，他们靠什么成为领头羊，又因为什么出现增长瓶颈，当前各自的大法测试什么，对我们自身的app有什么借鉴，后续我们要监控哪些数据。 分析方法论： 能够快速从一个较全面、逻辑性、价值性的角度去分析，而不是单点无架构分析。所有方法论都是通过不断提炼、总结、实践得出来的。 指标体系方法论 流量分析方法论 路径分析方法论 产品分析方法论 营销活动分析方法论 用户流失分析方法论 A/B 测试 最大概率法则、二八定律、幸存者偏差等 可视化能力： PPT 制作能力 PPT专题报告之间的逻辑性 内容是否符合金字塔原理 数据可视化内容美观性 演讲能力： 表达能力 讲故事能力 形象化能力 大心脏能力如何 协作沟通能力： 跟产品、业务、研发沟通时的软技能 如何在团队中定义好自己的位置并让其他人感到舒服 逻辑思维能力： 分析推导过程的全面性、合理性、价值型 技术能力： excel的常规操作 统计知识是否能够熟练应用 hive、sql的熟练程度 python 中常用的数据分析库能否熟练应用 算法模型是否熟练搭建并知道有哪些坑。 —如何规划数据职业生涯 数据产品经理-产品架构师 掌握基础技能和思想技能：excel, sql, hive, python, 统计学、 基础机器算法策略产品： 机制设计、冷启动、用户画像标签体系思维模式： 依赖数据做决策，建立产品闭环——从数据产生、收集、统计、分析、反馈。职能分工： 功能产品，策略产品 参与完整的业务闭环-open建立明确的优化目标(objective)打通数据记录和分析流程(process)建立A/B测试优化的框架(Experiment)将目标按转化网络分解(Net) 通过闭环的优化过程， 感知数据。 独立负责项目或产品 机器学习定理 没有免费午餐定理，NFL定理没有任何算法在所有数据情形下有天然优势， 在没有实际背景下，没有一种算法比随机胡猜的效果好。 所有模型是没有好坏之分的， 当数据有线性分布时， 模型就会有效。 先掌握领域前言知识， 用它来指导业务， 而不是把每一个模型都跑一遍。 丑小鸭定理两只白天鹅的区别与丑小鸭与白天鹅的区别一样远。 https://blog.csdn.net/mnshenyanping/article/details/51280731 不要追求成为全栈工程师。 产品的逻辑一定要懂。什么是产品？-定义问题， 解决问题的逻辑。 比如： 用户标签体系、冷启动策略等。 不要频繁切换从事的业务领域 业务领域先验知识的积累， 是成功进行数据建模的关键。 业务领域的商业逻辑需要花时间搞清楚， 这对产品的决策至关重要。 同一个领域的不断努力可以形成个人口碑。 工作内容周末报表： 整体业务报表：核心指标和产品核心业务与功能变化 新用户报表：用户获取与渠道质量、新用户行为分析、新用户相关运营活动分析 分析师分析报表：分析师结合核心指标与产品内部的活跃现象，自行去分析用户行为数据，挖掘有效增长点，并讨论后产出有价值的分析报告 异常分析报表： 日活、留存、新增等数据发生异常时，通过细分探寻原因，定位问题。 如果日活正常偏高，可提醒业务同事选择进行一些活动。 根据报表数据与其他部门进行沟通，推动业务产品的迭代和发展。 每次产品改进时，和产品经理沟通本次改版的方向和目的 在新版本或功能上线后， 在第一时间（1-2天内）给产品经理搭建新版本报表，反馈改进结果。（新版本覆盖情况，新功能使用情况，新优化的数据表现；整体表现数据：留存，转化。具体功能的使用人数、使用率、使用留存等） 在1-2周内，给出3-4份详细的分析报告，进一步分析用户对新功能或新版本的反馈，挖掘信息，证明有效性，分享给产品经理，推动他们解决问题 3. 数据分析师能力如何展现 如何提高自己的数据分析能力 行业认知， 多看一些咨询公司的报告，了解分析的角度和行业趋势及现状。 数据认识， 对数据的敏感度不能局限在统计意义， 而是要掌握数据来源的方式，业务之间的埋点如何， 采集方式是什么样 分析的目的是什么，能否还原用户或产品行为 注重数据的探索， 多角度去思考， 输出与目的相关的发现，由点到面，还原到用户行为或营销策略的制定，比如， 用户使用的一款app, 什么时间发的信息能够让DAU迅速提升。 掌握统计学， 统计学是对各种假设检验的还原，要深入理解每个算法的场景和弊端 要尝试总结归纳，输出自己对行业的看法，动手+动脑思维。 假设你是领导，你如何看数据而不是玩数据；假设你是运营人员， 你如何建立分析体系， 指标设定如何让目标更有效 写文章，能够提高自己的故事构思和思维能力， 更有效的是能够积累文笔、表达、沟通深度等。想的越多， 说的才能越有深度。 如何算是懂业务、有思路的数据分析师 数据君 硬实力 工具的掌握 算法的掌握 有自己的技术博客 工具的操作和应用上有自己的输出和展示 软实力 要有好的思路 说话表达有套路，比如 总分总 等逻辑 要对方法路有深入的思考。 比如：swot,pest,5w2h,生命周期，aaarrr,4p， 金字塔原理等 训练方式： 脑图，逻辑树 案例 例如： 为什么最近GMV的增速放缓了？ 大局观，用pest,swot等方法先看看大环境的状态是什么？竞争对手如何 GMV计算的切入点，比如，人、货、物等（思维框架） 用5w2h、aaarrr、生命周期等方法去定位和排查问题出现的可能性。（不同方向有不同的方法论） 数据罗列和验证过程， 从数据分析的本身流程去全面核实，数据来源追溯、口径核实、对比历史等，找到产品用户放缓的原因到底是什么，假设验证的过程 问题和策略落实到人或者部门，要不然你的建议就是咨询，无法很好的落地，假设几个角色，要不面试官会觉得你只停留在学术层面 面试官要的不是答案，而是你思考的过程。 战斗力 你学了什么课程， 有没有输出自己的作品。把自己所学的做成一份从0-1的报告呈现给面试官，你的印象分会很高，他们会觉得你的心智更成熟，很有想法，每个团队其实都缺少一定的活力， 这才是年轻人应该有的担当。 当你把自己的作品展现出来， 总会有一些人和你产生共鸣，而你在共鸣中不断的迭代自己， 这才是一个良性的循环，你才有可能有不平庸的资本。 其实数据分析师一个入门简单， 深入难的学习， 他不是依靠参加各类培训和课程积累，而是依靠你的经验和思维系统。 4. 数据分析方法论数据分析的主要方法 趋势分析 通常我们在数据分析产品中建立一张数据指标的线图或者柱状图，然后持续观察，重点关注异常值。在这个过程中，我们要选定第一关键指标(OMTM，One Metric That Metter)，而不要被虚荣指标(Vanity Metrics )所迷惑。以社交类APP为例，如果我们将下载量作为第一关键指标，可能就会走偏;因为用户下载APP并不代表他使用了你的产品。在这种情况下，建议将DAU(Daily Active Users，日活跃用户)作为第一关键指标，而且是启动并且执行了某个操作的用户才能算上去;这样的指标才有实际意义，运营人员要核心关注这类指标。 多维分解 多维分解是指从业务需求出发，将指标从多个维度进行拆分;这里的维度包括但不限于浏览器、访问来源、操作系统、广告内容等等。 为什么需要进行多维拆解?有时候一个非常笼统或者最终的指标你是看不出什么问题来的，但是进行拆分之后，很多细节问题就会浮现出来。 举个例子，某网站的跳出率是0.47、平均访问深度是4.39、平均访问时长是0.55分钟。如果你要提升用户的参与度，显然这样的数据会让你无从下手;但是你对这些指标进行拆解之后就会发现很多思路。 用户分群 用户分群主要有两种分法：维度和行为组合。第一种根据用户的维度进行分群，比如从地区维度分，有北京、上海、广州、杭州等地的用户;从用户登录平台进行分群，有PC端、平板端和手机移动端用户。第二种根据用户行为组合进行分群，比如说每周在社区签到3次的用户与每周在社区签到少于3次的用户的区别，这个具体的我会在后面的留存分析中介绍。 用户细查 用户行为数据也是数据的一种，观察用户在你产品内的行为路径是一种非常直观的分析方法。在用户分群的基础上，一般抽取3-5个用户进行细查，即可覆盖分群用户大部分行为规律。绝大多数产品都或多或少存在一些反人类的设计或者BUG，通过用户细查可以很好地发现产品中存在的问题并且及时解决。 漏斗分析 漏斗分析是一套流程式数据分析，它能够科学反映用户行为状态以及从起点到终点各阶段用户转化率情况的重要分析模型。漏斗分析模型已经广泛应用于网站用户行为分析和APP用户行为分析的流量监控、产品目标转化等日常数据运营与数据分析的工作中 漏斗分析要注意的两个要点：第一：不但要看总体的转化率，还要关注转化过程每一步的转化率;第二：漏斗分析也需要进行多维度拆解，拆解之后可能会发现不同维度下的转化率也有很大差异。 留存分析 留存分析是一种用来分析用户参与情况/活跃程度的分析模型，考察进行初始行为的用户中，有多少人会进行后续行为。这是用来衡量产品对用户价值高低的重要方法 衡量留存的常见指标有：次日留存率、7日留存率、30日留存率等等 留存分析可以帮助回答以下问题：一个新客户在未来的一段时间内是否完成了您期许用户完成的行为？如支付订单等；某个社交产品改进了新注册用户的引导流程，期待改善用户注册后的参与程度，如何验证？想判断某项产品改动是否奏效，如新增了一个邀请好友的功能，观察是否有人因新增功能而多使用产品几个月？ A/B测试与A/A测试 A/B测试是为了达到一个目标，采取了两套方案，一组用户采用A方案，一组用户采用B方案。通过实验观察两组方案的数据效果，判断两组方案的好坏。在A/B测试方面，谷歌是不遗余力地尝试;对于搜索结果的显示，谷歌会制定多种不同的方案(包括文案标题，字体大小，颜色等等)，不断来优化搜索结果中广告的点击率。 这里需要注意的一点，A/B测试之前最好有A/A测试或者类似准备。什么是A/A测试?A/A测试是评估两个实验组是否是处于相同的水平，这样A/B测试才有意义。其实这和学校里面的控制变量法、实验组与对照组、双盲试验本质一样的。 5. 如何分析业务数据问题陈述、产生假设、收集数据、分析数据、获取结论、采取行动 评估和定位问题 在深入研究任何类型的数据之前，应该快速找到你需要解决的真正问题，并用最简单的话定义它 如果无法用简单的语言解释你要解决的业务问题，那么任何数据分析都无法解决问题。 快速评估和定位问题的三问： 这是否是系统异常导致的问题？ 下载量下跌，但激活量没有，也许是下载数据没有埋点或埋点错误？ 这是更大问题的预兆吗？ 注册号码的下降是网站故障的指示吗？ 你在看一个重要的度量指标吗？ 如果网站的转化率下降，但原始注册量没有下降， 那么就从一个紧急事件变成了一个谜团待揭开 确定潜在原因 经验检索，快速寻找原因根据经验，寻找任何明显的可能原因或问题的答案, 当你检查显示问题的来源或报告后，是否有任何异常原因立即浮现在脑海中？ 例如，你的电子商务网站的ssl认证可能过期，导致浏览器弹窗窗口警告数据不安全，从而显著降低购物车转化率 询问相关人员原因这个问题会影响和涉及其他团队吗？如果是这样，他们是否对可能的原因有任何了解？即使问题与其他团队之间没有明显的联系，也有必要咨询一下。 营销经理可能会问客户支持： 我注意到注册数据下降了，你能否想一想过去几周你发现过什么相关的变化吗？ 创建假设一个假设知识一个尚未得到证实的有根据的猜测。 在分析数据之前，清楚地说明问题的几个可能原因非常重要，这有助于防止常见的数据分析误区。 导致注册量突然下降的假设： 某些地区的公众假期 最近对营销网站的更改 星期一网站中断导致注册过程中出现错误 转换率下降减少了注册量 产品页面在某搜索排名下降到搜索结果的第二页 技术思维的方法——科学假设需具备的条件： 涉及一个自变量和一个因变量 它是可测试的 它是可证伪的 有时查看数据可能会产生一个新的假设，需要再次测试。最终，我们的假设会在下一步通过数据分析得到证实或反驳。 分析数据 分割并确定相关数据指标根据你的假设，你需要查看哪些数据？哪些指标可以帮助你证明或者反驳假设？ 你可以按国家/地区， 渠道和网络会话持续时间细分注册次数，以测试你的假设 注意你的数据基于你已知的业务指标，你可以判断数据是否出现异常？如果无基准线，请用历史数据作为起点。 app注册量同比下降20% 评估异常或趋势的影响经过前两个步骤，你要查看发现的趋势/异常是否足以解释问题 在寻找数据中的异常或趋势时，要注意这些异常或趋势不仅要具有统计意义，也要具有实际意义。我们需要弄清楚是什么会对我们发现的问题产生实际影响。 统计显著性检验用于确定你注意到的异常是由于抽样误差还是适用于所有对象。 6. 数据分析面试问题 假设你的产品新功能上线，那么哪些数据指标可以量化这些功能？这些变化产生的影响，你准备如何入手？ 提示：从数据角度讲，可能从页面的热力图选取热区，或根据 A/B test 实验来选择方案可能会更加理想。 如果抖音一个视频的页面变宽， 那么它的点击数，曝光数，和点击率如何变化 面试技巧：理论基础要非常好，面试环节和谈薪资环节，一定要非常淡定，有一颗大心脏，不到最后签合同，绝不松懈。 活动的拉新效果怎么评估 国企面试技巧 软技能——与人相处能力、展示能力、时间管理能力、预判力 面试技巧——积极主动、沟通能力、逻辑框架性、表现自信、淡定。 BAT面试 1面-电话面说话要有逻辑性，简历中项目的数据要非常清晰， 目前公司app的日活、月活、渠道留存。 2面-现场面多使用应聘岗位公司的app, 带一份自己写的优秀的分析报告，当面试官问你有什么想问的，结合公司app内容， 提前想好一些问题， 问面试官， 并给出自己的见解。 3面-总监面要有亮点， 有自己代表性的项目， 了解产品的宏观知识，行业趋势， 了解产品竞品， 行业痛点， 产品改进， 之前项目的上下游关系， 要多有想法。 4.必问的三个问题 流量波动 常用的三个app 之前产品的商业模式，如cpc, cpm 7. 专题分析流量分析 从哪里来 经过什么 产生什么价值 波动分析 渠道分析——常见渠道内部渠道外部渠道作用： 拉新用户前期靠渠道推广， 中后期靠自传播，或用户的免费推广 软技能+面试技巧进入一家公司靠能力， 在一家公司的发展空间靠能力+软技能。同样一个问题，如何快速领会出领导的意图，并且能够有效的表达和展现，往往决定后面的发展高度。 开会提前准备会议内容，并且随时准备表达自己的数据分析能力。把会议当做展示自我的一个机会。 比如ceo开员工大会，对业务提出一些问题，或数据不太清楚，业务该怎么走。如果你能够对公司的业务数据了然于胸，并对该类问题有过见解，必然会脱颖而出。 BAT工作阿里：对寻找方向，重点解决，方法论研究。腾讯： 埋点，指标体系，检测，异常，A/B测试。 增长黑客。百度： 用户运营，体系 用户增长 日常主要工作：数据异常排查专项分析KPI埋点 数据异常排查：解释数据波动，排查原因。 数据较大波动说明两个问题：1. 数据本身有问题。 2. 业务本身有问题 数据异常排查的前期准备： 业务理解 指标口径 当前数据的产出过程 异常排查的方法论：判断是否异常 亲自看数据的准确性 时间轴拉长，看是近期异常，还是周期异常把近三个月的数据拉出来 看和该指标关联的其他指标或其他核心指标是否异常 找到一个关键人物，提前沟通一下。 最大概率法则闭合 8. 数据分析方法案例如何提高全站留存率？ 选择高留存的行为，并选择其中比较容易扩大使用者面积的几个，在产品功能层面放大，让更多的人看到并发生转化，从而提升留存。 评估最终效果时，首先评估该功能的使用人群和占比是否有效提升，有则认为改进有效 以上主要因为产品在改进期间会同时发生很多其他改进，也会有各种不同的市场，运营活动，单纯用留存来评估，无法有效验证。 例如：即刻产品的核心功能之一就是内容的分发，用户绝大多数的时间都在浏览关注、推荐和动态页的内容与评论。按照用户使用这些内容模块的行为，可以按其门槛和深浅分为路人行为（如浏览帖子）、围观行为（点击并查看评论）、普通参与行为（点赞、转发）、深度参与行为（发布评论）等。通过留存分析功能，发现行为深度越深，用户的留存就越高。但由于扩大发生参与行为的门槛过高，落地性也就较差。因此，选择围观行为，通过放大其面积来提升用户全站留存，所以，就在产品信息流列表页露出一条热评，一方面让更多的用户看到最精彩评论，另一方面更有力的引导用户点击查看更多内容。 新增热评功能后，成功将阅读评论的用户比例提升，并且成功提高了全站留存 具的操作只是帮你快速业务流程梳理出关键性的分析和指标体系，那么这些体系和指标的度量和之间的关系就要依靠统计学去界定和规范 一个网站改版了，新版的页面没有改变原来的交互操作，只是改变了视觉样式，用户访问量和点击量变化了，这些变化是好是坏? 1、我们已知的是改版前后点击量的数据和用户访问量的数据2 、我们想知道这个变化是好是坏 要怎么做?算一下改版前后用户的百分比和点击量的百分，如果改版后用户量下降了，点击量下降了是不是改版就不成功?显然我们不能如此简单的看问题。要比较这两个样本，我们可以使用T检验。 T检验(Student’s t test)是用于小样本(样本容量小于30，总体标准差σ未知的正态分布)的两个平均值差异程度的检验方法。 我的置信区间是95%，所以如果sig&lt;.05就代表差异显著。 从表上看，改版前后点击量和用户数两项上差异并不显著，所以我们可以认为这次改版至少没引起什么不良的影响。 当然现实问题往往更复杂，仅就改版为例，我们需要考虑很多问题，例如： 1、改变了哪些内容? 外观还是交互方式?或者外观+交互方式?布局有什么变化?交互方式的变化对用户完成一个任务所需的步骤或点击次数是否有改变?2、改版前的数据采集了多少天?改版后的数据采集了多少天?3、改版前后的时期在每一年的相应劫夺，用户的访问量是否有显著变化?趋势是怎样的? tabula工具， 将PDF表格提取到Excel 对新版本的分析 改版后，新功能是否受欢迎 要衡量一个新功能是否受欢迎，基本就看这个功能上线之后，用的人数多不多，用的人越多，表示这个新功能还是挺受欢迎的（当然，这里还有一些运营推广的因素）。 一个比较好的衡量指标是功能活跃比，什么叫功能活跃比，也就是使用了新功能的用户数/同期活跃用户数，比如说新功能的用户数是1000人，而同时期产品的整体活跃用户数是10000人，那么这个功能的活跃比就是10%。 对产品流程转化率是否有提升 我们就需要通过去观察整个产品的流程转化率是否因为产品迭代改版而有所提升。最基本的方法，就是通过创建流程漏斗来进行数据观察们就可以将上述事件组装成一个转化漏斗，如果你优化了商品详情页或者是搜索页面，那么就可以很好地通过漏斗来看出，改版之前和改版之后，这个流程的转化数据发生了什么变化，每个小环节的漏斗转化率又发生了什么变化，这样就能比较准确地评估出产品迭代对流程转化率是否具备提升作用 对产品整体留存的影响 在迭代之后，也可以好好观察一下产品的整体留存是否产生了变化，比如次日留存、周留存、月留存等等指标，是否朝着更好的方向发展 用户究竟如何使用新功能 用户究竟是如何使用产品新功能的，是否符合你预期设想的那样，还是说用户自己创造出了新的玩法 向老板汇报注册量下降的原因 核心问题： 我们的注册量下降， 是什么原因导致的？ 假设： 由于….导致了注册量的下降 某些地区的公众假期 最近对营销网站的更改 星期一网站中断导致注册过程中出现错误 转换率下降减少了注册量 产品页面在某搜索排名下降到搜索结果的第二页 数据分析：思考数据有多少变化?是否真的异常分别按照天和小时查看数据的变化，发现更改网站（网站注册会出现中断）时注册量确实有下降，但网站更改完成后注册量仍然在下跌，且进一步分析发现对注册表单样式的更改，使转化率略有上升，因此不是对营销网站的更改导致。通过进一步查看数据，发现在总体注册量下降期间，到达注册页面的人数减少了大约 10％，因此，可能是上游问题导致的。接下来，考虑假期假设（下跌开始时间是某些地区的公众假期）。但随着时间的推移，各国的注册量都有所下跌，所以该假设排除此时，她整理了一下思绪，决定分析点击付费广告投放更改后的数据。发现其中一个广告系列转化率下降了 50％，但这个只占注册量的 1％，所以不是主要原因。Jody 又通过渠道查看注册率，发现有机搜索（占注册量的 70％）下降了 20％，推测是几周前页面更改引起的页面排名的变化。于是，她开始检查 SEO 数据，发现主要关键字已降低排名，现在位于第二页，这样一来，除了注册量其他方面也会受影响。最终，Jody 除了解释注册量下降之外，还创建了一个策略来恢复注册，并将分析报告呈现给了产品副总裁和首席执行官。 为什么购物车到下单的转化率在降低？ 核心问题：许多潜在客户在购物车结算这一步流失，我们该如何降低流失率？ 假设： 由于…转化率下降 放入购物车的人绝对数量增加 最近对付费流程的更改 季节性（即假期，学校休息等） 促销结束导致更多人放弃下单 某些商品出现问题，影响下单 Tyler 第一步思考加入购物车的人绝对数量是否增加？如果有大量人开始向购物车添加商品但完成购买的人数保持不变，那么可以判断有一批购物者的转化率降低，他注意到加入购物车的人数略有增加。 然后他开始询问相关团队的人，如有没有促销活动？有没有推出新产品？会不会有季节性影响？付费过程有什么变化吗？价格是否经过调整？（注意：根据业务和产品范围的不同，这可能会有很大差异。） Tyler 最终得知付费流程发生了一些小变化。现在，他们不仅仅列出购物车中的商品，而是展示每件商品的图片。 为了进一步分析这种变化的影响，他将付费流程分成了不同的步骤，发现用户的浏览数据正常，事实上，更多的人正在进行下一步，所以这似乎不太可能是罪魁祸首。 接下来，Tyler 通过将本周的购物车转化率与前几年的同一周进行比较来寻找任何季节性影响，他还通过快速浏览日历，了解任何可能的线索，但由于会话和电子邮件开放率等相关指标未受影响，季节性因素假设也排除。 Tyler 之前咨询到最近的促销结束了，按照常识，当人们意识到促销已经结束时，他们更有可能放弃下单。Tyler 在购物车转化率下降之前使用促销代码查看付费比例，发现只占 5％，但放弃率的变化是三倍，所以这只能算一个促成因素。 Tyler 又开始思考这是商品库存的原因吗？但所有商品的性能相当一致，这个假设也不成立。 在考虑其他可能的原因时，Tyler 再次审查付费流程。发现商品价格页面中对运费的描述部分大大减少，他回忆起之前对产品页面进行一些外观修改的时间与流失率增加时间完美吻合。 回顾这些变化后，他的新假设是潜在客户放弃下单，是因为他们期望下单时购买的价格是产品页面设定的较低价格，一旦他们看到全价（包括运费），就会放弃下单。 发现这一点后，Tyler 非常有自信的准备使用 A / B 测试来检验假设，如将产品更改恢复到以前的设计，或者尝试使用包含运费的版本。最终，他验证了假设，并调整了页面。 想找出所负责的 App 激活率降低的原因 核心问题： 初始下载后，打开和使用 App 的人数减少了，怎样才能提高激活率？ 假设：由于……激活率下降 App 的更改使人们不太愿意激活 一群新的（或不同的）人开始尝试这种产品 她注意到在过去 3 个月中激活（打开并开始使用 App）的比例一直在稳步下降（与下载总数相比） Sofia 先查看一些数据来获得更多背景资料。最终她发现下载的绝对数量明显增加，而激活人数仅略有增加。不过两个指标的绝对数字都在增加，让她松了一口气。（注意：根据不同的企业，这可能是也可能不是问题。最终，这取决于是否浪费了额外的注册资金。） Sofia 后续很快确定应用程序中与激活下降相关的初始体验没有任何变化。 现在，她更密切地关注哪些人正在下载应用程序以及人口统计数据是否发生了变化。因此，她按地区对下载人群进行了分层，发现来自较低激活区域的下载量略有增加，但这远远不足以解释激活率的下降。 接下来，Sofia 分析不同渠道（例如，应用商店搜索，社交广告，推荐等）的下载情况，发现推荐下载渠道的下载量大幅增加，且似乎与她之前提到的增加的下载次数大致相同。深入分析后，她发现通过推荐下载的激活率明显低于其他渠道。 Sofia 通过咨询营销团队，了解到基本是一个高流量文章引起的推荐渠道的下载量增加，而且它没有任何成本，具有很大的潜力待开发。Sofia 和营销经理下一步准备采取行动增加该渠道的激活量。 Sofia 最终通过一些快速的数据分析，使最初的问题变成了一个机会 9. 如何避免数据分析中的坑要避开哪些坑？ 不要重复无意义的工作。许多刚入行的小伙伴喜欢把清理数据作为主要工作，纪敏认为这只是让你接触数据的一种方法，每天重复地提出需求、整理表格，会磨灭掉许多对于分析师岗位的热情。 不要“全手动”，要寻找代替的工具。既然不能重复地做无意义的工作，那么就要学会用工具去代替人工，选择合适的用户行为模型和工具，能把分析师的主要精力放在规律和策略的探索上，才能充分发挥一名数据分析师应有的价值。 数据分析只是一种辅佐手段，它无法从根本上改变产品方向、功能价值，主要辅佐和支持的产品，探索更有价值的数据意义。 https://www.sensorsdata.cn/blog/20181107/ 避免数据偏见在分析数据时受个人偏见和动机的影响，即仅选择支持你声明的数据，同时丢弃不支持声明的部分。“数据偏见”将让数据的客观性荡然无存。 避免这种谬误的方法是在分析数据时，尽可能收集相关数据，并询问他人意见 避免数据疏浚数据疏浚（Data Dredging）是指未能确认相关性，实际上是偶然的结果。 在寻找问题的原因时，很容易被数据蒙蔽。乍一看，这些数据可能具有统计学意义，但进一步测试（例如，检查趋势是否持续，查看相关指标等）可能会发现只是偶然结果。 避免这种谬误的方法是在分析数据时，从假设开始检查相关指标和观察数据变化趋势。 区分因果关系和相关性 在数据分析时很容易将两个事件同时发生（相关），判断为因果关系。 避免这种谬误的方法是，收集更多数据并查看可能的第三方原因，有时会发现他们的相关关系可能与第三个独立因子相关，而不是彼此相关。 例如，我们发现放弃其在线购物车的潜在客户往往具有较低的总购物车价值（放弃时购物车中物品的总成本）。此时，我们没有足够的数据来确定这是一致的相关性，是偶然结果，还是由其他因素引起的。深入挖掘我们可能会发现运输成本导致购物车到下单的流失率上升，因为免费送货仅适用于超过特定最低购物车价值的订单。 解决问题，做出明智的决定在找到数据支持的结论后，你需要记下一个简短的摘要（包括问题，数据显示的内容以及由此产生的决策 / 行动），这样做有两个目的： 1.将你所分析的数据和结论告知可能涉及或受影响的任何其他团队，为其他人提供有价值的背景信息。 2.这个记录也将使你在将来出现类似情况时更容易参考和以防其他人想要查看数据本身。 最终，问题解决了，也总结了有价值的经验。 对数据进行分析的最佳途径： 业务梳理——了解业务需求 确定业务目标——弄清产品目标以及当下的首要问题 事件设计——记录和目标相关的用户行为，并定义为相应事件 数据采集——保证采集质量，确定好事件采集时机，和开发进行沟通 构建指标体系——确定想要看的指标，想要达到的分析粒度，建立产品的第一关键指标 数据分析——业务人员根据自己的经验，进行数据分析，迭代优化 以子弹短信app来进行举例 产品信息结构图了解产品承载了哪些信息和功能，这些信息的目的是什么， 想要用户干什么 产品功能结构图将产品的功能模块梳理出来， 功能之间怎样跳转，功能的上游入口和下游入口是什么，都要想清楚，并标记出来 核心业务梳理流程图弄清梳理出产品的核心业务流程，密切观察用户在核心业务流程运转的整个过程，比如：注册流程，新手完成任务流程等 梳理产品需求首先明确我们要分析什么样的场景，解决什么样的业务问题，要解决这个业务问题，要看什么数据，要衡量什么指标。 定义指标明确产品的目标和目前产品阶段来说最重要的问题。比如，要增加销售额，销售额等于活动流量乘以付费转化率乘以客单价，我们就需要把这个指标需求进一步细化，分给不同的角色各有侧重点的去执行。 要梳理这个指标需求和产品逻辑有什么关系？比如，活跃用户指标，一般是启动app的用户来定义活跃，实际上，每一款产品的核心功能不同，只有完成核心功能的用户才能算活跃 事件设计和数据采集埋点设计根据用户在使用产品功能时产生的行为数据，从用户使用行为分析的角度去分析需要的埋点 用户行为：发布话题——上传图片、视频——进入本地视频列表——选择视频——输入话题——发布话题 操作系统，应用版本，话题分类 操作系统，应用版本 操作系统，应用版本，是否取消 操作系统，应用版本，操作类型，是否取消 操作系统，应用版本，输入话题名称方式，话题名称 操作系统，应用版本，话题分类，话题名称，是否成功 https://www.sensorsdata.cn/blog/20180929/ https://www.geckoboard.com/learn/data-literacy/basic-data-analysis-guide/ 一个提升用户体验的绝好方法：触点管理]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>分析流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[专题分析]]></title>
    <url>%2F2018%2F12%2F22%2F%E4%B8%93%E9%A2%98%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[专题分析数据分析师的一个重要考核， 就是专题分析的效果 需求沟通 一定要将20%以上的时间分配在沟通需求上，一定要当面沟通。 原始需求——了解需求—— 本质需求——逻辑树——sql提数——分析方法——写分析报告 例如上线活动效果的专题分析流程： 业务需求解读————需要与业务方当面沟通， 业务沟通贯穿整个分析流程。 活动效果的数据情况 目前活动对整个APP活跃用户，或者留存率的影响，以及活动的问题有哪些 活动的拉新、促活，用户粘性情况， 活动效果的优劣 通过逻辑树进行整理 建立分析AP， 分析师与业务方之间完成分析数据对焦，形成最终报告框架和结构。 通过SQL进行取数验证 分析： 组成部分、数量比较、维度变化、各项变化、各项相关性、其他数据挖掘 撰写报告： 90%的图+10的文，标题就是结论。 结论前置。 讲故事，报告的逻辑性一定要强。 项目专题分析的特征 有目标——围绕项目KPI来进行。 有节奏——2-3周输出一份完整的报告。 有闭环——报告不能太技术性， 要写成大白话最好， 说人话。 要明确给业务方提供建议改进方向， 而不是自己技术展现。 3.专题分析实战 项目背景：电信app要在数据分析的基础上， 对产品运营进行优化，提升用户留存5%的绝对值。 第1阶段：新用户留存整体分析目的： 摸清数据现状， 同时找到若干个切入点。关键点： 不要太注重细节， 该过程讲究报告产出的时效性，指明数据分析的方向， 让其他人员感受到数据分析师的存在。过程：渠道方面， 各渠道的uv, 一、二级渠道的次留、7留。 产品方面, 主要功能的渗透率，功能是否出问题， 关键漏斗数据如何， 漏斗数据上有无发现。 用户方面， 产品用户的画像是什么， 用户的行为分布如何。 使用产品用户的分布。 第2阶段： 寻找优化切入点， 一般是1-2个。比如： 1. 关键立即数据发现曝光pv到点击pv的ctr很低， 我们可以围绕这个点细致分析： 对于新用户， 应该曝光什么，在什么时候， 什么位置曝光。 2. 某个量大的二级渠道的次留明显低于其他渠道， 围绕这个点，来进一步分析原因： 渠道本身质量存在问题？用户安装了竞品？当前产品设计与渠道用户不太匹配？高留存的渠道本身特征是什么？通过这两点， 给渠道和运营的同事提供建议， 结合A/B测试， 就能看到数据分析的效果。 第3阶段： 不断重复前两个阶段， 继续寻找其他切入点。 同时进行竞品分析， 营销活动分析，用户流失分析等等 每一次分析报告都要有能落地的点， 并且真正的落地了。 形成闭环， 评判数据分析师的标准： 产出专题分析的质量和数量。 质量就是数据分析落地的点和提神的kpi。 改版分析 新改版页面的效果怎么样？ 产品迭代的需求来源： 市场调研、竞品分析、用户反馈、数据分析、团队头脑风暴。 分析方向： 改版后， 新功能是否受欢迎 改版后， 对产品的流程转化率是否有提升 改版后， 对产品的整体留存的影响 改版后， 用户究竟如何使用新功 功能活跃比新功能的用户数 / 同期客户端活跃用户数 漏斗转化提升 次日留存、周留存、月留存等指标是否朝着更好的方向发展。 用户使用新功能， 是否符合你的预设， 还是说用户创造出了新的玩法。]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>分析流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[业务分析——埋点方案]]></title>
    <url>%2F2018%2F12%2F22%2F%E5%9F%8B%E7%82%B9%E6%96%B9%E6%A1%88%2F</url>
    <content type="text"><![CDATA[数据埋点方案https://www.sensorsdata.cn/blog/20181114-2/ 问题： 需求整理不完善， 没有整体考虑， 版本更替原有埋点不可用 数据统计口径不清楚，没有将埋点的具体采集时机郑群传达，埋点不是自己想要定义的指标 数据采集方案没有想清楚， 哪些应该在前端埋点，哪些应该在后端埋点，埋点采集sdk如何正确使用没有了解清楚]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>分析流程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[营销活动分析]]></title>
    <url>%2F2018%2F12%2F22%2F%E8%90%A5%E9%94%80%E6%B4%BB%E5%8A%A8%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>数据分析</category>
        <category>各项细分</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
        <tag>活动分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据运营]]></title>
    <url>%2F2018%2F11%2F28%2F%E7%94%B5%E5%95%86%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[数据运营的内容 什么是数据运营用数据指导运营决策、驱动业务增长 数据运营与其他运营的区别常见的运营： 用户运营、内容运营、产品运营、活动运营、社群运营、品类运营、流量运营、APP运营 工作岗位上的区别： 从事数据采集、清理、分析、策略支持等工作，支持整个运营体系朝精细化方向发展。 从思维方式来看：数据化运营属于运营必备的的一种技能或思维方式，即通过数据分析的方法发现问题、解决问题、提升工作效率、促进业务增长。 数据运营的常用场景 拉新：获取新用户 广告投放效果 转化：注册转化率、订单转化率 漏斗模型 促活： 如何让用户经常使用我们的产品 分析具体用户行为， 用用户行为分析做用户画像 留存：提前发现可能流失的用户，降低留存率 根据用户的细微行为，来预测用户是否会流失 变现：发现高价值用户，提高销售效率 数据运营的具体工作 数据规划：收集 整理业务部门的数据需求，搭建数据指标体系 数据采集：采集业务数据，向业务部门提供数据报表 数据分析：通过数据挖掘、数据模型等方式，深入分析业务数据；提供数据分析报告，定位问题，提出解决方案 数据规划两个重要的概念：指标与维度 搞清楚数据分析的目的是什么， 需要什么样的数据 指标用来衡量具体的运营效果，指标的选择来源于具体的业务需求，从需求中归纳事件，从事件对应指标。 比如： uv、 DAU（日活跃用户数）、 销售金额、转化率等 电商网站为例，选择指标的过程为： 明确需求： 网站主要业务是商品销售，希望通过数据分析来提升网站销售金额 归纳事件：用户购买是一连串关键事件的结果，包括访问网站、浏览商品、注册账号、加入购物车、开始结算、支付等 对应指标：销售金额 = 访问流量下单转化率支付转化率*客单价 通过分析，可以得出销售金额是OMTM，同时，整个销售金额的指标提醒包括访问流量、下单转化率、支付转化率、客单价四个可操作的指标。 维度体系 维度是用来对指标进行细分的属性，选择维度的原则是： 记录那些对指标可能产生影响的维度 维度分为：维度类别和具体维度人口属性： 性别、年龄、职业、爱好、城市、地区、国家设备属性：平台、设备品牌、设备型号、屏幕大小、浏览器类型、屏幕方向流量属性：访问来源、广告来源、广告内容、搜索词、页面来源行为属性： 活跃度、是否注册、是否下单 数据采集 埋点在产品中手动添加统计代码收集需要的数据，如果要手机用户注册数，就需要在注册按钮处加载相应的统计代码。容易发生楼埋，错埋的情况。 可视化埋点通过可视化交互的方式来代替手动埋点。 Mixpanel 采用了可视化的埋点方案 无埋点只需加载一个SDK就可以采集全量的用户行为数据，可以自定义分析所有行为数据。 数据报表定期向业务部门提交数据报表是数据运营的工作之一，包括日报、周报、月报甚至年报。数据报表建立在数据指标体系的基础上，数据运营应该让这部分工作尽量自动化。搭建数据看板是除了数据报表之后的又一项工作，与企业BI系统连在一起，属于数据可视化的部分。 数据分析通过数据分析的方法定位问题，提出解决方案，促进业务增长。选择什么样的分析方法要和你的业务场景相结合。 数据运营岗位弱化了对编程统计的要求，更强调在现有工具基础上灵活使用分析方法。一个好的数据运营一定要及时发现问题、定位问题、并提出可行的解决方案。 分析方法 运营场景 流量标记.UTM 广告投放、对外推广 多维度分析 细化问题 转化漏斗 转化过程流失分析 留存曲线 用户留存分析 魔法数字 用户留存分析 用户画像 用户细分、精准营销 用户细查 用户研究、产品研究 热力图 用户产品访问偏好 数学建模 预测分析、精准营销 A/B测试 产品、运营优化 用户留存分析详解 用SQL替代机器学习，这是新时代的“电风扇吹香皂盒”吗？ 还在用 AI 和机器学习？简单的 SQL 脚本就能替代！]]></content>
      <categories>
        <category>数据分析</category>
        <category>电商分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[投资系统]]></title>
    <url>%2F2018%2F10%2F27%2F%E6%8A%95%E8%B5%84%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[理想的收入来源 工资收入 出卖时间 成为某个领域的行家，做出成绩，争取超出领导的期待，然后升职加薪。 兴趣收入 个人品牌变现 持续给大家提供价值，帮助大家解决问题，满足大家的需求提升自己的写作能力 投资收益 资本生息]]></content>
      <categories>
        <category>个人系统</category>
        <category>投资</category>
      </categories>
      <tags>
        <tag>生活</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[统计学]]></title>
    <url>%2F2018%2F10%2F20%2F%E7%BB%9F%E8%AE%A1%E5%AD%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言：为什么应该学点统计学统计学是人类发明用来研究我们自身的科学，它与我们的生活息息相关。 小到支付宝根据用户个人消费数据判断其消费水平，从而有针对性进行限额借贷。大到国家通过国民生产总值这样的统计数据分析，来研究国家经济发展趋势等， 都需要用到统计学知识。 我们可以从统计大师 Hans Rolling 的演讲中， 来看下 1960 年到 2003 年的世界各国出生率与经济发展是如何变化的。 Hans Rolling —— 统计的魅力 有人会问，统计学那么多高深的概念和复杂的算术， 在平时的生活中自己也应用不到。 这种想法其实是错误的。我们学习统计学， 不光是学习怎么对数据进行统计计算，更重要的是学会运用统计思维去更理性地看待周围的事物。 比如， 最近报道的一名美国公民在 10 月 23 号中了 16 亿美元的彩票， 看到这则消息，你会不会也有点心动， 也想去买个彩票。其实， 关于是否要买彩票，统计学有一个重要的概念来衡量：期望值 即同一种行为多次重复之后，所能得到的平均收益 举例来说， 假如某彩票规则为：每次买张彩票需要 2 元，假设 200 次抽奖可以中奖一次，奖金为 300 元。 期望值 = 300 (1/200) + 0 (199/200) = 1.5。 期望值是 1.5 元， 但是每次抽奖成本为 2 元， 于是每次净亏损 0.5 元。如果你偶尔买一次就算了， 但如果你长期买彩票，就肯定会亏很多钱。 况且现实生活中，中奖的概率远远低于 1/200 你可能会觉得，概率那么低， 那我怎么感觉天天有人中奖呢， 这背后其实是媒体的选择性报道， 也就是统计学中的选择性偏差问题 例如：二战期间，盟军为减少飞机在敌人防空炮火中的损失，军方决定为飞机加装防护，多数人认为，应该在机身中弹多的地方加强防护。但统计学家沃德认为，应该给那些没有中弹的油箱和驾驶部位进行防护，因为这些部位中弹的飞机根本没有机会飞回来。 现实生活中，也往往会存在一些选择性偏差的数据，我们生活中接触的数据越来越多， 解读数据背后的信息， 辨析数据真伪就显得非常的重要，这则 TED 视频对此有更深入的讲解。 为什么应该学点统计学 那么统计学到底要学什么呢？统计学主要学习两个方面 描述统计， 描述数据的基本情况 通过研究数据的平均值，中位数，标准差等指标， 来了解数据的整体分布状况，从杂乱的数据中得出有用的信息 推理统计，根据样本数据来对总体进行估计 通过对样本数据的研究， 来对总体数据进行估计，评估推理数据的准确度，统计学中就会通过置信度， 卡方分布等来对总体进行估计。 统计学是我们将客观数据转化成有用信息的一把钥匙， 运用统计概念对一些更为广泛而大致的信息及规律进行思考， 可以改善我们做出的判断和决定。我们当然不能指望这些判断不出错误， 但每一个好的决策都会帮助你更有效的利用这些信息，积少成多，把事情做成的概率会大很多 。 Hans Rolling —— 东方崛起 统计学知识点在python中各种图形的绘制 条形图 1234567891011121314from matplotlib import pyplot as pltplt.rcParams[&apos;font.sans-serif&apos;]=[&apos;SimHei&apos;] #解决中文乱码# 语法：plt.bar(left, height, width=0.8, bottom=None, hold=None, data=None, **kwargs)num_list = [1, 2, 3, 4]name_list = [&apos;Monday&apos;,&apos;Tuesday&apos;,&apos;Friday&apos;,&apos;Sunday&apos;]# 如何想让条形图横放， 将 plt.bar 改为 plt.barhplt.bar(range(len(num_list)), num_list, color = &apos;rgb&apos;, tick_label = name_list) plt.show() 折线图 123plt.plot([1, 2, 3, 4], [1, 4, 9, 16], &apos;g&apos;)plt.ylabel(&apos;some numbers&apos;)plt.show() 折线图要注意起始位置， 避免被图形误导。 饼图 1234567891011121314151617plt.figure(figsize=(6,9)) #调节图形大小labels = [u&apos;大型&apos;,u&apos;中型&apos;,u&apos;小型&apos;,u&apos;微型&apos;] #定义标签sizes = [46,253,321,66] #每块值colors = [&apos;red&apos;,&apos;yellowgreen&apos;,&apos;lightskyblue&apos;,&apos;yellow&apos;] #每块颜色定义explode = (0,0,0,0) #将某一块分割出来，值越大分割出的间隙越大patches,text1,text2 = plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct = &apos;%3.2f%%&apos;, #数值保留固定小数位 shadow = False, #无阴影设置 startangle =90, #逆时针起始角度设置 pctdistance = 0.6) #数值距圆心半径倍数距离#patches饼图的返回值，texts1饼图外label的文本，texts2饼图内部的文本# x，y轴刻度设置一致，保证饼图为圆形plt.axis(&apos;equal&apos;)plt.show() 箱型图 12345678910111213141516171819import matplotlib.pyplot as plt def draw_plot(data, edge_color, fill_color): bp = ax.boxplot(data, patch_artist=True) for element in [&apos;boxes&apos;, &apos;whiskers&apos;, &apos;fliers&apos;, &apos;means&apos;, &apos;medians&apos;, &apos;caps&apos;]: plt.setp(bp[element], color=edge_color) for patch in bp[&apos;boxes&apos;]: patch.set(facecolor=fill_color) example_data1 = [[1,2,0.8], [0.5,2,2], [3,2,1]] example_data2 = [[5,3, 4], [6,4,3,8], [6,4,9]] fig, ax = plt.subplots() draw_plot(example_data1, &apos;red&apos;, &apos;tan&apos;) draw_plot(example_data2, &apos;blue&apos;, &apos;cyan&apos;) ax.set_ylim(0, 10) plt.show() 对数据位置和变异程度的度量 均值 12import numpy as npa = [1, 3, 3, 4, 5, 7, 7, 15, 15, 15] 12# 均值np.mean(a) 中位数 1np.median(a) 众数 12345678#方法1# np.bincount, 计算非负的int数组中，每个值出现的次数counts = np.bincount(a)#[0 1 0 2 1 1 0 2 0 0 0 0 0 0 0 3]#返回沿轴最大值的索引np.argmax(counts) 123# 方法2from scipy import statsstats.mode(a)[0][0] 极差 1b = max(a) - min(a) 均值 总体均值： $\mu = \frac{\sum_{i=1}^{N}{x_{i}}}{N}$ 样本均值：$\frac{}{x} = \frac{ \sum_{i=1}^{n}{x_{i}}}{n}$ 方差 总体方差： $\sigma ^{2} = \frac{ \sum_{i=1}^{N}(x_{i}-\mu)^{2}}{N}$ 样本方差 总体方差一般很难求出， 因为你没有办法获得总体数据。 但样本方差是可以求出的。对样本来进行分析， 从而估计出总体参数 $s^{2} = \frac{ \sum_{i=1}^{n}(x_{i}-\frac{}{x})^{2}}{n}$ 用样本方差来估计总体方差， 通常会低估总体方差的,所以我们要用这样的公式： $s_{n-1}^{2} = \frac{ \sum_{i=1}^{n}(x_{i}-\frac{}{x})^{2}}{n-1}$ 标准差 总体标准差 $\sigma = \sqrt{\sigma ^{2}} =\sqrt{\frac{ \sum_{i=1}^{N}(x_{i}-\mu)^{2}}{N}} $ 样本标准差$ s = \sqrt{s_{n-1}^{2}} = \sqrt{\frac{ \sum_{i=1}^{n}(x_{i}-\frac{}{x})^{2}}{n-1}}$ 标准差 比 方差 更容易解释， 因为标准差与数据的单位相同 对分布形态， 相对位置的度量 偏度 偏度 ：$\frac{n}{(n-1)(n-2)} * \sum (\frac{x_{i}-\frac{ }{x}}{s})^{3}$ 左偏， 偏度为负数， 右偏，偏度为正数，数据对称，偏度为0 当数据严重偏离时， 中位数是位置的首选度量。 z-分数 z-分数：$\frac{x_{i}-\frac{ }{x}}{s}$ 衡量数据对于平均值的相对位置， 比如z-分数为-1.5， 则表示此数据比平均值小1.5个标准差 切比雪夫定理 定理： 与平均值的距离在z个标准差之类的数据项所占比例至少为$(1-1/z^{2})$， z大于1。 当z为2,3和4个标准差时： 至少75%的数值与平均数的距离在z=2 个标准差之内 至少89%的数与平均数的距离在z=3个标准差之内 至少94%的数与平均数的距离在z=4个标准差之内 检验异常值 异常值： 当z-分数小于-3或着大于+3的数值视为异常值。 对两变量之间关系的度量 样本相关系数 相关系数的范围是-1~+1， 当数为0时， 线性不相关 $r _{xy} = \frac{ s _{xy}}{s _{x}s _{y}}$ 样本协方差：$s_{xy} = \frac{\sum (x_{i} - \frac{ }{x})(y_{i} - \frac{ }{y})}{n-1}$ x的标准差：$s _{x}$ 用python进行计算 12345a = [1,2,3,4]b = [2,4, 6,9]import numpy as npnp.corrcoef([a,b]) 1234import scipy.stats as statsstats.pearsonr(a,b)# 结果的第一个数为相关系数 123456import pandas as pddf= pd.DataFrame()df[&apos;a&apos;] = [1,2,3,4]df[&apos;b&apos;] = [2,4, 6,9]df.corr() 概率 组合 从N项中任取n项的组合 $c_{n}^{N} = \frac{N!}{n!(N-n)!}$ 用python来进行计算 123from scipy.special import comb, permcomb(5,2) 排列 从N项中任取n项的排列数 $P_{n}^{N} = \frac{N!}{(N-n)!}$ 用python来进行计算 123from scipy.special import comb, permperm(5,2) 概率的基本性质 事件的补$P(A) = 1 - P (A^{c})$ 事件的并 $P(A\bigcup B ) = P(A) + P(A) - P(A\bigcap B )$ 互斥事件 $P(A\bigcup B ) = P(A) + P(A)$ 条件概率 在事件B发生的条件下， A条件发生的概率 $P(A|B ) = \frac{P(A\bigcap B)}{P(B)}$ $P(A\bigcap B) = P(B)P(A|B) = P(A)P(B|A)$ 独立事件 两个事件A和B是相互独立的 $P(A|B) = P(A) $ $P(A\bigcap B) = P(A) P(B)$ 贝叶斯定理 计算方法： 先假定一个概率， 然后根据样本获得新的信息， 根据这些信息对 原先假设的概率进行修正， 得到准确的概率。 $P(A|B) = \frac{P(A) * P(B|A)}{P(B)}$ 数学之美番外篇：平凡而又神奇的贝叶斯方法 贝叶斯奥卡姆剃刀《数学之美》第24章《决策与判断》Machine Learning, a Probabilistic Perspective 贝叶斯推断及其互联网应用（一）：定理简介 贝叶斯学习与未来人工智能 《统计学关我什么事》 快速理解贝叶斯定理假设一家商城， 顾客分为： 想买商品的顾客，和随便逛逛的顾客。 假设， 随机走进来一个顾客，他为有意愿度的顾客占20%， 为随便逛逛的顾客占80%。 现在增加了一个主动询问店员的动作。 假设有意愿度购买的顾客， 向店员询问的概率为70%， 不询问的概率为30%。随便逛逛的客户， 主动询问店员的购买概率为 10%， 不询问的概率为 90%。 现在问 如果一顾客主动向店员询问， 那么他是有意愿购买的顾客的概率是多少。 有意向且询问的概率为 14%。 有意向不询问的概率为 6% 无意向且询问的概率为 8%， 无意向且不询问的概率为 72% 现在 主动询问这个动作已经做出了， 所以总体为两部分： 有意向且询问， 和无意向且询问 14:8 = 7:4。 所以她有意向且愿意购买的概率为 7/11 63.6% 如何判断她喜欢你的概率 贝叶斯定理 假设 你是一名女生， 在情人节这天， 一名男生送给你一盒巧克力， 你可能会有疑问，他是不是喜欢你， 他喜欢你的概率是多大？ 因为你没有证据来说明你就是他喜欢的类型， 所以， 我们假设 你或者是他喜欢的类型， 或者是一名路人。 假设各有50%的概率。 通过调查， 我们发现 一个男生对心意女生送出巧克力的概率为 45% 对路人送出巧克力的概率 为 20%， 那他现在送给你一盒巧克力， 在他已经送你巧克力的这件事情已经确定了， 所以他们现在是一个整体他喜欢你的概率是多少呢。 0.5 0.45 0.5 0.2 0.225 0.1 225 ： 100 225 / 325 = 69.2% 的概率会喜欢你 当然， 贝叶斯概率 能够计算出这个概率， 但是否要继续， 取决与你。 贝叶斯概率在我们生活中的应用是非常广泛的， 常见的， 比如 在邮件中的反垃圾邮件。 这是一种贝叶斯概率非常好的使用案例。 在上面的推理中， 我们总会觉得贝叶斯定理有些“牵强”牵强的原因主要是因为先验概率。 这种主观上假定或者大概的概率， 会让人感觉牵强。 但也正是由于设定了先验概率， 贝叶斯定理才会有即是只有少量信息， 也能够进行推理。 当然， 贝叶斯定理有另一学习功能， 就是信息越多， 推理结果就越精确。 离散型概率分布 离散型概率函数的基本条件： $f(x) \geqslant 0$ $\sum f(x) = 1$ 离散型随机变量的数学期望：$E(x) = \mu \approx \sum xf(x)$ 比如：掷一枚公平的六面骰子，其每次“点数”的期望值是3.5 方差$Var(x) = \delta ^{2} = \sum (x-\mu )^{2} f(x)$ 连续性概率分布 离散型概率分布和连续性概率分布的区别离散型： 概率函数给出了随机变量x取某个特定值的概率 连续性： 概率密度函数， 通过面积给出该区间取值的概率。 正态概率分布 正态分布的最高点在均值处， 标准差决定了曲线的宽度和平坦度。 正态分布曲线的总面积为1 标准正态分布 均值为0，且标准差为1]]></content>
      <categories>
        <category>数据分析</category>
        <category>统计学</category>
      </categories>
      <tags>
        <tag>统计学</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[要写的文章——目标]]></title>
    <url>%2F2018%2F10%2F18%2F%E8%A6%81%E5%86%99%E7%9A%84%E6%96%87%E7%AB%A0%2F</url>
    <content type="text"><![CDATA[目标 统计学 反思日记 注意力 OKR工作法 给自己找乐子的能力 正能量 知识付费 贝叶斯定理 普瑞马法则]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>文章构思</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[逻辑系统]]></title>
    <url>%2F2018%2F10%2F14%2F%E9%80%BB%E8%BE%91%E6%80%9D%E7%BB%B4%2F</url>
    <content type="text"><![CDATA[《精准表达：让你的方案在最短的时间内打动人心》 逻辑思考能力， 语言组织能力 为什么说话要讲逻辑为了让任何人都能够听懂并接受我们的方案。对方与我们的文化背景大不相同， 或者与我们持相反一件， 不注意讲话的逻辑， 对方可能听不懂我们的方案。 什么是逻辑把语言合理的组织起来 有逻辑的讲话分为哪些方面 纵向逻辑 因为A， 所以B。 因为B， 所以C。 横向逻辑 - MECE 分析法 A包含B和C。 缺乏逻辑性： 缺乏纵向逻辑 - 真的是这样吗 缺乏横向逻辑 - 仅仅是这样吗 真的是这样吗？纵向逻辑组织的不好， 因果关系比较薄弱。 当对方要求你仔细解释你的纵向逻辑时， 你能回答出来。 做到针对对方的提问能够详细的做出解释。 真的是这样吗？未能合理把握整体思路， 出现了遗漏和重复。 覆盖范围广、分类细致。 纵向逻辑薄弱的原因 前提条件不同因为A， 所以B。 但是A 包含 A1, A2, A3, A4等隐形前提。 要做到有逻辑性地讲话， 应该先质疑自己的前提条件。想想一个与自己前提条件大不相同的人， 设想他可能会提出怎么的问题。 发现自己的隐形前提。 把不同性质的东西混为一谈因为A, 所以B。 但A的范围里包含了A1、A2、A、3 等不同性质的东西 反省自己的额言论， 看自己是否把不同东西混为一谈了。 是否有必要把这件事细致分类， 再进行讨论。 偶然的必然化因为A， 所以B。 但是A到B之间原因太过跳跃， 让听众以为是偶然事件。 思考哪些印因素可能会破坏因果关系，先想象一下具体情形， 然后按照顺序想想最坏的情形。 如何加强横向逻辑 体会语言中的层次感把不同角度的概念拉拢到同一平面。 然后再讨论遗漏与重复的重复等问题。 如何把不同角度的概念拉到同一平面？ 面对同一概念， 不同的人有不同的看法。 面对同一概念， 不同的人会从不同的切入点开始展开联想。 首先， 确认对方是以何种立场还思考问题的。其次， 确认对方设想的是何种场景。 如何实现MECE状态？ 使用架构5p, 营销理论3c, 战略模型7S模型SPRO, SPCO模型AIDAS原理PDCA循环根据具体情况采用合理架构。 避免遗漏六维度理论常见的三个维度，维度4, 时间的流动。维度5， 信息、电力、交易等物质的流动维度6， 人们的心情和习惯 消除重复 当我们同时具备了 横向逻辑 和 纵向逻辑， 就构建了金字塔结构。 幸存偏误在生活中更容易看到成功者的故事，看不到失败， 你会系统性地高估成功的概率。 游泳选手身材错觉你看到游泳选手的身材好，因此你觉得游泳就可以练就这样的身材。你觉得用这个护肤品的模特都好漂亮， 因此你觉得用了这个护肤品自己也可以变漂亮。其实是因为他们有这样的好身材， 所以他们才能被选为游泳运动员。 和游泳能够练就这样的身材没有直接的关系。用这个护肤品的模特好看， 是因为这个模特好看， 所以她才能被选做拍这个护肤品的广告， 和这个护肤品可以让自己更漂亮没有直接关系。 过度自信效应系统性地高估自己的学识和能力过度自信会令你忽视你真正掌握的能力与你已知的知识之间的区别这个怎么理解呢， 就是你很多知识， 你只是了解， 而并非已经内化成你的能力， 而你会错误的把它归为你能力的一部分。 比如你说你会python, 但当你真正写的时候， 你却写不出来， 其实你没有掌握 诉诸结果依据某观点成立所产生的结果好坏，来判断一个观点是否正确。 稻草人谬误有意地模仿他人观点，以达到攻击模仿出来的观点而非实际论点的目的。 诉诸无关权威诉诸不是问题专家的人， 其观点更可能是错误的。 例如：信仰中医，而不相信现代医学 虚假两难给出一个有两种范畴组成的有限集合， 并假设讨论范围内的一切事物都必须属于该集合。因此若拒绝其中一个范畴，便只好接受另一个。例如： 世界上只有两种人，男人和女人， 你既然不是女人， 那么你一定是男人。 人身攻击通过攻击一个人本身，而不是攻击其论点，以转移讨论话题，最终达到诋毁其论点的目的。“侮辱性人身攻击” ： 你又不是专家， 你有什么资格发言“处境类人身攻击” ： 对他们的动机作出判断。 你又不是真正关心降低城市犯罪率， 你只是想要人们投票给你。 循环论证你完全错了， 因为你说得没有道理。你应该相信上帝， 否则你会下地狱。 从众心理 纠缠于沉迷成本我已经买了电影票了， 电影再烂也要看完。我已经在这段恋情中投入了那么多感情， 现在离开她是不明智的。我已经在这支股票中投了那么多钱了， 现在就收手会亏本的， 说不定以后会涨的。 你应该看的是现在的形势以及你对未来的评估。 互惠偏误先送你一束花， 然后让你捐赠。]]></content>
      <categories>
        <category>个人系统</category>
        <category>逻辑</category>
      </categories>
      <tags>
        <tag>逻辑</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习系统]]></title>
    <url>%2F2018%2F10%2F12%2F%E5%AD%A6%E4%B9%A0%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[阅读《好好学习》 如何写反思日记： 什么是反思反思的实质是对假设进行校正做事的顺序：做出假设——采取行动——产生结果反思的顺序： 观察结果——研究原先的假设——反思校正假设 反思的作用 发现知识误区——跳跃性假设 没有通过深入思考就得出结论，比如：遇到大牛就躲闪一旁。 跳跃性假设帮我们选择了思考路径。而缺乏深入思考的过程，又让我们进一步失去了发现解决问题的方法。 如何解决：要深入思考如何深入思考：通过提问来放慢思考速度 提什么问题 今天我做得不好的事情是什么？ 我当时是怎么考虑的？ 如果我重新来做会有哪些改进？ 促进已有知识产生新知识 在反思的时候主动的进行知识的联想和联结，将生活中其他经历和经验串联起来， 重新认识和审视自己过往经历，能够将自己分散的生活经验进行重新组织， 从而产生新的知识。 如何做 我过去还遇到过这样的事情吗？ 我还听过有其他人经历的同样的事情吗？ 有什么相关的方法可以应用到这件事情中吗 检验学习到的新知识是否用了起来 对标管理：提前设定一个标准， 然后每天反思， 与之比较寻找差距 培养反思的额方法 从小事反思，深入突破 把生活案例化处理 培养记反思日记的习惯 每日反思问题： 今天我做得不好的事情是什么？ 我当时是怎么考虑的， 身体是怎么反应的？ 如果我重新来做会有哪些改进？ 我过去还遇到过这样的事情吗？ 我还听过有其他人经历同样的事情吗？ 有什么相关的方法可以应用到这种事情中吗？ 我今天没有有效控制自己的事情是什么？ 我今天在心理上感觉很好的2件事情是什么？ 我今天学到的新技能时什么？ 我离我的职业目标还差哪些技能？ 如何以教带学 重复三次左右会面临一个临界点， 度过这后就很容易继续了。 刻意练习是痛苦的过程。 如何保证高效利用好时间？学习仪式感：基于这种仪式感， 给自己带来一种强烈的自我暗示——利用这种暗示， 把自己的专注力、反应能力、运动能力迅速提升。 物质准备： 水， 巧克力， 电脑 精神准备： 第二天高效学习， 第一天晚早早睡去。 时间准备： 9点正式学习， 9点之前提前赶到图书馆 预热：翻看即将到来的两个小时之内需要学习的内容， 心里有个大概。 静心：在9点之前的一分钟， 会盖上书本， 静等一分钟的流逝， 八点一到， 就带着喜悦的心情静静地翻看书本。 必须做的5件事 想尽办法，跟牛人在一起 在跟牛人的交流中， 能迅速打开视野，增长见识， 还能交到朋友。经常交流的人， 对你有很大的反向塑造里，想成为牛人，就一定要常见牛人，付费约见是一种很好的方式。 读以致用， 好书至少读十篇 一定要每天坚持读书， 哪怕每天只读5页 输出， 读完之后， 需要总结，写出文章来，没有输出的输入，质量是不行的。 好书读十遍，遇到好书， 一定要反复读 读以致用， 成长的关键在于干事情，并且做出成绩来，读完就要用 做人大方，持续真诚地利他如果你是个持续大方的人，别人就会知道，跟你合作、和你交往，不会吃亏，朋友、机会才会越来越多 视时如命，有极强的抗扰力主动见想见的人，去做该做的事情，但不要被别人打乱你的节奏。 持续分享，传播有用的内容想要打造个人品牌，持续对外输出有用的内容，是至关重要的事情。 付费学习 门槛低的群，加了不要在里面混，要不然就是浪费生命 收费高的地方，听课是其次，观模式、交朋友、盘资源会被大家放在更重要的位置 选择老师的时候，一定要看他是否在相关领域做出不错的成绩 一定要跨领域，不要只是在某个领域的各种学习群里学，这会导致你的视野受限 要学会重点展示你的独特价值， 在自我介绍环节，你要想想你有哪些独特的价值，优势，是大家所不具备的，然后持续重点展现，这样做，会让你更容易脱颖而出。 最好是大家能够嘻嘻哈哈的玩在一起，这样才能有利于关系的发展，如果都端着，只能证明信任还不够，感觉还没到位。 要不卑不亢，跟大家平等沟通，哪怕你比他要穷的多。傲和怂，都是很损个人品牌的。 将利他作为一种习惯，利他，才是最好的利己 不管你的学历如何，要多读书，多听优秀的人分享，提高自己的写和说的能力 知识转换为付费 通过坚持做一件非常有用的事情，提升自己，并影响他们和帮助他人，是打造个人品牌的“捷径” 有了良好的个人品牌，增加收入只是顺带的事情 当学不进去的时候， 用【普瑞马法则】在学习和生活中，我们有时会想要做某件事情，但过了好久发现还是没有做， 或者觉得力气使不出来，生活好像配一种惰性缠绕，知道那不好，却又不知道从何入手来改变。 我们可以利用普瑞马法则， 把一件困难的事情放在容易完成的事情前面去做， 把工作中不想完成的事情放在前面完成，这样做不仅强化了工作能力， 增强效率， 同事也让人们意识到， 困难的任务其实也没有那么的困难。 这是一种强化作用的方式——先处理困难的事情， 再处理不那么困难的事情，那是一种对于前面行动的强化，然后继续， 强化的效果会越来越大，一直大到觉得有力量来完成任何事情。 例如，先可以用一天到两天时间给自己做一个行为记录， 把每天要做的事情记下来，大概十几件，然后把这些事情按照兴趣排列，把最不喜欢的事情放在第一位， 把最喜欢的事情放在最后一位。这样在以后的一周，每天一早起来，从最不喜欢的事情开始做起，并且坚持做完第一件事情，再做第二件事情….一直做到最后一件喜欢的事情。 通过结束懒惰的行为，从而结束懒惰的心理。 普瑞马法则今日要做的事情： 思考用户生命周期 考虑设计应用：用户访问触点，依据用户行为轨迹建模，第二天短信推送或者apppush做相应出点或者活动等推荐，结合目前的活动和业务，看能做哪些场景 锻炼， 跑步 写作 英语阅读 写反思日记 吃饭 玩游戏 看《别闹了，费曼先生》 打电话 排序： 思考用户生命周期 考虑设计应用：用户访问触点，依据用户行为轨迹建模，第二天短信推送或者apppush做相应出点或者活动等推荐，结合目前的活动和业务，看能做哪些场景 英语阅读 写作 锻炼， 跑步 写反思日记 吃饭 看《别闹了，费曼先生》 打电话 玩游戏 《好好学习》第二遍笔记： 什么是知识？有能够改变你行动的信息才是知识。 如何判断你掌握了知识？是否根据学习到的知识改变了你的思考和行动 个人知识学习的三个维度：知识数量， 知识获取速度，知识深度 知识管理的核心通过管理知识来提升我们的认知深度，进而改变我们的行为模式 什么是认知深度？ 分析问题时，能够找出具体现象背后的抽象规律 寻找答案时，结论依托于实验验证或数据支持 什么是技术效率和认知效率？ 学习在不断掌握应对具体工作场景和问题的方法，就是在提升技术效率 学习了解问题本质，了解解决方案的底层规律，能够让我们认清问题表象背后的实质，就是在提升认证效率 学习那些知识能够提升认知效率 执行能力： 时间管理，精力管理，沟通技巧 专业能力：解决系统问题的能力 结构能力：认知事物底层规律 读书，如何跳出低水平的勤奋？ 将学到的新知识和已有知识进行联系，构成知识网络。 描述读书后受启发的内容，这些启发和过去的哪些经验相关 当要解决某个问题的时候，主动寻找相关的文章和书籍。 观察作者是用什么样的思路来解决问题？在这个解决方案背后，是否有我熟悉的知识？我还能把这个解决方案的原理， 应用在什么领域？ 学习知识应该具备的两个心态 绿灯思维：积极地考虑新观点里有价值的地方,而不是一味的抵触。 以慢为快：结硬寨，打呆仗。打通知识的阻塞，实现融会贯通。 提升学习能力的三个底层方法 反思： 观察结果——研究原先假设——反思校正假设， 反思日记 以教为学： 把教别人的过程作为帮助自己学习的过程 刻意练习： 对基本核心知识划小圈——对基本概念，知识堵塞等关键地方反复探究和思考将基本知识组合成更大的知识能力单元——和其他相关知识组合成一个新的知识能力单元在各知识能力单元之间构建认知框架——依靠模型组成的框选来安排你的经验 提升学习能力的三个技巧： 记录——如实地记录整个事情的发展过程 定期回顾： 周/月回顾：微观审视解决问题的假设和效果年度回顾：检视基本思考方式5年以上回顾：探寻基本规律如何影响生活 付费购买： 买书是为了更快速地寻找问题可能的解决方案买时间 如何发现自己的临界知识 从自己感兴趣的领域入手，学习这个学习的重要知识。 找到最重要的知识和原理的原始出处 尝试用更加基本的原理来解释这个知识 没有解释的时候，想办法寻找或者自己创造个假设，并验证 13 如何应用临界知识 刻意练习 在不同的场景中，重复应用同一个临界知识在不同的时间里，重复应用同一个临界知识]]></content>
      <categories>
        <category>个人系统</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[写作系统]]></title>
    <url>%2F2018%2F10%2F10%2F%E5%86%99%E4%BD%9C%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[写作准备： 从生活中想到一个你想要写的主题。 在日常中思考素材， 注意素材的积累。 想清楚你想表达的思想是什么， 用一句话表达清楚 写作， 是自我对话， 认识自己的一种方式。 写作的本质， 情绪 与 传递 文章风格： 逻辑 + 生活 + 有趣 写： 生活 —— 素材库 —— 观点库（逻辑，情绪） —— 结构库 —— 草稿库 检查： 逻辑是否清晰， 是否有自己的观点， 是否有趣 排版， 标题， 错别字 写作模板： 这篇文章主要表达什么思想， 用两句话来说清楚。 自己准备的素材都有哪些 自己选择什么布局？ 从证据到结论 从结论到证据 将第二重要的内容放首位， 最重要的内容放最后 写文章引言 提出一个现象， 或者一个有争议的观点， 由此导出你的观点。 讲一则简洁的故事， 这个故事能够很好地阐述你将要表达的观点 撰写文章的结论 简明进行综合， 告诉读者为什么应该赞同你的观点 对引言的故事加以评论 记住哪些观点需要论证， 以及你将如何论证 提供高质量的伦军来支撑自己的观点， 检查每一个观点和证据， 用以下各类方式呈现出来 提供事实性的细节， 比如数据 描述人或事 提供一篇文章或一本书的总结 引用或转述某个学者的观点 对真实或假设性事件简明或扩展地叙述 追溯历史发展 呈现字面上或者深层次的含义 细化某一过程或程序 解释相似性和差异性的联系与区别 分析原因或产生的影响 评价某人的观点 阐述其优点和不足 根据以上步骤写出你的草稿 修改用批判的眼光阅读初稿， 大声朗读出来。问自己以下问题： 我可以做什么上中心思想表达更清楚 哪个段落需要做进一步全面解释，以便上读者了解深意 怎样增强文章连贯性 用哪种形式编排句子和段落， 让文中思想更容易理解 我需要在哪加上过度聚， 用来标志转入下一个观点 哪些想法需要重点强调， 哪些可以一笔带过 怎样更好地强调重点， 我是否没有找到更充足的证据来支持观点 编辑找出表改正那些在语法、错字， 标点、加粗等问题 努力达到的标准 让文字看起来很自然 力求简洁 用简单的语言来表达你的观点 让你的句子富于变化 尝试不同的句子， 试着变化结构， 学习长短句交叉使用 让文字生动起来 尽量把你的想法用有想象力和有个人色彩的语言表现出来 好的写作能力， 对个人发展的赋能实在太大了，写作这件事，你需要突破 下笔困难，写不出什么好东西来，最大的问题在于输入的质和量跟不上 练习很重要，你可以每天写几十个字，每隔一段时间写一篇长点的 起步阶段，把某个你特别喜欢的作者的书籍，都找过来，逐字读5遍，你会发现，自己的表达风格，会有他的烙印。 追求简洁流畅、表达精准、有说服力 参考书籍：《思考的艺术》《金字塔原理》 有效写作的四个特点： 统一协调 用一两句话来清晰地表达你的想法，这篇文章的中心思想，指导你后面的写作。 连贯一致 明白观点的先后顺序， 当你转入另一观点时， 一定要将前一观点表述清楚。同时用一些连接词进行连贯的过渡。 重点突出 对重要观点进行详细阐述 将观点按重要程度进行排序，最重要的观点放在最后，次重要的论点放在开头，另外两个论点放中间。 拓展升华 详细阐述重要的观点，强调的点要能够引发读者的共鸣，可以使用例证、描述、定义、释义等方法 写作的阶段渐进法计划 整合观点在产生和评价了自己的想法之后，你就需要整理下自己要用到的材料。想法、笔记、他人观点等 选择布局 从结论到证据 先呈现结论，再通过例证来支持结论。 从证据到结论 让读者逐步接受你的结论。适用于辩驳那种广为人知或根深蒂固的观点。 从原因到结果 先讨论产生某一现象的原因，然后说明这一现象的影响。从结论到原因 重要性顺序 将第二重要的内容放在首位，最重要的内容放在最后 撰写文章引言的方法 提出一些突出的问题，并讨论可能的答案 讲一则简洁的故事，这则故事能够很好地阐述你将要表达的观点 引用名人名言，从而导入你的中心思想 提出一个你不同意的观点，由此导入你的观点 撰写文章结论的方法 根据内容中提到的争议提出一个可行方案 回答引言提出的问题，或对引言和故事加以评论 利用新的引言来深化中心思想 描述一则新故事，确保这则故事能够强化你的中心思想，并且不会带入新的问题 阐述采纳你观点的益处和不采纳你观点的害处 简明扼要地进行统合， 告诉读者为什么应该赞同你的观点。 哪些观点需要进行论证以及你将如何进行论证 写作系统-辉哥 2015年之前，随便写写2015-2018年， 为终身事业而写18年——至今， 日更 什么是写作系统 为什么要建立写作系统 怎么建立写作系统 写作技能： 要专注哪个领域个人成长 为什么要选择这个领域我擅长， 大家需要 写作技能要怎样才能持续提升大量的阅读相关的书籍， 大量的实践， 通过阅读和实践， 总结出自己的个人成长系统，并通过足够多的案例， 让自己的成长系统本身去迭代。 读者社群： 选择哪类读者对未来抱有希望，但是在现实中遭遇困惑的人。 为什么要选择这类读者从他们看到身上看到自己的影子 怎样获得更多匹配的读者持续就这个领域写出更好的作品， 出版书籍， 获取更大的认可。 怎样才能更好的满足读者在这个领域更好的需求自己更好的成长，并对成长系统梳理的更加清晰。 商业： 如何从写作中获益让读者获益， 并让读者支付会员费。 怎样获得持续稳定的收益在会员体系中提供稳定的， 并持续增加的正向收益，并获得很好的口碑， 鼓励会员进行链接。 怎样不断提升收益更好的作品，更多的读者，更好的引导转化 梳理自己的写作系统 驱动写作的一页纸：使命：愿景：价值观： 写作系统的目标：关键结果： 与写作相关的微习惯：每日阅读：每日写作： 如何判断自己的文章好坏-建议： 多写 这篇文章对读者是否真的有用 写作是双向沟通而不是单向表达 你能理解多少人，就能拥有多少读者。 内容质量判断： 逻辑严谨 论证精彩 逻辑推理的三个层面： 形式逻辑-formal logic 非形式逻辑-informatl logic 认知偏差纠正-coginitive bias correction 《thinking fase and slow》 cognitive bias list of cognitive bias 精彩的例子是攒出来的，不是找出来的。 《successe quation》 文采的判断： 修辞——类比 韵律—— 格式化写作： 提出一个观点 说明这个观点的意义究竟有多大 证明这个观点 驳斥对这个观点的质疑 说清楚这个观点的超级意义 我要说的是什么概念 这个概念为什么重要 这个概念普遍被如何误解 这个概念实际有什么意义 正确理解这个概念有什么意义 如何正确理解这个概念 错误使用这个概念有什么可怕之处 这个概念与什么其他重要的概念有重要的联系 《the walking dead》 写作的本质， 为了产生影响。我的读者读过之后有什么样的变化？ 《ask right questions》]]></content>
      <categories>
        <category>个人系统</category>
        <category>写作</category>
      </categories>
      <tags>
        <tag>写作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[阅读系统]]></title>
    <url>%2F2018%2F10%2F10%2F%E9%98%85%E8%AF%BB%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[困惑在日常的读书学习中，自己总是会出现这样的困惑， 就是发现自己看了很多书， 但是看完就忘。看完一本书也很难用其中的论点指导自己的生活，难道自己陷入看了那么多书，却依然过不好这一生的困境中吗？ 借鉴在最近翻阅的一本书《万万没想到-用理工科思维理解世界》中， 万维钢老师写的一篇【用强力研读书】中提出的读书观点，自己很受启发。在这篇文章中， 作者主要强调了读书要做到两点： 1. 一本好的书籍最少要读两遍2. 第二遍读书的时候一定要做笔记 反思自己的读书方法，就能够看到自己过去读书的很多问题: 自己看书为了追求看完， 总是囫囵吞枣的看， 看完一遍就将书扔到一边。 在看书过程中即使自己会有一些思考， 自己也不会马上记笔记将自己的思考写下来， 没有将书中的观点和知识内化到自己的系统中， 在以后遇到类似的知识也不会进行升级， 因为自己已经将这个知识忘光了。 方法当然，记笔记并不是将书中的总结性内容抄到笔记本上就完事了，在这篇文章中， 万维钢老师还详细的介绍了如何做才能让自己的读书效率最大化: 读好书一定要读两遍。第一遍是陷进去，按作者的思路去读。 第二遍跳出来， 抓住文章的精髓仔细读， 做笔记。 在读第二遍的时候要找出文章的思想脉络。很多时候你没有真正理解书中的内容， 就是因为看不到书中的脉络。因为你作为读者，看书是将用线性的视角来一行一行看的，而作者写一篇文章是将网状的思考， 把树状的结构， 用线性的语言表达出来。 书中每一个小章节的逻辑结构可能就只有几句话。 在你读书过程中要做的就是找出这样的逻辑结构。 当我们看到文章好的亮点时， 我们要把它记下来据为己有，也许是一句话， 也许是一个故事。记笔记， 是当你看到了一个想法之后很激动，必须把这个想法记下来据为己有的行为。 读书，在某种程度上就是寻找能够刺激自己思维的那些亮点， 我们在分析书本脉络的时候要忽略故事，分析完脉络再好的把故事带走。 当我们看书的时候，我们要写下自己对书中内容的评论， 就好像和作者对话一样。你可以对书中的观点表达支持， 质疑， 或者写下这个观点给了你启发— 让你想到了自己最近发生的一件事， 听过的一个故事， 你都可以写下来。 当你看完一本书的时候， 你也许会发现自己在另一本书上又发现相似的论点， 或者你在网上看到了另一个人对这个观点的表达， 这个时候， 你就要将不同地方看到的蕾西知识整理到自己的笔记上， 不断的完善它， 形成自己的观点， 并写文章发表出来。 再引述一下作者的话： 好书之所以要读两遍， 最重要的目的就是为了读书后的心得、灵感和联系。记笔记是对一本书最大的敬意，记笔记是个人知识的延伸。 总结总结文章的观点， 就是读两遍，理脉络，记笔记，写心得，融体系。能够做上述几点，相信你的读书效率一定会大大改善的。 最后，结合自己的经验，我认为如果为了提高自己的认知能力而读书， 就一定要记读书笔记， 要不读书的效率会差很多。要想改变读了那么多书却依然过不好这一生的困境，我觉得可以用自己很喜欢的一句话来进行回答： 知识不会改变命运，除非它带来行动；行动也不会改变命运，除非它带来结果；是一个一个的结果，在改变你的命运； 参考资料：1.《万万没想到-用理工思维看世界》2.《超级个体》 读书三问： 为什么我坚持读书写作很久了，却没有进步 有认真消化吗，自己读了以后，能教别人吗 你所写的内容， 是简单的搬运，还是行动之后的反思总结 不要只是读读写写，你得去做事，事上磨练，才是最好的学习方式 一年要读多少数 你更应该关心的是， 你能践行多少内容]]></content>
      <categories>
        <category>个人系统</category>
        <category>阅读</category>
      </categories>
      <tags>
        <tag>阅读系统</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Pandas 库学习总结]]></title>
    <url>%2F2018%2F09%2F28%2FPandas%E5%BA%93%E5%AD%A6%E4%B9%A0%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[python中的单引号，双引号，三引号的不同作用 Pandas简介Pandas是python数据分析中一个非常核心的数据库， 在日常的工作中经常需要使用Pandas库来对数据进行处理分析。Pandas的核心为两大数据结构， Series和DataFrame，Series用于存储一维数据， 而DataFrame存储多维数据。 常用的软件Anaconda是数据分析中运行python的一款利器， 安装教程可参考Anaconda入门使用指南 Series对象Series用于存储一维数据，由两个相互关联的数组组成， 主数组用来存放数据。主数据每个元素都有一个与之关联的标签，这些标签存储在另一个叫Index的数组中。 创建Series对象 1zy = pd.Series([2, 3, 4, 6, 7, 4], index = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;]) 查看Series对象的两个数组 12345# 查看元素zy.values# 查看索引zy.index 查看元素 1234567# 将zy看做Numpy数组，指定键zy[2]zy[0:2]# 指定标签zy[&apos;c&apos;]zy[[&apos;b&apos;, &apos;c&apos;]] 筛选元素 1zy[zy &gt;3] 查看组成元素 123# 查看包含的不同元素zy.unique()zy.value_counts() 通过字典来创建 1zy=Series(&#123;&apos;a&apos;:1,&apos;b&apos;:2,&apos;c&apos;:3&#125;) DataFrame对象 读取与写入Excel数据 读取文件夹的内容 1234567891011import pandas as pdimport numpy as npfrom pandas import Series, DataFrameimport osfile_list = os.listdir(r&apos;E:\工作文件\周报\周数据\测试\0902-0908&apos;)print(file_list, &apos;\t&apos;)# 读取当前文件夹地址os.getwd() 读取xls格式Excel表 1234df = pd.read_excel(&apos;E:/工作文件/周报/周数据/测试/0902-0908/an-商品汇总-uv.xls&apos;)df = pd.read_excel(r&apos;E:\工作文件\周报\周数据\测试\0902-0908\an-商品汇总-uv.xls&apos;)df = pd.read_excel(r&apos;E:\工作文件\周报\周数据\测试\0902-0908\an-商品汇总-uv.xlsx&apos;) 读取csv格式Excel表 1df = pd.read_csv(&apos;E:/工作文件/周报/周数据/测试/0902-0908/商品汇总.csv&apos;) 读取txt格式数据 123456df = pd.read_table(r&apos;C:\Users\Administrator\Desktop\haha.txt&apos;)with open(r&apos;C:\Users\Administrator\Desktop\haha.txt&apos;, &apos;r&apos;) as f: df = f.readlines() df = np.loadtxt(r&apos;C:\Users\Administrator\Desktop\haha.txt&apos;) # 将txt文件存为numpy数组 将数据写入Excel表， 并输出 123data.to_excel(&apos;C:/Users/Administrator/Desktop/&apos;+&apos;商品分类.xlsx&apos;)data.to_excel(r&apos;C:\Users\Administrator\Desktop\\&apos;+&apos;商品分类.xlsx&apos;)data.to_excel(r&apos;C:\Users\Administrator\Desktop/&apos;+&apos;商品分类.xlsx&apos;) 其他数据格式 1234567891011121314151617181920# 从SQL表/库导入数据 pd.read_sql(query, connection_object)# 从JSON格式的字符串导入数据 pd.read_json(json_string)# 解析URL、字符串或者HTML文件，抽取其中的tables表格 pd.read_html(url)# 从你的粘贴板获取内容，并传给read_table() pd.read_clipboard()# 从字典对象导入数据，Key是列名，Value是数据pd.DataFrame(dict)# 导出数据到SQL表 df.to_sql(table_name, connection_object)# 以Json格式导出数据到文本文件df.to_json(filename) 常见问题： 当文件有中文时， 可能会出现错误：Initializing from file failed 有中文， 可以用此方法解决 12f = open(&apos;我的文件.csv&apos;)res = pd.read_csv(f) 修改数据格式，并存储 1234567import pandas as pdimport numpy as npimport osos.chdir(r&apos;E:\detil_data&apos;)# os.getcwd()datas = pd.read_csv(&apos;a.csv&apos;,encoding=&apos;gbk&apos;)datas.to_csv(&apos;test12.csv&apos;, index=False,encoding=&apos;utf_8_sig&apos; ) 当文件特别大， 1个多G时， 可以用for循环查看数据 12for i in data: print(i) 查看大文件有多少列 1234data = open(&apos;E:/电信活跃用户数/2018-09-01至2018-09-11全国活跃用户明细.csv&apos;)data1 = pd.read_csv(data, iterator=True)data2 = data1.get_chunk(5)print(data2) 迭代器 描述数据 表信息 1df.info() 显示数据的行列数 1df.shape 查看数据格式dtpyes 1df.dtypes 显示列名、元素 12df.columnsdf.values 添加默认列名 12# 如果数据没有标题行，可用pandas添加默认的列名df = pd.read_excel(&apos;x.xlsx&apos;, header = None) 显示前数据前5行 12df.head(5)df[[&apos;标题&apos;, &apos;客户端uv&apos;]].head() 显示数据后5行 1df.tail(5) 值 1df.values 读取a列 1df[&apos;a&apos;] 修改索引 1df = df.set_index[&apos;标题&apos;] 显示数据唯一值（unique函数） 12# 数据有0， 是因对缺失值进行了填充df[&apos;经纪人级别&apos;].unique() 对第几行数据不读取 12#不读取哪里数据，可用skiprows=[i]，跳过文件的第i行不读取df = pd.read_excel(&apos;x.xlsx&apos;,skiprows=[2] ) 对缺失值进行识别 12# 所有缺失值显示为Truepd.insull(df) # df.isnull() 计算 1234567891011#计算此data的数量df[&apos;data&apos;].value_counts()# 升序计数df[&apos;data&apos;].value_counts(ascending = True)# 升序计数并分组df[&apos;data&apos;].value_counts(ascending = True, bins = 2)# 计数df[&apos;data&apos;].count() 数据清洗 删除空值 （dropna函数） 1df.dropna(how=&apos;any&apos;) 填充空值（fillna函数） 12345# 空值用0填充df.fillna(value=0)# 用均值对空值进行填充df[&apos;经纪人响应时长&apos;].fillna(df[&apos;经纪人响应时长&apos;].mean()) 更改数据格式 12# 将数据格式int64,改为float格式df[&apos;大区&apos;].astype(&apos;float64&apos;) 更改列名称, 修改列名。 123data2.columns = [[&apos;导航&apos;,&apos;uv&apos;, &apos;pv&apos;,&apos;户均点击&apos;]]df.rename(columns=&#123;&apos;IM渠道&apos;: &apos;渠道&apos;&#125;) 找到重复值 1df.duplicated() 删除重复值 https://www.cnblogs.com/cocowool/p/8421997.html 12345# 默认第一次出现的保留，其余删除df[&apos;门店&apos;].drop_duplicates()最后一次出现的保留，其余删除df[&apos;门店&apos;].drop_duplicates(keep = &apos;last&apos;) 对列表内的值进行替换 1df[&apos;客户UCID&apos;].replace(&apos;10531975&apos;, &apos;110&apos;) 找出异常值 12print(data.describe())# 对异常值进行删除 修改数据 1234567891011121314# 修改结果df.replace(参数)# 修改索引df.rename(参数)# 增加, 将一列数据添加到另一列数据后。 df.append(参数)# 删除def df[&apos;a&apos;]df.drop([&apos;a&apos;, &apos;b&apos;], inplace = True) 对数据进行处理 对两个数据进行合并- mearge, join, concat函数 1234567891011121314151617181920212223242526272829303132# 按照轴把多个对象拼接起来pd.concat(df1, df2)# join函数适合根据索引进行合并，合并索引相同但列不同的对象# merge函数，根据一个或多个键连接多行left = pd.DataFrame(&#123;&apos;key&apos;:[&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k3&apos;], &apos;key2&apos; : [&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k3&apos;], &apos;A&apos; :[&apos;ao&apos;,&apos;a1&apos;,&apos;a2&apos;,&apos;a3&apos; ], &apos;B&apos; : [&apos;bo&apos;,&apos;b1&apos;,&apos;b2&apos;,&apos;b3&apos; ]&#125;)right =pd.DataFrame(&#123;&apos;key&apos;:[&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k3&apos;], &apos;key2&apos; : [&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k4&apos;], &apos;c&apos; :[&apos;co&apos;,&apos;c1&apos;,&apos;c2&apos;,&apos;c3&apos; ], &apos;d&apos; : [&apos;do&apos;,&apos;d1&apos;,&apos;d2&apos;,&apos;d3&apos; ]&#125;)# 将left和right进行合并pd.merge(left, right)# 指定以key为键进行合并pd.merge(left, right, on = &apos;key&apos;) pd.merge(name_3, name_1, left_on = [&apos;ming&apos;], right_on = [&apos;标记&apos;])# key2列不相同的部分会直接舍弃掉pd.merge(left, right, on = [&apos;key&apos;, &apos;key2&apos;])# 保留key2列不相同的部分pd.merge(left, right, on = [&apos;key&apos;, &apos;key2&apos;], how = &apos;outer&apos;)# 不相同的部分指定以左表为基准pd.merge(left, right, on = [&apos;key&apos;, &apos;key2&apos;], how = &apos;left&apos;) 将 list 格式转化成 DataFrame 格式 1df = pd.DataFrame(data, columns = [&apos;省份&apos;, &apos;按钮名称&apos;, &apos;uv&apos;, &apos;pv&apos;] ) 对数据进行排序 12345data =pd.DataFrame(&#123; &apos;group&apos;:[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;a&apos;], &apos;data&apos; : [4, 2, 5, 6, 7, 8, 2, 9, 4]&#125;)# 在保证group列降序的情况下，对data列进行升序处理data.sort_values(by = [&apos;group&apos;, &apos;data&apos;],ascending = [False, True], inplace = True) 对数据进行分组——excel中的数据透视表 123456789# 如果price列的值&gt;3000，group列显示high，否则显示lowdf[&apos;group&apos;] = np.where(df[&apos;客户当天发送消息数&apos;] &gt; 5,&apos;high&apos;,&apos;low&apos;)# 对符合多个条件进行分组# 符合经纪人级别为A1且经纪人响应时长&gt;24的在sign列显示为1df.loc[(df[&apos;经纪人级别&apos;] == &apos;A1&apos;) &amp; (df[&apos;经纪人响应时长&apos;]&gt;= 24.0), &apos;sign&apos;]=1 对数据进行分列 12pd.DataFrame((x.split(&apos;网&apos;) for x in df[&apos;客户注册渠道&apos;]), index=df.index,columns=[&apos;客户注册渠道&apos;,&apos;size&apos;]) 新增一列 123data = data.assign(ration = [4, 2, 5, 6, 7, 8, 2, 9, 4])data[&apos;rations&apos;] = [5, 2, 5, 6, 7, 8, 2, 9, 4] 对数据进行切分 12bins = [1,3,6,9]data_cut = pd.cut(data[&apos;data&apos;], bins) 取出的数据， 指定取到小数点几位数？ 123# 取到小数点后3位for i in a : print(&quot;) 对数据进行提取,筛选12df = pd.DataFrame(&#123;&apos;A&apos;:[7,8,9,20, 10, 11, 14, 13, 14], &apos;B&apos; : [1,2,3,4,5, 6, 7, 7, 8]&#125;) 按条件进行提取 1234567891011121314151617181920212223242526272829# 选出B列的值大于3的数df[df[&apos;B&apos;]&gt;3]# 当 A列的值大于13时， 显示B，c列的值df[[&apos;B&apos;,&apos;C&apos;]][df[&apos;A&apos;]&gt;13]# 用isin函数进行判断# 使用isin函数根据特定值筛选记录。筛选A值等于10或者13的记录df[df.A.isin((10, 13))]# 判断经纪人级别是否为A3df[&apos;经纪人级别&apos;].isin([&apos;A3&apos;]) # 先判断结果，将结果为True的提取#先判断经纪人级别列里是否包含A3和M4，然后将复合条件的数据提取出来。df.loc[df[&apos;经纪人级别&apos;].isin([&apos;A3&apos;,&apos;M4&apos;])]# 使用&amp;（并）与| （或）操作符或者特定的函数实现多条件筛选 # A列值大于10， 并且B列值大于5df[(df[&apos;A&apos;] &gt; 10) &amp; (df[&apos;B&apos;] &gt;5)]df[np.logical_and(df[&apos;A&apos;] &gt; 10, df[&apos;B&apos;] &gt; 5)]# A列值大于10，或 B列值大于5df[(df[&apos;A&apos;] &gt; 10) | (df[&apos;C&apos;] &gt;20)]df[np.logical_or(df[&apos;A&apos;] &gt; 10, df[&apos;C&apos;] &gt; 20)] 按索引进行提取 12345678910111213141516171819202122232425262728293031323334353637383940414243# 按标签索引df[1:4]# 传入列名df[[&apos;A&apos;, &apos;B&apos;]]# loc函数# 知道column names 和index(这里df的index没有指定，是默认生成的下标)，且两者都很好输入，可以选择 .loc同时进行行列选择# 根据标签取第一行， 显示为DataFrame格式df.loc[:0]# 取标签为2,3,4， A列的数据， 显示为Series格式df.loc[2:4, &apos;A&apos;]# iloc函数# 行和列都用index来进行提取df.iloc[0:5, 1:3] # 返回第一行 df.iloc[0,:]# 返回第一列的第一个元素df.iloc[0,0]#[0, 2, 5] 代表指定的行，[ 4, 5 ] 代表指定的列df.iloc[[0,2,5],[4,5]]# ix#ix的功能更加强大，参数既可以是索引，也可以是名称，相当于，loc和iloc的合体df.ix[1:3, [&apos;A&apos;, &apos;B&apos;]]# at函数根据指定行index及列label，快速定位DataFrame的元素，选择列时仅支持列名df.at[3, &apos;A&apos;]# iat函数选择时只使用索引参数df.iat[3, 2] 按日期进行提取 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import pandasimport datetime as dt# 重新设置索引df.reset_index()#设置日期为索引df=df.set_index(&apos;日期&apos;)#提取2016年11月2号的数据df[&apos;2016-11-02&apos; : &apos;2016-11-02&apos;]dt_time = dt.datetime(year = 2018, month=9, day = 17, hour = 22, minute = 43)print(dt_time)#构造时间ts = pd.Timestamp(&apos;2018-09-17 22:43:00&apos;)ts = pd.to_datetime(&apos;2018-09-17 22:43:00&apos;)ts = pd.to_datetime(&apos;17/09/2018 22:43:00&apos;)# 月份ts.month#日期ts.day# 加日期ts + pd.Timedelta(&apos; 10 days&apos;)ts.hour# 构造时间序列， 构造十个日期， 每12分钟一次pd.Series(pd.date_range(start = &apos;2018-09-17 22:43:00&apos;, periods = 10, freq = &apos;12min&apos;))读取文件， 有时间列， 先将时间字符串转换成时间格式， 再进行处理或当读取数据时， 就对数据格式进行修改data = pd.read_csv(&apos;.../db.csv&apos;, index_col = 0, parse_dates = True)# 读取时间为2013年的所有数据data[&apos;2013&apos;]# 取所有8点到12点之间的数据, 不包含8点和12点data[(data.index.hour &gt; 8) &amp; (data.index.hour &lt; 12)]# 包含8点到12点data.between_time(&apos;08:00&apos;, &apos;12:00&apos;)# 时间序列的重采样-看每月的平均值data.resample(&apos;M&apos;).mean() 数据汇总 对数据进行分类 - group by函数 123456789101112131415161718192021222324# 创建数组df = pd.DataFrame(&#123;&apos;key&apos; : [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;,&apos;a&apos;, &apos;b&apos;, &apos;c&apos;,&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], &apos;data&apos; : [0, 2, 4, 5, 6, 7, 8, 9, 4]&#125;)# 分别计算a, b, c 的和df.groupby(&apos;key&apos;)[&apos;data&apos;].sum()df.groupby(&apos;key&apos;)[&apos;data&apos;].mean()s = pd.Series([1, 2, 3,1, 2, 3],[8,7,6,8,7,6])# 对索引进行排序grouped = s.groupby(level = 0， sort =False)grouped.first()df2 = pd.DataFrame(&#123;&apos;x&apos;:[&apos;a&apos;, &apos;b&apos;, &apos;a&apos;, &apos;b&apos;], &apos;y&apos; : [1, 2, 3, 4]&#125;)# 只关注x中的bdf3 = df2.groupby([&apos;x&apos;]).get_group(&apos;b&apos;)# 查看个数df2.size() 2, 对数据进行透视, 相当于Excel中的数据透视表功能。12345678910111213pd.pivot_table(data, values=None, index=None, columns=None, aggfunc=&apos;mean&apos;)df = pd.DataFrame(&#123;&quot;A&quot;: [&quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;], &quot;B&quot;: [&quot;one&quot;, &quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;two&quot;, &quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;two&quot;], &quot;C&quot;: [&quot;small&quot;, &quot;large&quot;, &quot;large&quot;, &quot;small&quot;, &quot;small&quot;, &quot;large&quot;, &quot;small&quot;, &quot;small&quot;, &quot;large&quot;], &quot;D&quot;: [1, 2, 2, 3, 3, 4, 5, 6, 7]&#125;)table = pd.pivot_table(df, values=&apos;D&apos;, index=[&apos;A&apos;, &apos;B&apos;], columns=[&apos;C&apos;], aggfunc=np.sum) 对数据进行映射 12# 用map函数对字典进行映射， 新加一列data[&apos;upper&apos;] = data[&apos;group&apos;].map(dataUpper) 数据统计 数据采样 12345678910111213# 简单随机抽取sampledf.sample(n=3)# 设置采样权重# 需要对每一行进行权重设置，列表行数少可行，过多不可行# 假设有4行数据，设置采样权重weights = [0, 0, 0.5, 0.5]df.sample(n=4, weights=weights)## 确定采样后是否放回# 采样后放回，Truedf.sample(n=6, replace=True) 统计计算 1234567891011121314151617181920# 描述统计 describe函数#自动生成数据的数量，均值，标准差等数据#round（2）,显示小数点后面2位数，T转置df.describe().round(2).T# 标准差std()df[&apos;经纪人响应时长&apos;].std()# 协方差covdf[&apos;经纪人当天发送消息数&apos;].cov(df[&apos;客户当天发送消息数&apos;]# 相关性分析corrdf[&apos;客户当天发送消息数&apos;].corr(df[&apos;经纪人当天发送消息数&apos;])# 中位数df.median() 对字符串进行操作 大小写 12a.lower()a.upper() 长度 12# 长度a.len() 去除空格 123a.strip()a.lstrip()alrstrip() 替换 1df.columns.str.replace(&apos; &apos;, &apos;_&apos;) 切分与分列、 合并 123456789101112131415161718#切分a.split(&apos;_&apos;)# 切分， 且成为新列a.split(&apos;_&apos;, expand = True)# 对切分进行限制, 只切1次a.split(&apos;_&apos;, expand = True, n=1)# 查看是否包含a.str.contains(&apos;A&apos;)# 分列s.str.get_dummies(sep= &apos;|&apos;)对两列数据进行合并df[&apos;省份_名称&apos;] = df[&apos;省份&apos;].str.cat(df[&apos;名称&apos;],sep = &apos;_&apos;)]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析常见的术语]]></title>
    <url>%2F2018%2F09%2F27%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[数据分析方法论 SWOT 方法 通过评价企业的优势（Strengths）、劣势（Weaknesses）、竞争市场上的机会（Opportunities）和威胁（Threats），用以在制定企业的发展战略前对企业进行深入全面的分析以及竞争优势的定位 进而需用USED技巧来产出解决方案 如何善用每个优势？ How can we Use each Strength?如何停止每个劣势？ How can we Stop each Weakness?如何成就每个机会？ How can we Exploit each Opportunity?如何抵御每个威胁？ How can we Defend against each Threat? PEST分析法PEST 为一种企业所处宏观环境分析模型，从政治(Politics)、经济(Economy)、社会(Society)、技术(Technology)四个方面分析内外环境，适用于宏观分析。PEST分析与外部总体环境的因素互相结合就可归纳出SWOT分析中的机会与威胁。PEST、SWOT 与 SLEPT 可以作为企业与环境分析的基础工具。 5W2H分析法 （1） WHAT——是什么？目的是什么？做什么工作？（2）WHY——为什么要做？可不可以不做？有没有替代方案？（3）WHO——谁？由谁来做？（4）WHEN——何时？什么时间做？什么时机最适宜？（5） WHERE——何处？在哪里做？（6）HOW ——怎么做？如何提高效率？如何实施？方法是什么？（7） HOW MUCH——多少？做到什么程度？数量如何？质量水平如何？费用产出如何？ 4P理论经典营销理论，认为产品(Product)、价格(Price)、渠道(Place)和促销(Promote)是影响市场的重要因素。 AARRR增长黑客的海盗法则，一种以用户为中心的着眼于转化率的漏斗型的数据收集测量模型，从获取(Acquisition)、激活(Activition)、留存(Retention)、收益(Revenue)和推荐(Referral)5个环节增长。 在整个AARRR模型中，这些量化指标都具有很重要的地位，而且很多指标的影响力是跨多个层次的。及时准确地获取这些指标的具体数据，对于应用的成功运营是必不可少的。 波特五种竞争力分析模型 这五种竞争力就是企业间的竞争、潜在新竞争者的进入、潜在替代品的开发、供应商的议价能力、购买者的议价能力。这五种竞争力量决定了企业的盈利能力和水平。 战略地位与行动评价矩阵SPACE矩阵有四个象限分别表示企业采取的进取、保守、防御和竞争四种战略模式。这个矩阵的两个数轴分别代表了企业的两个内部因素——财务优势（FS）和竞争优势（CA）；两个外部因素——环境稳定性（ES）和产业优势（IS）。 SCP分析模型SCP（structure、conduct、performance）模型，分析在行业或者企业收到表面冲击时，可能的战略调整及行为变化。 SCP模型从对特定行业结构、企业行为和经营结果三个角度来分析外部冲击的影响 逻辑树分析法战略分析常用的一个场景是新孵化项目的市场可行性分析，要解决的问题比较明确：新市场我们是否可以切入？如何进入？开始的时候我们可以运用逻辑树把问题拆分成几个子议题，并据此制定项目具体计划，识别核心关键问题，逐一分析 基于用户生命周期的数据分析体系 RFM分析客户数据库中有三个要素：R（Recency）、F（Frequency）、M（Monetary）。 麦肯锡七步分析法麦肯锡七步分析法又称“七步分析法”是麦肯锡公司根据他们做过的大量案例，总结出的一套对商业机遇的分析方法。它是一种在实际运用中，对新创公司及成熟公司都很重要的思维、工作方法。 界定问题- 将问题分解成议题- 去除不重要的议题（优先排序）- 制定详细工作计划 - 分析重要议题 - 汇总研究成果 - 准备你的故事 用户行为决策分析模AISAS 模型是目前最通用的“用户决策行为分析模型注意- 兴趣-搜索-行动-分享 KANO模型将影响用户满意度的因素划分为五个类型，包括： 魅力因素：用户意想不到的，如果不提供此需求，用户满意度不会降低，但当提供此需求，用户满意度会有很大提升;期望因素(一维因素)：当提供此需求，用户满意度会提升，当不提供此需求，用户满意度会降低;必备因素：当优化此需求，用户满意度不会提升，当不提供此需求，用户满意度会大幅降低;无差异因素：无论提供或不提供此需求，用户满意度都不会有改变，用户根本不在意;反向因素：用户根本都没有此需求，提供后用户满意度反而会下降; PDCA 它是单词Plan、Do、Check和Action的首字母缩写。分别代表计划、执行、检查、处理。P (Plan) 计划：方针和目标的确定，以及整体规划的制定；D (Do) 执行：根据已知的信息，设计具体的方法、方案和计划布局并执行；C (Check) 检查：总结执行计划的结果，分清执行中的过与得，明确效果，找出问题；A (Adjust）处理：对总结检查的结果进行处理，对成功的经验加以肯定，并予以标准化；对于失败的教训也要总结，引起重视。对于没有解决的问题，应提交给下一个PDCA循环中去解决。 象限法 公式拆解法 对比分析 可行域分析福格模型图 帕累托法则 假设分析 同期群分析 常见缩写 ROI —— 投资回报率 https://www.zzidc.com/info/dsyy/1575.html DAU —— 日活 MAU —— 月活 PP —— 英文percent point的简称，意思为百分比 UGC —— 用户原创内容 OLAP 与 OLTP当今的数据处理大致可以分成两大类：联机事务处理OLTP、联机分析处理OLAP。OLTP是传统的关系型数据库的主要应用，主要是基本的、日常的事务处理，例如银行交易。OLAP是数据仓库系统的主要应用，支持复杂的分析操作，侧重决策支持，并且提供直观易懂的查询结果。 CTR —— 点击通过率点击次数/曝光次数， 是衡量广告效果的重要指标。 SEO —— 搜索引擎优化 CR —— 转化率]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>术语</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个好用的插件]]></title>
    <url>%2F2018%2F09%2F25%2F%E5%A5%BD%E7%94%A8%E7%9A%84%E6%8F%92%E4%BB%B6%E5%B7%A5%E5%85%B7%2F</url>
    <content type="text"><![CDATA[一个好用的插件神器最近发现了一个特别能够提高生活幸福感的插件：Tampermonkey, 中文翻译过来叫做油猴。 安装好这个插件最大的作用， 就是我们可以从Greasy Fork这个网站来安装我们需要的脚本， 从而极大的提高浏览器的使用效率。 比如：我们在Greasy Fork网站找到智能划词翻译这个脚本， 然后点击安装脚本即可。安装完成脚本之后， 我们打开一个英文网页，对需要翻译的段落进行框选，点击翻译按钮，就可实现在原网页查看中文翻译， 对于我这种英语不好的人来说， 有很大的帮助。 我们还可以安装微博过滤设置脚本，来对微博页面进行个性化设置， 自己设置完成后的微博页面是这个样，相对于原版网页来说简洁了不少。 也可安装微博浮图脚本， 查看微博图片也比较方便， 只需把鼠标光标放在图片上即可 如果想找资料或电影资源的话， 也可以下载豆瓣资源下载大师, 或百度网盘直接下载助手等脚本， 当然，有能力还是要支持正版。 例如：安装豆瓣资源下载大师脚本后， 打开豆瓣电影网页， 页面是这个样子。 在Greasy Fork这个网站还有很多别人写好的脚本， 比如百度文库文字复制、购物党自动比价工具、 Download Youtube videos and subtitles等好用的脚本， 可以根据自己的需要进行安装。]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>网站</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[hive 基础查询]]></title>
    <url>%2F2018%2F09%2F11%2FHive%E5%9F%BA%E7%A1%80%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[什么是 HiveHive 是一种建立在Hadoop文件系统上的数据仓库架构, 并对存储的数据进行分析和管理，可以将 SQL 语句转换为 MapReduce 任务进行运行，这样就使得数据开发和分析人员很方便的使用 SQL 来完成海量数据的统计和分析。 Hive 擅长的是非实时的、离线的、对响应及时性要求不高的海量数据批量计算，统计分析。 Hive 不适用于在线交易处理 Hive 的常见查询语句 Hive 中的 SELECT 基础语法和标准 SQL 语法基本一致，支持 WHERE、DISTINCT、GROUP BY、ORDER BY、HAVING、LIMIT、子查询等 Hive 脚本如何注释 可以用 - - 开头的字符串来表示注释， 也可以将需要注释的 sql 选中， 然后用 ctrl + ? 快捷键来进行注释。 切换数据库 1use android; 123# 查看当前数据库select current_database() 12# 重置默认数据库use default; 查看表 查看当前使用的数据库中有哪些表 1show tables; 查看非当前使用的数据库中有哪些表 1show tables in myhive; 查看数据库中以 android 开头的表 12use android;show tables like &apos;android*&apos; 查看表的详细信息 1desc formatted android select…from 语句 基本查询 12# 查询 employee 表中的 name 和 salary。select name, salary from employee; 加入表中一列含有多个元素， 我们可以只查找此列的第一个元素 1select name, subord[0] from employees; 使用键值进行索引 1234567select name, deductions[&quot;state taxes&quot;] from employees;# 可以使用 &quot;点&quot; 符号， 类似：表的别名 . 列名 这样的用法select name, address.city from employees; 使用列值进行计算 1234567select upper(name), salary, deductions[&quot;Federal Taxes&quot;], rount(salary * (1 - salary, deductions[&quot;Federal Taxes&quot;]))from employees;# ZHANGYU 100000.0 0.2 80000 使用正则表达式 12345# 选出所有列名以 price 作为前缀的列select &apos;price.*&apos; from stocks; 常用的关系运算 12345678910111213 等值比较: = 等值比较:&lt;=&gt; 不等值比较: &lt;&gt;和!= 小于比较: &lt;小于等于比较: &lt;= 大于比较: &gt; 大于等于比较: &gt;= 区间比较 空值判断: IS NULL 非空判断: IS NOT NULL LIKE比较: LIKE JAVA的LIKE操作: RLIKE REGEXP操作: REGEXP 数学运算 123456789加法操作: +减法操作: –乘法操作: *除法操作: /取余操作: %与操作: &amp;或操作: |异或操作: ^取反操作: ~ 常用的聚合函数 1234567891011121314151617count(*) # 个数统计函数count(distinct col) # 统计去重之后的个数sum(col) # 求和sum(distinct col) #去重之后的和avg(col) # 平均值avg(distinct col) # 去重之后的平均值min(col) # 最小值max(col) # 最大值corr(col1, col2) # 相关系数var_pop(clo) # 方差var_samp(col) # 样本方差stddev_pop(col) # 标准偏差stddev_samp(col) # 标准样本偏差covar_pop(col1, col2) # 协方差covar_samp(col1, col2) # 样本协方差select count(distinct account), avg(salary) form employees; 使用别名 1234select count(distinct acount) as uv from employees; 使用limit语句限制返回的行数 123456# 只显示 10 行select count(distinct account) as uvform employees limit 10; 嵌套 select 语句 12345678910select e.name, e.salaryfrom( select upper(name) from employees) as ewhere e.salary &gt; 500; case…when..then句式 123456select name , salary, case when salary &lt; 5000 then &apos;low&apos; when salary &gt; = 5000 and salary &lt; 70000 then &apos;middle&apos; else &apos;high&apos; end as bracket from employees; where 语句, 添加条件 常见用法 1select * from employees where country = &apos;us&apos; and state = &apos;ca&apos;; 可以在where条件下计算 12345678select name , salary, deductions[&apos;first taxes&apos;], salary * (1-deductions[&apos;first taxes&apos;])from employeeswhere round(salary * (1-deductions[&apos;first taxes&apos;]) ) &gt; 70000;# zhangyu 100000.0 0.2 80000 对上式进行优化 1234567891011select e.* from ( select name , salary, deductions[&apos;first taxes&apos;], salary * (1-deductions[&apos;first taxes&apos;]) from employees ) ewhere round(salary * (1-deductions[&apos;first taxes&apos;]) ) &gt; 70000; 条件中有浮点数 1234567# 对浮点数进行比较select name, salary, duductions[&apos;first taxes&apos;]from employees where duductions[&apos;first taxes&apos;] &gt; 0.2; 出现的结果中会有 0.2， 因为 DOUBL 和 FLOAT 类型不同 123456select name, salary, duductions[&apos;first taxes&apos;]from employees where duductions[&apos;first taxes&apos;] &gt; cast (0.2 as float); 出现的结果中不会有0.2 like 和 rlike rlike 子句是Hive功能的一个扩展， 可以通过Java的正则表达式来指定匹配条件 12345678select name, address.streetfrom employees where address.street rlikt &apos;.*(beijing|shanghai).*&apos;;# 用likeselect name, address from employeeswhere address.street like &apos;%beijing%&apos; or address.street like &apos;%shanghai%&apos;; group by 语句, order by, 与 having 分类并排序 12345678910select year(ymd), avg(price_close) from stockswhere exchange = &apos;nasdaq&apos; and symbol = &apos;aapl&apos;group by year(ymd)order by year(ymd) desc; having 子句来限制输出结果 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849# 例子1select year(ymd), avg(price_close) from stockswhere exchange = &apos;nasdaq&apos; and symbol = &apos;aapl&apos;group by year(ymd)having avg(price_close) &gt; 50.0 ;# 例子2select col1from t1group by col1having sum(col2) &gt; 10# 如果没有having， 将要使用嵌套select子查询# 例子1select s2.year, s2.avg from( select year(ymd) as year, avg(price_close) as avg from stocks where exchange = &apos;nasdaq&apos; and symbol = &apos;aapl&apos; group by year(ymd)) s2where s2.avg &gt; 50.0# 例子2select col1 from (select col1, sum(col2) as col2sum from t1 group by col1 ) as t2where t2.col2sum &gt; 10 having 与 where 的区别 Where 是一个约束声明，使用Where约束来自数据库的数据，Where是在结果返回之前起作用的，Where中不能使用聚合函数。Having是一个过滤声明，是在查询返回结果集以后对查询结果进行的过滤操作，在Having中可以使用聚合函数。 12345678# 查找平均工资大于3000的部门select deparment, avg(salary) as average from salary_info group by deparment having average &gt; 3000#查询每个部门工资大于3000的员工个数select deparment, count(*) as c from salary_info where salary &gt; 3000 group by deparment join 语句 Hive中Join的关联键必须在ON ()中指定，不能在Where中指定 内连接 只有进行连接的两个表中都存在与连接标准相匹配的数据才会被保留下来。 123456SELECT a.ymd, a.price_close, b.price_closeFROM a JOIN b ON a.ymd = b.ymdWHERE a.symbol = &apos;Apple&apos; and b.symbol = &apos;Ibm&apos; ON 子句指定了两个表间数据进行连接的条件。 对于多张表进行连接查询 1234567SELECT a.ymd, a.price_close, b.price_close, c.price_closeFROM a JOIN b ON a.ymd = b.ymd JOIN c ON a.ymd = c.ymdWHERE a. symbol = &apos;Apple&apos; AND b.symbol = &apos;Ibm&apos; AND c.symbol = &apos;Google&apos; 为什么条件内不加表 b 和表 c 进行连接操作， 因为 Hive总是按照从左到右的顺序来执行 Join 优化 Hive 会假定查询中最后一个表是最大的表， 在对每行记录进行连续操作时， 它会尝试将其他表缓存起来，然后扫描最后那个表进行计算。 因此， 我们在查询时， 要保证连续查询中的表的大小从左到右依次是增加的。 假如，在 a, b 两个表中，b表最小， 则 sql 需要修改为： 123456SELECT a.price_close, b.price_closeFROM b JOIN a ON b.ymd = a.ymd AND b.symbol = a.symbolWHERE a.symbol = &apos;APPLE&apos; 使用 “标记” 来指定哪张表是大表， 不需要排序 123456SELECT /*+Streamtable(a)*/ a.price_close, b.price_closeFROM a JOIN B on a.ymd = b.ymd AND a.symbol = b.symbolWHERE a.symbol = &apos;Apple&apos; 左外连接 123456SELECT a.price_close, b.price_closeFROM a LEFT OUTER JOIN b on a.ymd = b.ymd AND a.symbol = b.symbolWHERE a.symbol = &apos;Apple&apos; 左边表符合 WHERE 条件的全部返回，右表不符合 ON 条件的返回 NULL 完全外链接 123456SELECT a.price_close, b.price_closeFROM a FULL OUTER JOIN b on a.ymd = b.ymd AND a.symbol = b.symbolWHERE a.symbol = &apos;Apple&apos; 返回所有表中符合 WHERE 语句条件的所有记录 Hive 不支持右半开连接 排序 ORDER BY Order by 对查询的所有结果进行排序 可在字段加 DESC 关键字， 进行降序排序。 （默认 ASC， 升序） 1234567891011SELECT a.price_close,FROM a WHERE a.symbol = &apos;Apple&apos;GROUP BY a.price_closeORDER BY A.PRICE_close DESCLIMIT 10; 子查询 Hive中如果是从一个子查询进行SELECT查询，那么子查询必须设置一个别名 From 子句进行子查询 1234567891011121314151617181920212223242526select dt, count(distinct account) as uv, count(1) as pvfrom (select dt, count(distinct account) as uv, count(1) as pv from client.android_log_viewUNION ALL select dt, count(distinct account) as uv, count(1) as pv from client.ios_log_view ) group by dtorder by dt Hive 0.13 开始， Where 子句也支持子查询 1234567SELECT *FROM AWHERE A.a IN (SELECT foo FROM B); SELECT AFROM T1WHERE EXISTS (SELECT B FROM T2 WHERE T1.X = T2.Y) 将子查询作为一个表的语法，叫做Common Table Expression（CTE） 如果用 distinct, select 后面必须直接跟 distinct 1234567891011121314151617181920212223242526272829with a1 as (select distinct user_account, provincefrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos;union allselect distinct user_account, provincefrom computer_view.client_ios_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos;)select province, count(distinct user_account) as uvfrom a1group by provinceorder by uv DESC Hive Sql 的查询优化 在查询中， 避免使用 select *, 使用条件限制取需要的列 在使用 Join 进行外关联时， 将副表的过滤条件写在 where 后面，会先全表关联， 再进行过滤， 这样会耗费资源。 123456SELECT a.price_close, b.price_closeFROM b JOIN a ON b.ymd = a.ymd AND b.symbol = a.symbolWHERE s.symbol = &apos;APPLE&apos; 正确的写法是将 where 条件卸载 on 后面 1234SELECT a.price_close, b.price_closeFROM b JOIN a ON ( b.ymd = a.ymd AND b.symbol = a.symbol and s.symbol = &apos;APPLE&apos;) count(distinct ),在数据量特别大的情况下，效率较低, 可以用先 group by 再 count 的方式进行代替 因为count(distinct)是按group by 字段分组，按distinct字段排序 12345678910use computer_view;select hit_date, count(distinct user_account) as uvfrom client_android_log_viewwhere hit_date between &apos;2018-10-01&apos; and &apos;2018-10-02&apos;group by hit_date 可以转换成： 1234567891011121314151617use computer_view;select hit_date, count(user_account) as uvfrom(select hit_date, user_accountfrom client_android_log_viewwhere hit_date between &apos;2018-10-01&apos; and &apos;2018-10-02&apos;group by hit_date, user_account) agroup by hit_date 子查询 不要使用 group by 123456789101112131415SELECT * FROM ( SELECT * FROM t1 GROUP BY c1,c2,c3 UNION ALL SELECT * FROM t2 GROUP BY c1,c2,c3)t3 GROUP BY c1,c2,c3 应该写成 12345678910111213SELECT * FROM ( SELECT * FROM t1 UNION ALL SELECT * FROM t2)t3 GROUP BY c1,c2,c3 子查询内 不要使用 count(distinct), max, min 1234567891011121314151617SELECT * FROM ( SELECT * FROM t1 UNION ALL SELECT c1, c2, c3, COUNT(DISTINCT c4) FROM t2 GROUP BY c1,c2,c3)t3 GROUP BY c1,c2,c3; 可以用临时表来进行优化123456789101112131415161718192021222324252627INSERT t4 SELECT c1,c2,c3,c4 FROM t2 GROUP BY c1,c2,c3; SELECT c1,c2,c3,SUM(income),SUM(uv) FROM ( SELECT c1, c2, c3, income, 0 AS uv FROM t1 UNION ALL SELECT c1, c2, c3, 0 AS income, 1 AS uv FROM t2)t3 GROUP BY c1,c2,c3; 常用函数 字符串截取函数：substr,substring 语法: substr(string A, int start, int len),substring(string A, int start, int len) 返回值: string 说明：返回字符串A从start位置开始，长度为len的字符串 举例： 12345678910use computer_view;select substring(charge_products,2,30)from client_android_log_viewwhere hit_date between &apos;2018-10-01&apos; and &apos;2018-10-05&apos;group by charge_productslimit 15 1234567891011121314select substring(a2.charge_products,2,80), a1.namefrom lookup.products_lookup as a1 join computer_view.client_android_log_view as a2 on a1.product = substring(a2.charge_products,2,80)where hit_date between &apos;2018-10-07&apos; and &apos;2018-10-13&apos; and mall_events is not nullgroup by substring(a2.charge_products,2,80), a1.name 刷新数据表 1refresh table computer_log.client_ios_log 取数据的并集， 交集， 和差集 并集 1234567891011121314151617181920with a1 as (select distinct user_account from computer_view.client_android_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos;union all select distinct user_account from computer_view.client_ios_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos;) selectcount(distinct user_account) as uvfrom a1 交集 1234567891011121314151617181920212223242526272829with a1 as(select distinct two as user_accountfrom test.data_csvintersect(select distinct user_accountfrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos; union all select distinct user_accountfrom computer_view.client_ios_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos;))select count(user_account) as uvfroma1limit 100 差集 12345678910111213141516171819202122232425262728293031# 求的是test库的data_csv 数据， 与客户端数据的 差with a1 as(select distinct two as user_accountfrom test.data_csvexcept (select distinct user_accountfrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos; union all select distinct user_accountfrom computer_view.client_ios_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos;))select count(user_account) as uvfroma1limit 100 详细列出差集的版本号12345678910111213141516171819202122232425262728293031323334with a1 as(select distinct two as user_accountfrom test.data_csvexcept (select distinct user_accountfrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-03&apos; union all select distinct user_accountfrom computer_view.client_ios_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-03&apos;))select a2.six , COUNT(a2.two) as uv, count(a1.user_account) as uv_1froma1, test.data_csv as a2WHERE a1.user_account = a2.twogroup by a2.sixlimit 100 创建临时表12345678910111213141516use default;create table test.nine_android_user_version_10select user_account, app_versionfrom computer_view.client_android_log_viewwhere hit_date between &apos;2018-09-01&apos; and &apos;2018-09-30&apos; and user_account is not null and app_version is not nullgroup by user_account, app_version 12345678910111213141516use default;create table test.nine_user_version_10select url_par(url_query,&apos;account&apos;) as user_account, split(url_par(url_query,&apos;AppID&apos;),&apos; &apos;)[1] as app_versionfrom apache_log.client_ios_sensorwhere dt between &apos;2018-10-01&apos; and &apos;2018-10-20&apos; and url_par(url_query,&apos;account&apos;) is not null and url_par(url_query,&apos;AppID&apos;) is not nullgroup by url_par(url_query,&apos;account&apos;), split(url_par(url_query,&apos;AppID&apos;),&apos; &apos;)[1] 留存率 次日留存 1234567891011121314151617181920212223242526272829303132333435363738394041use computer_view;with a1 as(select hit_date, user_accountfrom client_android_log_view --client_ios_log_viewwhere hit_date between &apos;2018-10-10&apos; and &apos;2018-10-24&apos;and app_version like &apos;7.0%&apos; --app_id like &apos;%et 7.0%&apos;),a2 as(select hit_date, user_accountfrom client_android_log_view --client_ios_log_viewwhere hit_date between &apos;2018-10-10&apos; and &apos;2018-10-31&apos;and app_version like &apos;7.0%&apos; --app_id like &apos;%et 7.0%&apos;)select a.hit_date, count(distinct b.user_account) uvfrom a1 a, a2 bwhere a.user_account = b.user_accountand b.hit_date = date_add(a.hit_date,1)group by a.hit_dateorder by a.hit_datelimit 100 7日留存 1234567891011121314151617181920212223242526272829303132333435363738394041use computer_view;with a1 as(select hit_date, user_accountfrom client_android_log_view --client_ios_log_viewwhere hit_date between &apos;2018-10-10&apos; and &apos;2018-10-24&apos;and app_version like &apos;7.0%&apos; --app_id like &apos;%et 7.0%&apos;),a2 as(select hit_date, user_accountfrom client_android_log_view --client_ios_log_viewwhere hit_date between &apos;2018-10-10&apos; and &apos;2018-10-31&apos;and app_version like &apos;7.0%&apos; --app_id like &apos;%et 7.0%&apos;)select a.hit_date, count(distinct b.user_account) uvfrom a1 a, a2 bwhere a.user_account = b.user_accountand b.hit_date = date_add(a.hit_date,7)group by a.hit_dateorder by a.hit_datelimit 100 123456789101112131415161718192021# 留存sql优化select count(1)from( select userid, count(1) from( select t1.userid, t1.statdate from table1 t1 where t1.statdate &gt;= $&#123;上30天日期&#125; and t1.statdate &lt;= $&#123;上一天日期&#125; group by t1.userid, t1.statdate ) s1 group by userid having count(1) &gt; 2 ) R1 此sql为一个样例，计算连续跟任意都适用，至于计算第N天，只需要更改下日期过滤条件，变成=$[上N天日期]，=${上一天日期}。另外，这种方式适合跑当前周期数据，如果跑历史数据，可以写个循环。当然，最暴力还是直接用userid 关联。 这种写法，更多是针对现在大部分分布式处理平台的特性，尽可能将数据合理均匀分片，每台服务器各自运算自己的，最后汇总。 尽可能少用 count distinct 这种写法，因为无法利用分片的特性。 排序函数https://blog.csdn.net/bitcarmanlee/article/details/517459261.rank 函数, dense_rank()函数,row_number函数 rank函数， 返回数据项在分组中的排名， 排名相等的会留下空位， 如1、2、2、4dense_rank函数， 返回数据项在分组中的排名， 排名相等的不会留下空位， 如1、2、2、3row_number函数， 返回数据项在分组中的排名， 排名不管数据是否相等， 如1、2、3、4 函数的作用： 为每条函数返回一个数字 row_number 的函数格式：row_number() OVER (partition by 分组字段 ORDER BY 排序字段 排序方式asc/desc) 函数说明： 先根据分组字段对结果进行分组， 结果内部Itzhak Perlman按照 排序字段进行排序，并定义排序方式 1234567891011121314151617181920212223242526按照日期从小到大排序之后， 再按照uv进行从大到小排序， 并进行排序分类， 返回排序数字with a1 as ( select hit_date, nbtn_name, count(distinct user_account) as uv from computer_view.client_android_log_view where hit_date between &apos;2018-12-01&apos; and &apos;2018-12-05&apos; and nbtn_position like &quot;%Syjh-sy-zysj%&quot; group by hit_date, nbtn_name order by hit_date)select a1.hit_date, a1.nbtn_name, a1.uv, row_number() over(partition by hit_date order by uv DESC) as r_n, rank() over( partition by hit_date order by uv DESC) as rk, dense_rank() over (partition by hit_date order by uv DESC) as drankfrom a1 用python脚本连接数据库 首先， 用Python连接数据库 12345678from pyhive import hive import timeconn = hive.Connection(host=&apos;ip地址&apos;, port=10000, username=&apos;用户名&apos;, database = &apos;default&apos;, auth=&apos;NOSASL&apos;)cursor = conn.cursor()# 获得连接的游标 设置开始和结束时间 这样只要sql语句不变， 我们跑sql时， 只需要改日期即可。 12startdate = &apos;2018-09-01&apos;enddate = &apos;2018-09-19&apos; 用Python中的format函数将日期传入{}中 python中写sql脚本时， 需要用\来进行换行符的转换, \后面不能有空格。 日期用两个{}来代替， 用format函数将开始日期与结束日期传入 123456789101112131415161718192021222324252627# 提取积分类uv,pv数据sql_jifenxinxi_an = &quot;&quot;&quot;select count(distinct user_account) as uv, count(1) as pv from computer_view.client_android_log_view where hit_date between &quot;&#123;&#125;&quot; and &quot;&#123;&#125;&quot; and (btn_position like &quot;服务-查询-积分信息%&quot; or btn_home = &quot;积分-扇形左&quot; ) limit 1000&quot;&quot;&quot;.format(startdate,enddate)# format 插入时间cursor.execute(sql_jifenxinxi_an)# 运行此语句cursor.fetchall()#fetchall():接收全部的返回结果行.#fetchmany(self, size=None):接收size条返回结果行.如果size的值大于返回的结果行的数量,则会返回cursor.arraysize条数据.#fetchone(self):返回一条结果行. 备注：在进行删除或者更新之后，必须用commit函数进行提交，更新数据 1cursor.commit() 参考资料：Hive 编程指南Hive的那些事Hive 官网一起学HiveHive性能优化上的一些总结]]></content>
      <categories>
        <category>编程语言</category>
        <category>Hive</category>
      </categories>
      <tags>
        <tag>Hive</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据分析职业方向]]></title>
    <url>%2F2018%2F09%2F01%2F%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E5%85%A8%E6%A0%88%2F</url>
    <content type="text"><![CDATA[系统学习数据分析 直接上手80%的数据分析路径： 摸清从事行业的关注点， 看行业报告 数据分析流程 统计学 excel， sql python数据分析案例 数据可视化—— excel，powerbi, tableau ppt 数据挖掘算法 python数据挖掘 专业路径——编程方向： 数据分析流程 统计学 python数据分析 数据挖掘算法 机器学习算法 python机器学习 hadoop + hive + spark 数据可视化 （excel + echarts) ppt 商业分析： 行业报告 脑图工具 数据分析流程 excel + sql + ppt 黑客增长， 数据化管理 友盟， GA EXCEL + powerbi可视化 ppt 《全栈数据入门》要学的知识： linux sed grep shell shell快捷键：Emacs python数据库 】 numpy 】 pandas 】 Orange sklearn scipy Matplotlab scikit-learn Hadoop Spark-大数据分析框架 mysql Hive 】 统计学 。。。 算法 KNN kmeans 贝叶斯 决策树 随机森林 Zeppelin 提供Web环境， 主要用于进行数据分析与数据可视化。（支持Scala,Spark,Python,PySpark,Shell环境）]]></content>
      <categories>
        <category>数据分析</category>
      </categories>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[用python来提高工作中运行SQL语句的效率]]></title>
    <url>%2F2018%2F08%2F12%2F%E7%94%A8python%E6%8F%90%E9%AB%98%E8%BF%90%E8%A1%8Csql%E7%9A%84%E6%95%88%E7%8E%87%2F</url>
    <content type="text"><![CDATA[用python来提高工作中运行SQL语句的效率作为一名数据分析师，日报、周报、月报数据一个也不能少。 相应的， 就要在数据库中提取大量的数据， 并处理大量的Excel表格。 在提取和处理数据的过程中， 对于一些重复性的劳动， 写个Python脚本来实现半自动化， 能够大幅提高自己的工作效率。 以下是自己工作中的一点总结经验。 自己在工作中使用的最多的就是Hive-SQL查询语句，很多时候，每天的运行的SQL语句只需要改个日期即可。 但如果你每一天都要对每条SQL语句改日期， 然后再把每条SQL语句粘贴到数据库中跑的话， 会特别的费时耗力。 我们可以写个简单的Python脚本， 这样每次运行前只需要改一次日期，就可以将所有SQL语句运行一遍， 大大提高工作效率。 首先， 用Python连接数据库 对于数据库的ip地址，用户名，密码等， 如果不清楚，或数据库连接不上， 需要和开发人员对接 1234567from pyhive import hive import timeconn = hive.Connection(host=&apos;ip地址&apos;, port=10000, username=&apos;用户名&apos;, database = &apos;default&apos;, auth=&apos;NOSASL&apos;)cursor = conn.cursor() 设置开始和结束时间 这样只要sql语句不变， 我们跑sql时， 只需要改日期即可。 12startdate = &apos;2018-09-01&apos;enddate = &apos;2018-09-19&apos; 用Python中的format函数将日期传入{}中 python中写sql脚本时， 需要用\来进行换行符的转换, \后面不能有空格。 日期用两个{}来代替， 用format函数将开始日期与结束日期传入 123456789101112131415161718# 提取积分类uv,pv数据sql_jifenxinxi_an = &apos;select \ count(distinct user_account) as uv, \ count(1) as pv \from \ computer_view.client_android_log_view \where \ hit_date between &quot;&#123;&#125;&quot; and &quot;&#123;&#125;&quot; \ and \ (btn_position like &quot;服务-查询-积分信息%&quot; \ or \ btn_home = &quot;积分-扇形左&quot; \ ) \limit 1000&apos;.format(startdate,enddate)cursor.execute(sql_jifenxinxi_an)print(cursor.fetchall()) 我们可以按照这个格式写工作中需要运行的多个SQL语句。 这样， 当脚本运行的时候， 我们可以腾出时间来去干其他工作， 等过一段时间，所有的SQL语句都跑完了， 我们再进行统一的整理。]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[个人资料收集]]></title>
    <url>%2F2018%2F08%2F05%2F%E8%B5%84%E6%BA%90%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[视频： 科普-计算机科学速成课 纪录片-设计的艺术 纪录片-小兵小赵 访谈-子夜.大学之殇 月球视频 最后的演讲 性，死亡与生命的意义 统计学：statistics —— CrashCourse 锵锵行天下 蓝色星球 风味人间 成功的原则 youtube视频：Rachel’s English 纪录片-生门 电影-生门 书： The Non-Designer’s Design Book (4th Edition) 英文原版免费编程书籍 网站： RSS收集网站 知笔墨 微软海底机房摄像头直播 全球免费摄像头直播 设计类网站 漫画-海报 Our the in World 中国知网 统计学可视化 cnki免费下载文献：账号：hqwytsg015 密码：cnki015 北京值得去的地方 纪录片——AlphaGo youtube最受欢迎的频道 写作网站 博客 TED:阅读全世界 阮一峰的个人网站 追求对知识概念和原理进行更合适的描述 《用数据讲故事》作者博客 万维钢的博客 w4lle’s Notes = android技术博客 stormzhang 廖祜秋的博客 数据分析类网站 Kaggle 统计之都 纪杨的网站数据分析笔记 蓝鲸的网站分析笔记 Cloga的互联网笔记 陈老师的天善智能博客文章 秦路-文章 数据可视化网站 一起大数据 数据分析问答 数学公式转换MD格式 信息图制作 TED 如何掌控你的自由时间 —— 时间=选择 提升自信的技巧 —— 除非你做到了，否则没有人相信你 【TED】科技公司如何控制你的注意力 我从生活和写作中学到了12个真理 - 一个一个写，改初稿， 如果不知道写什么，就写你自己经历的事情 收入如何影响人们的生活方式——世界各国， 收入水平导致的生活条件改变都差不多官方网站 图表的魔力——图表能够让人更快的理解信息 如何利用大数据做出正确的判断-用大量数据去做分析， 去深入了解， 但要想成功， 就需要冒一定的风险 大数据时代：如何避免数据迷信？-不光要依靠大数据， 也要依靠厚数据，让解决问题的方法更加多元化 李开复：人工智能如何拯救人类-ai让我们明白我们为何为人 开启情绪识别的大门-用算法来识别人类情绪]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>网站</tag>
        <tag>书籍</tag>
        <tag>视频</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[python基础]]></title>
    <url>%2F2018%2F07%2F25%2Fpython%E5%9F%BA%E7%A1%80%2F</url>
    <content type="text"><![CDATA[数 整数-int 123i = 1# 查看变量的类型type(i) 浮点数-float 12340.2 * 0.10.020000000000000004 浮点数在计算机中表输入的十进制浮点数仅由实际存储在计算机中的近似的二进制浮点数表示,二进制来表达 1/10 是一个无限循环小数:0.0001100110011001100110011001100110011001100110011…,Python 只打印机器中存储的二进制值的十进制近似值。 如何限制显示的小数点个数 1print(&quot;&#123;:.4f&#125;&quot;.format(0.1*0.4)) 字符串-str 字符串的表示方式： python中有3种表示字符串的方式——单引号，双引号，三引号。 单引号和双引号的作用是相同的, 但双引号中可以将包含的特殊字符单引号输出，而单引号要输出’需要/转义 123456str = &apos; \&apos;hello world\&apos; &apos;print(str)str = &quot;&apos;hello python&apos;&quot;print(str) 三引号的用法特殊，三引号中可以输入单引号、双引号或换行符等字符,也可用作制作文档字符串 123str = &apos;&apos;&apos;&apos;hello&apos;/&quot;world&quot;&apos;&apos;&apos;print(str) 常用的字符串处理方法 1234# 重复字符串sta = &apos;hi&apos;print (sta * 2) 1234# 切片sta = &apos;1234567890&apos;print(sta[2:9:2]) 123456789#去除空格name = &apos; zhang yu &apos;# 去除左侧空格print(name.lstrip())# 去除右侧空格print(name.rstrip())# 去除两侧空格print(name.strip()) 123456789101112# 分割字符串#split()函数通过制定分隔符对字符串进行切片name = &quot;zhang and yu and han&quot;print(name.split())print(name.split(&apos; &apos;,2))print(name.split(&apos; and &apos;))# partition()函数将目标字符串分割为两个部分，返回一个三元元组（head,sep,tail），包含分割符url = &quot;https://zhangandyu.github.io//2018&quot;print( url.partition(&quot;//&quot;))print(url.rpartition(&quot;//&quot;)) 1234567# 替换字符串#str.replace(old, new,max)# max 可选字符串不能超过max次str = &quot;this is a apple&quot;print (str.replace(&quot;is&quot;, &quot;was&quot;)) print (str.replace(&quot;is&quot;, &quot;was&quot;, 1)) 1234# 拼接字符串a = &apos;_&apos;name = (&apos;zhang&apos;, &apos;and&apos;, &apos;yu&apos;)print(a.join(name)) 12345678910#查找字符串是否包含子字符串#str.find(str, beg=0, end=len(string))a = &apos; this is a apple&apos;b = &apos;is&apos;#从下标3开始，查找在字符串里第一个出现的子串，返回结果：3print(a.find(b,2)) #从下标5开始，查找在字符串里第一个出现的子串，返回结果：6print(a.find(b,5)) 12345678910111213#判断字符串是否以指定的前后缀结尾# str.startswith(str, beg=0,end=len(string))a = &apos;this is a apple&apos;b = &apos;th&apos;c = &apos;is&apos;print(a.startswith(b))print(a.startswith(c,2))a = &apos;this is a apple&apos;b = &apos;le&apos;c = &apos;app&apos;print(a.endswith(b))print(a.endswith(b,10)) 1234567891011121314151617181920212223#其他函数# 检测数字str.isdigit() # 检测字符串是否只由数字组成str.isnumeric() # 检测字符串是否只由数字组成,这种方法是只针对unicode对象str.isdecimal() # 检查字符串是否只包含十进制字符。这种方法只存在于unicode对象# 检测字母str.isalpha() # 检测字符串是否只由字母组成# 检测字母和数字str.isalnum() # 检测字符串是否由字母和数字组成# 检测其他str.isspace() # 检测字符串是否只由空格组成str.islower() # 检测字符串是否由小写字母组成str.isupper() # 检测字符串中所有的字母是否都为大写str.istitle() # 检测字符串中所有的单词拼写首字母是否为大写，且其他字母为小写str.capitalize() # 将字符串的第一个字母变成大写,其他字母变小写str.lower() # 转换字符串中所有大写字符为小写str.upper() # 将字符串中的小写字母转为大写字母str.swapcase() # 对字符串的大小写字母进行转换max(str) # 返回字符串 str 中最大的字母min(str) # 返回字符串 str 中最小的字母len(str) # 返回字符串的长度str(arg) # 将 arg 转换为 string 布尔值 and-逻辑与 or-逻辑或 not-逻辑非 not的优先级大于and和or的优先级，而and和or的优先级相等。 逻辑运算符的优先级低于关系运算符，必须先计算关系运算符，再计算逻辑运算符。 变量命名规则 只能包含字母、数字和下划线 不能包含空格 不能将python关键字和函数名用作变量名 变量名应简短又具有描述性 慎用小写字母l和大写字母O 序列 什么是序列 序列是Python中最基本的数据结构。 python中有6个序列的内置类型,包括列表、元组、字符串、Unicode字符串、buffer对象和xrange对象。 对于序列，都可以使用一下操作： 索引 切片 加 乘 成员检查in和not in 计算序列的长度len() 取序列中的最大、最小值max()和min() 列表 列表是最常用的Python数据类型，它可以作为一个方括号内的逗号分隔值出现 列表适合用于存储在程序运行期间可能变化的数据集。 列表是可以修改的， 这对处理网站的用户列表或游戏中的角色列表至关重要。 列表对象方法 12 list.append(x)# 把一个元素添加到列表的结尾 12 list.extend(x)# 将一个给定列表中的所有元素都添加到另一个列表中 12 list.insert(i,x)# 在指定位置插入一个元素 12list.remove(x)# 删除列表中值为 x 的第一个元素 12list.pop(i)# 从列表的指定位置删除元素，并将其返回 12 list.clear()# 从列表中删除所有元素 12list.index(x)# 返回列表中第一个值为 x 的元素的索引 12list.count(x)# 返回 x 在列表中出现的次数 12 list.sort()# 对列表中的元素进行排序 12list.sorted()# 对列表中的元素进行临时排序 12list.reverse()# 倒排列表中的元素 12list.copy()# 返回列表的一个浅拷贝 12list.len(x)#返回列表的长度 用列表实现栈和列队 栈是一种后进先出的数据结构，我们可以使用列表的append()和pop()方法了实现 123a = [1,2]a.append(3) #入栈a.pop() # 最后一个元素出栈 队列是一种先进先出的数据结构，我们可以使用列表的append()和pop(0)方法了实现 123a = [2,1]a.append(1) # 入队列a.pop(0) # 第0个元素出队列 列表推导式 为从序列中创建列表提供了一个简单的方法。 普通方法 12345a = []for i in range(20): a.append(i ** 2)print(a)# i 依然存在 123456b = []for x in [1,2,3,4]: for y in [2,3,4]: if x !=y: b.append((x,y))print(b) 推导式 12a = [i**2 for i in range(20)]print(a) 1[(x,y) for x in [1,2,3,4] for y in [2,3,4] if x !=y] 12from math import pi[str(round(pi, i)) for i in range(1, 16)] 12345matrix = [ [1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]][[row[i] for row in matrix] for i in range(4)] 元组 元组为不可变得列表， 在需要创建一系列不可修改的元素时使用。 只有一个元素元组中只包含一个元素时， 需要在元素后面添加逗号，否则括号会被当做运算符使用 12zy = (2,)zy[0] 修改元组 对元组进行连接组合 1234na = (&apos;z&apos;, &apos;y&apos;)me = (&apos;y&apos;, &apos;u&apos;)name = na + meprint(name) 给元组变量赋值 123na = (&apos;zy&apos;, &apos;yu&apos;)na = (&apos;y&apos;, &apos;u&apos;)print(na) 元组运算符 123456789101112# 计算元组个数len((2,45, 67, 8, 9))# 连接(1,2,3, 4) + (4, 5, 6)# 复制(1,2,3, 4)* 3#迭代 for i in (12, 3,4 , 5): print(i) 将列表转换为元组 123list = [ &apos;z&apos;, 1, 2, 3, 4, &apos;u&apos;]tup = tuple(list)print(tup) 映射和集合字典 字典：将相关信息关联起来 访问字典 12man = &#123;&apos;name&apos;:&apos;zhangyu&apos;, &apos;xingbie&apos;:&apos;man&apos;, &apos;hige&apos;:165&#125;print(man[&apos;name&apos;]) 添加键-值对 12man[&apos;home&apos;] = &apos;xian&apos;man 修改字典中的值 123man = &#123;&apos;name&apos;:&apos;zhangyu&apos;, &apos;xingbie&apos;:&apos;man&apos;, &apos;hige&apos;:165&#125;man[&apos;hige&apos;] = 170man 删除键-值对 123man = &#123;&apos;name&apos;:&apos;zhangyu&apos;, &apos;xingbie&apos;:&apos;man&apos;, &apos;hige&apos;:165&#125;del man[&apos;xingbie&apos;]man 遍历所有的键-值对 1234man = &#123;&apos;name&apos;:&apos;zhangyu&apos;, &apos;home&apos;:&apos;xian&apos;, &apos;hige&apos;:165, &apos;girfriend&apos;:&apos;null&apos;&#125;for key, value in man.items(): print( key,&quot;:&quot; , value) 分别遍历所有的键-值 1234567man = &#123;&apos;name&apos;:&apos;zhangyu&apos;, &apos;home&apos;:&apos;xian&apos;, &apos;hige&apos;:165, &apos;girfriend&apos;:&apos;null&apos;&#125;for key in man.keys(): print( key) for value in man.values(): print(value) 按倒序顺序遍历分别遍历所有的键-值 12for key in sorted(man.keys()): print( key) 字典列表 12345678man = &#123;&apos;name&apos;:&apos;zhangyu&apos;, &apos;home&apos;:&apos;xian&apos;, &apos;hige&apos;:165, &apos;girfriend&apos;:&apos;null&apos;&#125;alien = &#123;&apos;color&apos;:&apos;green&apos;, &apos;points&apos;:5&#125;computer = &#123;&apos;name&apos;:&apos;wangzhou&apos;, &apos;num&apos;: 40&#125;alients = [man, alien, computer]for i in alients: print(i) 在字典中存储列表 12345678910province = &#123; &apos;name&apos;:[&apos;zhangyu&apos;, &apos;han&apos;, &apos;dou&apos;], &apos;home&apos;:[&apos;xian&apos;, &apos;beijing&apos;], &apos;hige&apos;:[165, 170, 370, 2389]&#125;for na, las in province.items(): for la in las: print(na, la) 在字典中存储字典 12345678province = &#123; &apos;man&apos; :&#123;&apos;name&apos;:&apos;zhangyu&apos;, &apos;home&apos;:&apos;xian&apos;, &apos;hige&apos;:165, &apos;girfriend&apos;:&apos;null&apos;&#125;, &apos;alien&apos; : &#123;&apos;color&apos;:&apos;green&apos;, &apos;points&apos;: 5&#125;, &apos;computer&apos; : &#123;&apos;name&apos;:&apos;wangzhou&apos;, &apos;num&apos;: 10&#125;&#125;for i, a in province.items(): print(i, a) 字典键的特性 不允许同一键出现两次，创建时如果同一键被赋值两次， 后一个值会被记住。键必须不可变， 可以用数字，字符串，或元组充当，但不能用列表 12dict = &#123;&apos;name&apos;: &apos;zhang&apos;, &apos;name&apos;:&apos;yu&apos;&#125;dict python中关于字典的函数 1234567891011121314151617181920212223242526272829303132333435# 删除字典内所有元素dict.clear()#返回一个字典的浅复制dict.copy()# 创建字典seq = (&apos;Google&apos;, &apos;Runoob&apos;, &apos;Taobao&apos;)dict = dict.fromkeys(seq,10)dict#返回指定键的值，如果值不在字典中返回设定值dict1.get(&apos;Google&apos;, 40)#和get()类似, 但如果键不存在于字典中，将会添加键并将值设为设定值dict1.setdefault(&apos;google&apos;, &apos;20&apos;)dict1#把字典dict2的键/值对更新到dict里dict1=&#123;&apos;Google&apos;: 10, &apos;Runoob&apos;: 10, &apos;Taobao&apos;: 10&#125;dict2 =&#123;&apos;na&apos;: &apos;zhang&apos;, &apos;name&apos;:&apos;yu&apos;&#125;dict1.update(dict2)dict1#以列表返回可遍历的(键, 值) 元组数组dict.items()#以列表返回一个字典所有的键dict.keys()#以列表返回字典中的所有值dict.values() 集合类型 集合是一个无序的，不重复的数据集合。集合作用有以下两点： 去重： 把一个还有重复元素的列表或元组等数据类型变成集合， 其中的重复元素只出现一次，用set()方法 1234567891011121314#使用大括号之间创建集合f = &#123;1, 2, 2, 2, &apos;a&apos;&#125;print(f)print(type(f))# 用set()方法a = [1, 2, 2, &apos;a&apos;, &apos;a&apos;]b = (1,2,2, &apos;a&apos;, &apos;a&apos;)c = set(a)d = set(b)e = set()print(c)print(d)print(e) 进行关系测试：测试两组数据之间的交集，差集，并集等数据关系 12345678910111213141516171819202122232425262728293031323334# 查看集合的相关函数help(set)a = [1,2,2,&apos;a&apos;,&apos;a&apos;,&apos;d&apos;,&apos;e&apos;]b = [1,2,2,&apos;a&apos;,&apos;a&apos;,&apos;b&apos;,&apos;b&apos;]c = set(a)d = set(b)# 取交集e = c.intersection(d)print(e)# 取并集f = c.union(d)print(f)# 取差集（无重复）g = c.difference(d)print(g)#对称差集&quot;（不同时在c,d中存在）h = c.symmetric_difference(d)print(h)#判读是否为子集i = c.issubset(d)print(i)# &quot;判读是否为超集&quot;j = c.issuperset(d) #检查是否有相同元素,没有返回Truek = c.isdisjoint(d) 条件和循环if语句 if-else语句 12345age = 17if age &gt;= 18: print( &quot;you can seee six video&quot;)else: print(&quot; you should study&quot;) if-elif-else 语句 1234567age = 18if age == 18: print( &quot;you should find girlfriend&quot;)elif age&gt; 18: print(&quot;you can see six video&quot;) else: print(&quot; you should study&quot;) 多个elif 1234567891011age = 80if age == 18: print( &quot;you should find girlfriend&quot;)elif 18&lt;age&lt;30: print(&quot;you can see six video&quot;) elif 30&lt; age &lt; 60: print( &quot;you should go to work&quot;)elif age&gt; 60: print(&quot;you should go to tourism&quot;)else: print(&quot; you should study&quot;) if语句中的and和or 1234567num = 9if num &gt;= 0 and num &lt;= 10: print (&apos;hello&apos;)num = 10if num &lt;= 0 or num &gt;= 10: print(&apos;zy&apos;) while语句 for循环主要用于遍历迭代的对象， while循环主要用于条件判断 1234567891011numbers = [12, 21, 48, 8, 1230, 5, 7]even =[]odd = []while len(numbers) &gt; 0 : number = numbers.pop() if(number % 2 == 0): even.append(number) else: odd.append(number) print(even) continue 用于跳过该次循环 123456789101112numbers = [12, 21, 48, 8, 1230, 5, 7]even =[]odd = []while len(numbers) &gt; 0 : number = numbers.pop() if(number % 2 == 0): even.append(number) else: odd.append(number) continue print(even) break 用于退出循环 12345678910111213numbers = [12, 21, 48, 8, 1230, 5, 7]even =[]odd = []while len(numbers) &gt; 0 : number = numbers.pop() if(number % 2 == 0): even.append(number) else: odd.append(number) continue print(even) break 循环使用 else 语句 123456count = 0while count &lt; 5: print (count, &quot; is less than 5&quot;) count = count + 1else: print (count,&quot; is not less than 5&quot; ) for语句for 循环可以遍历任何序列的项目 12for letter in &apos;python&apos;: print(letter) 函数 函数是组织好， 可重复使用的，用来实现有关功能的代码段。函数能提高应用的模块行，和代码的重复利用率。 定义函数123456def zhangyu(): &quot;&quot;&quot;显示名称&quot;&quot;&quot; print(&quot;zhangandyu&quot;) # return[&apos;a&apos;] zhangyu() 向函数传入参数 1234567def zy(name): print(&apos;Hello, &apos; + name.title() + &apos;!&apos;)zy(&apos;zhangyu&apos;)# name 为形参#&apos;zhangyu&apos;为实参 位置实参 1234567def describe_pet(animal_type, name): print(&apos;I have a &apos; + animal_type) print(&apos;My &apos; + animal_type + &apos;is name is &apos; + name.title() + &apos;.&apos;)describe_pet(&apos;dog&apos;, &apos;huabao&apos;)# 警惕位置混淆describe_pet(&apos;huabao&apos;, &apos;dog&apos;) 关键字实参 123456def describe_pet(animal_type, name): print(&apos;I have a &apos; + animal_type) print(&apos;My &apos; + animal_type + &apos;is name is &apos; + name.title() + &apos;.&apos;)describe_pet(name = &apos;huabao&apos;, animal_type = &apos;dog&apos;)# 位置混淆也没有关系 设置默认值 12345678910def describe_pet(name,animal_type=&apos;dog&apos;): &quot;&quot;&quot;设置animal_type的默认参数是dog&quot;&quot;&quot; print(&apos;I have a &apos; + animal_type) print(&apos;My &apos; + animal_type + &apos;is name is &apos; + name.title() + &apos;.&apos;)# 默认参数不改变describe_pet(name = &apos;huabao&apos;)# 默认参数改变describe_pet(&apos;huxbao&apos;, &apos;cat&apos;) 返回值函数返回的值可以使用return语句将值返回到调用函数的代码行中。从而将程序的大部分繁重工作移到函数中取完成。 12345def zy_name(first_name, last_name): name = first_name + last_name return name.title()zy_name(&apos;zhang&apos;, &apos;yu&apos;) 让实参变成可选的 12345678910def zy_name(first_name, last_name, middle_name =&apos; &apos;): if middle_name: name = first_name +&apos; &apos;+ last_name +&apos; &apos; + middle_name else: name = first_name + &apos; &apos; + last_name return name.title()zy_name(&apos;zhang&apos;, &apos;yu&apos;)zy_name(&apos;zhang&apos;, &apos;and&apos;, &apos;yu&apos;) 返回字典 1234567def build_person(first_name, last_name, age=&apos;&apos;): person = &#123;&apos;first&apos; : first_name, &apos;last&apos;: last_name&#125; if age: person[&apos;age&apos;] = age return personbuild_person(&apos;zhang&apos;, &apos;yu&apos;, age=25) 传递列表 12345def get_user(names): for name in names: print(name.title())zy = [&apos;a&apos;, &apos;b&apos;, &apos;v&apos;]get_user(zy) 传递任意数据的实参 12345def get_world(* names): for i in names: print(i)get_world(&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;) 使用位置实参和任意数量实参 必须将接纳任意数量实参的形参放到最后 1234def get_world(size, *names): for i in names: print(str(size) + i)get_world( 1,&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;) 使用任意数量的关键字实参 1234567def get_user(**user_info): profile = &#123;&#125; for key, value in user_info.items(): profile[key] = value return profileuser_profile = get_user(location = &apos;princeton&apos;, field = &apos;physics&apos;, home = &apos;xian&apos;)print(user_profile) 递归函数如果一个函数在函数内部，调用自己本身，这个函数就是递归函数。 12345def fan(n): if n == 1: return 1 else: return n * fan (n-1) 但递归函数在数特别大的情况下会导致栈溢出， 例如： 1fan(10000) 变量的作用域 局部作用域 1234def func(): name = &quot;zhangyu&quot;print(name) 运行报错， 因为name变量只在func()函数中生效，而在全局无法调用。 作用域链 12345678name = &quot;lzl&quot;def f1(): name = &quot;Eric&quot; def f2(): name = &quot;Snor&quot; print(name) f2()f1() f1()函数执行，最后输出snor，Python中有作用域链， 变量会由内到外找，先去自己作用域找，自己没有再去上级找，直到找不到报错。 终极作用域 1234567891011name = &apos;zhang&apos;def f1(): print(name)def f2(): name = &apos;yu&apos; return f1 ret = f2()ret() 输出结果为zhang, 分析可知， f2()函数执行结果为函数f1的内存地址。执行ret()就是执行f1()，name =’zhang’与fi()在一个作用域链。 12345678910# 新浪面试题li = [lambda : x for x in range(10)]print(type(li))print(type(li[0]))#lambada 面试题 li = [lambda :x for x in range(10)]res = li[0]()print(res) Numpy库 Numpy是科学计算库,特点是有N维数组对象ndarray，是Scipy、Pandas等的基础 array结构 1234567891011121314151617181920212223import numpy as np# 给列表每个元素增加1zy = [1, 2, 3, 4, 5]zy = zy + 1# 错误， 因为列表不支持这样的错误# 用array函数zy = np.array(zy)zy += 1zy# 计算by = np.array([ 2, 3, 4, 5, 6])zy + byzy * byzy ** by# 取值zy[0] zy[2:] 底层为创建ndarray对象，有丰富的可选参数 12345zy.shape # 多维数组的形状type(zy) # 类型zy.dtype # 数组中元素的类型, array内部必须为同一类型， 不同类型会默认进行转换zy.size # 数组中元素个数zy.ndim # 数组的维度 索引 12345678tang_array = np.array([[1, 2, 3], [3, 4, 5], [6, 7, 8]])tang_array[1]tang_array[1, 1]tang_array[:,1]tang_array[1,0:2] 1234567891011tang_array2 = tang_arraytang_array2# 将tang_array2中的4 改为10tang_array2[1,1] = 10tang_array2tang_array# tang_array 中的4也改为了10， 修改tang_array2,实际是指向了tang_array中的内存# 要想修改tang_array2 而不改tang_arra, 需要用copy() 巧用布尔类型 1234567891011121314# 构造等差数组zy = np.arange(0, 100, 10)zy# array([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])# 构造布尔类型by = np.array([0, 0, 1, 0, 1, 1, 1, 0, 1, 0], dtype=bool)by# array([False, False, True, False, True, True, True, False, True,False])zy[by]# array([20, 40, 50, 60, 80]) array数组的数值计算 123456789101112131415161718192021222324252627282930313233343536373839404142zy = np.array([[1, 2, 3, 4], [5, 6, 7, 8]])#所有数据求和 np.sum(zy)# 指定要按什么维度进行计算np.sum(zy,axis = 0)np.sum(zy,axis = 1)# 所有数据乘积zy.prod()zy.prod(axis = 0)zy.prod(axis = -1)# 最小最大值zy.min(axis = 0)zy.max()# 最大值的索引zy.argmax()# 均值zy.mean()zy.mean(axis = 0)# 标准差zy.std(axis = 1)# 方差zy.var()# 进行限制， 小于3的值都变成3， 大于7的值都变成7zy.clip(3, 7)# 进行四舍五入zy.round()# 进行四舍五入到第一个小数点zy.round(decimals = 1) 排序 12345678import numpy as nptang_array = np.array([[1.1, 4.3, 5.2 , 5.1], [5.2, 6.4, 2.3, 4.6]])# 排序np.sort(tang_array)np.sort(tang_array, axis = 0) 数组形状操作 123456789101112zy_num = np.arange(10)zy_num# array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])zy_num.shape = 2, 5zy_num# array([[0, 1, 2, 3, 4],[5, 6, 7, 8, 9]])zy_num.reshape(1,10)#array([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]]) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465# 增加维度zy_num = np.arange(10)zy_num.shape#(10,)zy_num = zy_num[np.newaxis, :]zy_num.shape#(1, 10)zy_num = zy_num[ :, np.newaxis]zy_num.shape#(10, 1)# 压缩维度zy_num = zy_num.squeeze()zy_num#array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])zy_num.shape#(10,)# 转置zy_num.shape = 2,5zy_num#array([[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]])zy_num.transpose() # zy_num.T#array([[0, 5],[1, 6], [2, 7], [3, 8], [4, 9]])# 数组的连接a = np.array([[1,2, 3, 4], [5, 6, 7, 8]])b = np.array([[0, 3, 5, 7], [8, 0, 10, 21]])c = np.concatenate((a, b))c# array([[ 1, 2, 3, 4], [ 5, 6, 7, 8],[ 0, 3, 5, 7], [ 8, 0, 10, 21]])c = np.confatenate((a, b), axis = 1)cnp.vstack((a,b))# array([[ 1, 2, 3, 4],[ 5, 6, 7, 8],[ 0, 3, 5, 7],[ 8, 0, 10, 21]])np.hstack((a,b))# array([[ 1, 2, 3, 4, 0, 3, 5, 7],[ 5, 6, 7, 8, 8, 0, 10, 21]])a# array([[1, 2, 3, 4],[5, 6, 7, 8]])a.flatten()a.ravel()# array([1, 2, 3, 4, 5, 6, 7, 8]) 生成数组 123456789101112131415161718192021222324252627282930np.arange(10)np.arange(2,20,2)# array([ 2, 4, 6, 8, 10, 12, 14, 16, 18])np.arange(2,20,2, dtype= np.float32)# array([ 2., 4., 6., 8., 10., 12., 14., 16., 18.], dtype=float32)# 构造等距数组np.linspace(0, 10, 5)# array([ 0. , 2.5, 5. , 7.5, 10. ])# 构造行向量，列向量np.r_[0:10:1]np.c_[0:10:1]np.zeros(3)# array([0., 0., 0.])np.zeros((3,3))np.ones(3)# array([1., 1., 1.])np.ones((3,3)) * 8zy_num = np.array([1,2,3, 4])np.zeros_like(zy_num)# array([0, 0, 0, 0]) 运算 12345678910111213141516171819202122232425# 乘法x = np.array([5,5])y = np.array([2,3])np.multiply(x,y)# array([10, 15])np.dot(x,y)# array([10, 15])x = np.array([1, 1, 1])y = np.array([[1, 2, 3],[4, 5, 6]])print(x * y)#[[1 2 3] [4 5 6]]x = np.array([1, 1, 2])y = np.array([1, 1, 1])x == y# array([ True, True, False])np.logical_and(x,y)np.logical_or(x,y)np.logical_not(x,y) 随机模块 123456789101112131415161718192021222324252627282930# 随机浮点数np.random.rand(3,2)# 随机整数np.random.randint(10, size = (2,4))# array([[4, 6, 7, 8], [7, 6, 2, 3]])# 随机数np.random.rand()np.random.random_sample()# 0-10中随机找三个数np.random.randint(0, 10, 3)# array([8, 6, 9])# 随机高斯分布mu, sigma = 0, 0.2np.random.normal(mu, sigma, 5)# 设置数字精度, 输出数精度为小数点后3位np.set_printoptions(precision = 3 )# 洗牌， 打乱排列顺序zy_num = np.arange(15)zy_num# array([ 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14])np.random.shuffle(zy_num)zy_num# array([ 1, 5, 4, 11, 14, 9, 8, 6, 7, 3, 0, 2, 10, 12, 13]) 读取写入文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263# 写一个名为tang的txt文件%%writefile tang.txt1 2 3 4 5 6 2 3 4 5 5 7# 读取tang文件data = np.loadtxt(&apos;tang.txt&apos;)data# array([[1., 2., 3., 4., 5., 6.],[2., 3., 4., 5., 5., 7.]])%%writefile tang.txt1, 2, 3, 4, 5, 6 2, 3, 4, 5, 5, 7data = np.loadtxt(&apos;tang.txt&apos;, delimiter = &apos;,&apos;)%%writefile tang.txta, b, c, d, e, f1, 2, 3, 4, 5, 6 2, 3, 4, 5, 5, 7# 不读取第一行data = np.loadtxt(&apos;tang.txt&apos;, delimiter = &apos;,&apos;, skiprows =1)# 指定使用哪几列data = np.loadtxt(&apos;tang.txt&apos;, delimiter = &apos;,&apos;, skiprows =1, usecols = (0,1, 4))# 写入数组文件zy_num = np.array([[1, 2, 3], [4, 5, 6]])np.savetxt(&apos;tang.txt&apos;, zy_num)# 保存成指定格式np.savetxt(&apos;tang.txt&apos;, zy_num, fmt= &apos;%d&apos;)np.savetxt(&apos;tang.txt&apos;, zy_num, fmt= &apos;%.2f&apos;)# 指定分隔符np.savetxt(&apos;tang.txt&apos;, zy_num, fmt= &apos;%d&apos;, delimiter = &apos;,&apos;)# 读写文件zy_num = np.array([[1, 2, 3], [4, 5, 6]])np.save(&apos;zy_num.npy&apos;, zy_num)zy_num = np.load(&apos;zy_num.npy&apos;)# 将两个文件保存在同一文件夹，并进行读写zy_num2 = np.arange(10)np.savez(&apos;zy_npz&apos;, a = zy_num, b = zy_num2)data = np.load(&apos;zy.npz&apos;)data.keys()# [&apos;a&apos;, &apos;b&apos;]data[&apos;a&apos;]# array([[1, 2, 3],[4, 5, 6]]) pandas库Pandas简介Pandas是python数据分析中一个非常核心的数据库， 在日常的工作中经常需要使用Pandas库来对数据进行处理分析。Pandas的核心为两大数据结构， Series和DataFrame，Series用于存储一维数据， 而DataFrame存储多维数据。 常用的软件Anaconda是数据分析中运行python的一款利器， 安装教程可参考Anaconda入门使用指南 Series对象Series用于存储一维数据，由两个相互关联的数组组成， 主数组用来存放数据。主数据每个元素都有一个与之关联的标签，这些标签存储在另一个叫Index的数组中。 创建Series对象 1zy = pd.Series([2, 3, 4, 6, 7, 4], index = [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;, &apos;f&apos;]) 查看Series对象的两个数组 12345# 查看元素zy.values# 查看索引zy.index 查看元素 1234567# 将zy看做Numpy数组，智定键zy[2]zy[0:2]# 指定标签zy[&apos;c&apos;]zy[[&apos;b&apos;, &apos;c&apos;]] 筛选元素 1zy[zy &gt;3] 查看组成元素 123# 查看包含的不同元素zy.unique()zy.value_counts() 通过字典来创建 1zy=Series(&#123;&apos;a&apos;:1,&apos;b&apos;:2,&apos;c&apos;:3&#125;) DataFrame对象 读取与写入Excel数据 读取文件夹的内容 12345678import pandas as pdimport numpy as npfrom pandas import Series, DataFrameimport osfile_list = os.listdir(r&apos;E:\工作文件\周报\周数据\测试\0902-0908&apos;)print(file_list, &apos;\t&apos;) 读取xls格式Excel表 12df = pd.read_excel(&apos;E:/工作文件/周报/周数据/测试/0902-0908/an-商品汇总-uv.xls&apos;)df = pd.read_excel(r&apos;E:\工作文件\周报\周数据\测试\0902-0908\an-商品汇总-uv.xls&apos;) 读取csv格式Excel表 1df = pd.read_csv(&apos;E:/工作文件/周报/周数据/测试/0902-0908/商品汇总.scsv&apos;) 读取txt格式数据 123456df = pd.read_table(r&apos;C:\Users\Administrator\Desktop\haha.txt&apos;)with open(r&apos;C:\Users\Administrator\Desktop\haha.txt&apos;, &apos;r&apos;) as f: df = f.readlines() df = np.loadtxt(r&apos;C:\Users\Administrator\Desktop\haha.txt&apos;) # 将txt文件存为numpy数组 将数据写入Excel表， 并输出 123data.to_excel(&apos;C:/Users/Administrator/Desktop/&apos;+&apos;商品分类.xlsx&apos;)data.to_excel(r&apos;C:\Users\Administrator\Desktop\\&apos;+&apos;商品分类.xlsx&apos;)data.to_excel(r&apos;C:\Users\Administrator\Desktop/&apos;+&apos;商品分类.xlsx&apos;) 其他数据格式 1234567891011121314151617181920# 从SQL表/库导入数据 pd.read_sql(query, connection_object)# 从JSON格式的字符串导入数据 pd.read_json(json_string)# 解析URL、字符串或者HTML文件，抽取其中的tables表格 pd.read_html(url)# 从你的粘贴板获取内容，并传给read_table() pd.read_clipboard()# 从字典对象导入数据，Key是列名，Value是数据pd.DataFrame(dict)# 导出数据到SQL表 df.to_sql(table_name, connection_object)# 以Json格式导出数据到文本文件df.to_json(filename) 描述数据 表信息 1df.info() 显示数据的行列数 1df.shape 查看数据格式dtpyes 1df.dtypes 显示列名、元素 12df.columnsdf.values 添加默认列名 12# 如果数据没有标题行，可用pandas添加默认的列名df = pd.read_excel(&apos;x.xlsx&apos;, header = None) 显示前数据前5行 12df.head(5)df[[&apos;标题&apos;, &apos;客户端uv&apos;]].head() 显示数据后5行 1df.tail(5) 值 1df.values 读取a列 1df[&apos;a&apos;] 修改索引 1df = df.set_index[&apos;标题&apos;] 显示数据唯一值（unique函数） 12# 数据有0， 是因对缺失值进行了填充df[&apos;经纪人级别&apos;].unique() 对第几行数据不读取 12#不读取哪里数据，可用skiprows=[i]，跳过文件的第i行不读取df = pd.read_excel(&apos;x.xlsx&apos;,skiprows=[2] ) 对缺失值进行识别 12# 所有缺失值显示为Truepd.insull(df) # df.isnull() 计算 1234567891011#计算此data的数量df[&apos;data&apos;].value_counts()# 升序计数df[&apos;data&apos;].value_counts(ascending = True)# 升序计数并分组df[&apos;data&apos;].value_counts(ascending = True, bins = 2)# 计数df[&apos;data&apos;].count() 数据清洗 删除空值 （dropna函数） 1df.dropna(how=&apos;any&apos;) 填充空值（fillna函数） 12345# 空值用0填充df.fillna(value=0)# 用均值对空值进行填充df[&apos;经纪人响应时长&apos;].fillna(df[&apos;经纪人响应时长&apos;].mean()) 更改数据格式 12# 将数据格式int64,改为float格式df[&apos;大区&apos;].astype(&apos;float64&apos;) 更改列名称 1df.rename(columns=&#123;&apos;IM渠道&apos;: &apos;渠道&apos;&#125;) 找到重复值 1df.duplicated() 删除重复值 12345# 默认第一次出现的保留，其余删除df[&apos;门店&apos;].drop_duplicates()最后一次出现的保留，其余删除df[&apos;门店&apos;].drop_duplicates(keep = &apos;last&apos;) 对列表内的值进行替换 1df[&apos;客户UCID&apos;].replace(&apos;10531975&apos;, &apos;110&apos;) 找出异常值 12print(data.describe())# 对异常值进行删除 修改数据 1234567891011121314# 修改结果df.replace(参数)# 修改索引df.rename(参数)# 增加df.append(参数)# 删除def df[&apos;a&apos;]df.drop([&apos;a&apos;, &apos;b&apos;], inplace = True) 对数据进行处理 对两个数据进行合并- mearge, join, concat函数 12345678910111213141516171819202122232425262728293031# 按照轴把多个对象拼接起来pd.concat(df1, df2)# join函数适合根据索引进行合并，合并索引相同但列不同的对象# merge函数，根据一个或多个键连接多行left = pd.DataFrame(&#123;&apos;key&apos;:[&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k3&apos;], &apos;key2&apos; : [&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k3&apos;], &apos;A&apos; :[&apos;ao&apos;,&apos;a1&apos;,&apos;a2&apos;,&apos;a3&apos; ], &apos;B&apos; : [&apos;bo&apos;,&apos;b1&apos;,&apos;b2&apos;,&apos;b3&apos; ]&#125;)right =pd.DataFrame(&#123;&apos;key&apos;:[&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k3&apos;], &apos;key2&apos; : [&apos;ko&apos;,&apos;k1&apos;,&apos;k2&apos;,&apos;k4&apos;], &apos;c&apos; :[&apos;co&apos;,&apos;c1&apos;,&apos;c2&apos;,&apos;c3&apos; ], &apos;d&apos; : [&apos;do&apos;,&apos;d1&apos;,&apos;d2&apos;,&apos;d3&apos; ]&#125;)# 将left和right进行合并pd.merge(left, right)# 指定以key为键进行合并pd.merge(left, right, on = &apos;key&apos;)# key2列不相同的部分会直接舍弃掉pd.merge(left, right, on = [&apos;key&apos;, &apos;key2&apos;])# 保留key2列不相同的部分pd.merge(left, right, on = [&apos;key&apos;, &apos;key2&apos;], how = &apos;outer&apos;)# 不相同的部分指定以左表为基准pd.merge(left, right, on = [&apos;key&apos;, &apos;key2&apos;], how = &apos;left&apos;) 对数据进行排序 12345data =pd.DataFrame(&#123; &apos;group&apos;:[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;a&apos;], &apos;data&apos; : [4, 2, 5, 6, 7, 8, 2, 9, 4]&#125;)# 在保证group列降序的情况下，对data列进行升序处理data.sort_values(by = [&apos;group&apos;, &apos;data&apos;],ascending = [False, True], inplace = True) 对数据进行分组——excel中的数据透视表 123456789# 如果price列的值&gt;3000，group列显示high，否则显示lowdf[&apos;group&apos;] = np.where(df[&apos;客户当天发送消息数&apos;] &gt; 5,&apos;high&apos;,&apos;low&apos;)# 对符合多个条件进行分组# 符合经纪人级别为A1且经纪人响应时长&gt;24的在sign列显示为1df.loc[(df[&apos;经纪人级别&apos;] == &apos;A1&apos;) &amp; (df[&apos;经纪人响应时长&apos;]&gt;= 24.0), &apos;sign&apos;]=1 对数据进行分列 12pd.DataFrame((x.split(&apos;网&apos;) for x in df[&apos;客户注册渠道&apos;]), index=df.index,columns=[&apos;客户注册渠道&apos;,&apos;size&apos;]) 新增一列 123data = data.assign(ration = [4, 2, 5, 6, 7, 8, 2, 9, 4])data[&apos;rations&apos;] = [5, 2, 5, 6, 7, 8, 2, 9, 4] 对数据进行切分 12bins = [1,3,6,9]data_cut = pd.cut(data[&apos;data&apos;], bins) 对数据进行提取,筛选 12df = pd.DataFrame(&#123;&apos;A&apos;:[7,8,9,20, 10, 11, 14, 13, 14], &apos;B&apos; : [1,2,3,4,5, 6, 7, 7, 8]&#125;) 按条件进行提取 1234567891011121314151617181920212223242526272829# 选出B列的值大于3的数df[df[&apos;B&apos;]&gt;3]# 当 A列的值大于13时， 显示B，c列的值df[[&apos;B&apos;,&apos;C&apos;]][df[&apos;A&apos;]&gt;13]# 用isin函数进行判断# 使用isin函数根据特定值筛选记录。筛选A值等于10或者13的记录df[df.A.isin((10, 13))]# 判断经纪人级别是否为A3df[&apos;经纪人级别&apos;].isin([&apos;A3&apos;]) # 先判断结果，将结果为True的提取#先判断经纪人级别列里是否包含A3和M4，然后将复合条件的数据提取出来。df.loc[df[&apos;经纪人级别&apos;].isin([&apos;A3&apos;,&apos;M4&apos;])]# 使用&amp;（并）与| （或）操作符或者特定的函数实现多条件筛选 # A列值大于10， 并且B列值大于5df[(df[&apos;A&apos;] &gt; 10) &amp; (df[&apos;B&apos;] &gt;5)]df[np.logical_and(df[&apos;A&apos;] &gt; 10, df[&apos;B&apos;] &gt; 5)]# A列值大于10，或 B列值大于5df[(df[&apos;A&apos;] &gt; 10) | (df[&apos;C&apos;] &gt;20)]df[np.logical_or(df[&apos;A&apos;] &gt; 10, df[&apos;C&apos;] &gt; 20)] 按索引进行提取 12345678910111213141516171819202122232425262728293031323334353637383940414243# 按标签索引df[1:4]# 传入列名df[[&apos;A&apos;, &apos;B&apos;]]# loc函数# 知道column names 和index(这里df的index没有指定，是默认生成的下标)，且两者都很好输入，可以选择 .loc同时进行行列选择# 根据标签取第一行， 显示为DataFrame格式df.loc[:0]# 取标签为2,3,4， A列的数据， 显示为Series格式df.loc[2:4, &apos;A&apos;]# iloc函数# 行和列都用index来进行提取df.iloc[0:5, 1:3] # 返回第一行 df.iloc[0,:]# 返回第一列的第一个元素df.iloc[0,0]#[0, 2, 5] 代表指定的行，[ 4, 5 ] 代表指定的列df.iloc[[0,2,5],[4,5]]# ix#ix的功能更加强大，参数既可以是索引，也可以是名称，相当于，loc和iloc的合体df.ix[1:3, [&apos;A&apos;, &apos;B&apos;]]# at函数根据指定行index及列label，快速定位DataFrame的元素，选择列时仅支持列名df.at[3, &apos;A&apos;]# iat函数选择时只使用索引参数df.iat[3, 2] 按日期进行提取 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152import pandasimport datetime as dt# 重新设置索引df.reset_index()#设置日期为索引df=df.set_index(&apos;日期&apos;)#提取2016年11月2号的数据df[&apos;2016-11-02&apos; : &apos;2016-11-02&apos;]dt_time = dt.datetime(year = 2018, month=9, day = 17, hour = 22, minute = 43)print(dt_time)#构造时间ts = pd.Timestamp(&apos;2018-09-17 22:43:00&apos;)ts = pd.to_datetime(&apos;2018-09-17 22:43:00&apos;)ts = pd.to_datetime(&apos;17/09/2018 22:43:00&apos;)# 月份ts.month#日期ts.day# 加日期ts + pd.Timedelta(&apos; 10 days&apos;)ts.hour# 构造时间序列， 构造十个日期， 每12分钟一次pd.Series(pd.date_range(start = &apos;2018-09-17 22:43:00&apos;, periods = 10, freq = &apos;12min&apos;))读取文件， 有时间列， 先将时间字符串转换成时间格式， 再进行处理或当读取数据时， 就对数据格式进行修改data = pd.read_csv(&apos;.../db.csv&apos;, index_col = 0, parse_dates = True)# 读取时间为2013年的所有数据data[&apos;2013&apos;]# 取所有8点到12点之间的数据, 不包含8点和12点data[(data.index.hour &gt; 8) &amp; (data.index.hour &lt; 12)]# 包含8点到12点data.between_time(&apos;08:00&apos;, &apos;12:00&apos;)# 时间序列的重采样-看每月的平均值data.resample(&apos;M&apos;).mean() 数据汇总 对数据进行分类 - group by函数 123456789101112131415161718192021222324# 创建数组df = pd.DataFrame(&#123;&apos;key&apos; : [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;,&apos;a&apos;, &apos;b&apos;, &apos;c&apos;,&apos;a&apos;, &apos;b&apos;, &apos;c&apos;], &apos;data&apos; : [0, 2, 4, 5, 6, 7, 8, 9, 4]&#125;)# 分别计算a, b, c 的和df.groupby(&apos;key&apos;)[&apos;data&apos;].sum()df.groupby(&apos;key&apos;)[&apos;data&apos;].mean()s = pd.Series([1, 2, 3,1, 2, 3],[8,7,6,8,7,6])# 对索引进行排序grouped = s.groupby(level = 0， sort =False)grouped.first()df2 = pd.DataFrame(&#123;&apos;x&apos;:[&apos;a&apos;, &apos;b&apos;, &apos;a&apos;, &apos;b&apos;], &apos;y&apos; : [1, 2, 3, 4]&#125;)# 只关注x中的bdf3 = df2.groupby([&apos;x&apos;]).get_group(&apos;b&apos;)# 查看个数df2.size() 2, 对数据进行透视12345678910111213pd.pivot_table(data, values=None, index=None, columns=None, aggfunc=&apos;mean&apos;)df = pd.DataFrame(&#123;&quot;A&quot;: [&quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;foo&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;, &quot;bar&quot;], &quot;B&quot;: [&quot;one&quot;, &quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;two&quot;, &quot;one&quot;, &quot;one&quot;, &quot;two&quot;, &quot;two&quot;], &quot;C&quot;: [&quot;small&quot;, &quot;large&quot;, &quot;large&quot;, &quot;small&quot;, &quot;small&quot;, &quot;large&quot;, &quot;small&quot;, &quot;small&quot;, &quot;large&quot;], &quot;D&quot;: [1, 2, 2, 3, 3, 4, 5, 6, 7]&#125;)table = pd.pivot_table(df, values=&apos;D&apos;, index=[&apos;A&apos;, &apos;B&apos;], columns=[&apos;C&apos;], aggfunc=np.sum) 对数据进行映射 12# 用map函数对字典进行映射， 新加一列data[&apos;upper&apos;] = data[&apos;group&apos;].map(dataUpper) 数据统计 数据采样 12345678910111213# 简单随机抽取sampledf.sample(n=3)# 设置采样权重# 需要对每一行进行权重设置，列表行数少可行，过多不可行# 假设有4行数据，设置采样权重weights = [0, 0, 0.5, 0.5]df.sample(n=4, weights=weights)## 确定采样后是否放回# 采样后放回，Truedf.sample(n=6, replace=True) 统计计算 1234567891011121314151617181920# 描述统计 describe函数#自动生成数据的数量，均值，标准差等数据#round（2）,显示小数点后面2位数，T转置df.describe().round(2).T# 标准差std()df[&apos;经纪人响应时长&apos;].std()# 协方差covdf[&apos;经纪人当天发送消息数&apos;].cov(df[&apos;客户当天发送消息数&apos;]# 相关性分析corrdf[&apos;客户当天发送消息数&apos;].corr(df[&apos;经纪人当天发送消息数&apos;])# 中位数df.median() 对字符串进行操作 大小写 12a.lower()a.upper() 长度 12# 长度a.len() 去除空格 123a.strip()a.lstrip()alrstrip() 替换 1df.columns.str.replace(&apos; &apos;, &apos;_&apos;) 切分与分列 12345678910#切分a.split(&apos;_&apos;)# 切分， 且成为新列a.split(&apos;_&apos;, expand = True)# 对切分进行限制, 只切1次a.split(&apos;_&apos;, expand = True, n=1)# 查看是否包含a.str.contains(&apos;A&apos;)# 分列s.str.get_dummies(sep= &apos;|&apos;)]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python爬虫]]></title>
    <url>%2F2018%2F07%2F12%2FPython%E7%88%AC%E8%99%AB%2F</url>
    <content type="text"><![CDATA[爬取英雄联盟-英雄皮肤图片1. 前言最近自己在学爬虫， 有天朋友问我能否爬取英雄联盟的皮肤图片到本地，好实现快速浏览，折腾了半个小时，终于成功了。 2. 过程分析过程找到皮肤图片链接， 研究规律在抓取图片之前，我们需要分析网址链接的构成， 以便找到其中的规律。 打开英雄联盟网站, 点击其中的一个英雄， 我们可以看到一个英雄有1-6个皮肤甚至更多，且我们很容易从每个皮肤链接中找到规律。 123456789# 英雄1http://ossweb-img.qq.com/images/lol/web201310/skin/small266000.jpghttp://ossweb-img.qq.com/images/lol/web201310/skin/small266001.jpghttp://ossweb-img.qq.com/images/lol/web201310/skin/small266002.jpg# 英雄2http://ossweb-img.qq.com/images/lol/web201310/skin/small103000.jpghttp://ossweb-img.qq.com/images/lol/web201310/skin/small103001.jpghttp://ossweb-img.qq.com/images/lol/web201310/skin/small103002.jp 从以上的链接中，我们可以知道英雄皮肤的链接规律为：1&quot;http://ossweb-img.qq.com/images/lol/web201310/skin/small&quot; + &quot;英雄代号&quot; + &quot;0&quot; + &quot;01-10&quot; 找到每个英雄对应的数字代号那么我们需要解决的问题就变成了到每个英雄对应的代号是多少？ 通过搜索，我们发现每个英雄对应的代号存在champion.js文件中 从Headers中， 我们可以看到champion.js 对应的url为：1http://lol.qq.com/biz/hero/champion.js 我们通过正则表达式， 把js中对应的英雄代号提取出来。 通过以上把链接拼凑起来，我们就可以把链接对应的图片皮肤下载到本地了。 3. 代码1234567891011121314151617181920212223242526272829import requestsimport reimport jsonimport urlliburl = &quot;http://lol.qq.com/biz/hero/champion.js&quot;hd =&#123;&apos;User-Agent&apos;:&quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.802.30 Safari/535.1 SE 2.X MetaSr 1.0&quot;&#125;data = requests.get(url,headers = hd).contentdatas = data.decode()pat = &apos;&quot;keys&quot;:(.*?),&quot;data&quot;&apos;imglist = re.findall(pat,datas)datass = json.loads(imglist[0])for i in datass: try: for j in range(12): try: num = str(j) # print(num) if len(num) == 1: hero_num = &quot;00&quot; + num elif len(num) ==2: hero_num = &quot;0&quot; + num numstr = i + hero_num urls = &apos;http://ossweb-img.qq.com/images/lol/web201310/skin/big&apos;+ numstr +&apos;.jpg&apos; localfile = &quot;E:/张宇个人文件/英雄联盟/&quot; + str(i) + str(num) + &quot;.jpg&quot; urllib.request.urlretrieve(urls, filename = localfile) except Exception as err: pass except Exception as err: pass 爬取王者荣耀-英雄图片代码12345678910111213141516171819202122232425262728# 用python爬取王者荣耀皮肤import requestsimport reimport urlliburl = &quot;http://pvp.qq.com/web201605/herolist.shtml&quot;hd =&#123;&apos;User-Agent&apos;:&quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/535.1 (KHTML, like Gecko) Chrome/14.0.802.30 Safari/535.1 SE 2.X MetaSr 1.0&quot;&#125;data = requests.get(url,headers = hd)pat = &apos;a href=&quot;herodetail/(.*?).shtml&apos;imglist = re.compile(pat, re.S).findall(data.text)for i in imglist: # print(i) try: for j in [1,2,3,4,5,6]: try: numstr = str(i)+&apos;/&apos; +str(i)+&apos;-mobileskin-&apos;+ str(j) # print(numstr) urls = &apos;https://game.gtimg.cn/images/yxzj/img201606/heroimg/&apos;+numstr+&apos;.jpg&apos; print(urls) localfile = &quot;E:/张宇个人文件/官网图片/&quot; + str(i)+ str(j)+ &quot;.jpg&quot; urllib.request.urlretrieve(urls, filename = localfile) except Exception as err: pass except Exception as err: pass 爬取网站美女图片代码构建用户代理池1234567891011121314# 这里可以随意加多个浏览器uapools = [ &quot;Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)&quot;, &quot;Mozilla/5.0 (compatible; MSIE 10.0; Windows NT 6.2; WOW64; Trident/6.0)&quot;, &quot;Mozilla/5.0 (Windows NT 6.3; Trident/7.0; rv:11.0) like Gecko&quot;, &quot;Mozilla/5.0 (compatible; MSIE 10.0; Windows Phone 8.0; Trident/6.0; IEMobile/10.0; ARM; Touch; NOKIA; Lumia 920)&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64; rv:6.0.2) Gecko/20100101 Firefox/6.0.2&quot;, &quot;Opera/9.80 (Windows NT 6.1; WOW64) Presto/2.12.388 Version/12.12&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/28.0 Safari/537.36 OPR/15.0&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.17 (KHTML, like Gecko) Chrome/24.0.1312.57 Safari/537.17&quot;, &quot;Mozilla/5.0 (X11; CrOS armv7l 3428.193.0) AppleWebKit/537.22 (KHTML, like Gecko) Chrome/25.0.1364.126 Safari/537.22&quot;, &quot;Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/534.57.2 (KHTML, like Gecko) Version/5.1.7 Safari/534.57.2&quot;, &quot;Mozilla/5.0 (Windows; U; Windows NT 5.1; zh-CN) AppleWebKit/533.9 (KHTML, like Gecko) Maxthon/3.0 Safari/533.9&quot;,] 爬取并下载图片12345678910111213141516171819202122232425import reimport requestsimport urllib.request# uapools 如上所示for ua in uapools: hd =&#123;&apos;User-Agent&apos;:ua&#125; i = uapools.index(ua) # 限制爬取页数， 我们爬取前10页 if i &gt; 10: break try: url = &quot;http://www.iyuanqi.com/home/funimg/fun_list/m/Home/cp_uid/all/sort/30hot/p/&quot;+str(i)+&quot;.html&quot; data = requests.get(url, headers = hd) pat = &apos;class=&quot;lazy-img&quot; src=&quot;(.*?)&quot; data-original=&quot;&apos; imglist = re.compile(pat, re.S).findall(data.text) for j in range(0, len(imglist)): try: thisimg = imglist[j] thisimgurl = thisimg localfile = &quot;E:/张宇个人文件/网络图片/&quot; + str(i) + str(j) + &quot;.jpg&quot; urllib.request.urlretrieve(thisimgurl, filename = localfile) except Exception as err: pass except Exception as err: pass 爬取天善课程数据表存储到MYSQL前言天善智能是一个商业智能与大数据在线社区，有很多很好的学习课程。我们用爬虫来爬取网站的所有课程并存储到MYSQL数据库中， 以便于进一步的分析。 用python在MYSQL中创建名为zhanhyu的数据库 用python连接MYSQL数据库 1234567import pymysql# 因为本地mysql没有设置密码， 所以没有加password参数db = pymysql.connect(host = &apos;localhost&apos;, user = &apos;root&apos;, port = 3306)# 用cursor()方法获取MYSQL的操作游标， 利用游标来执行SQL语句cursor = db.cursor() 创建一个新的数据库， 名字叫做zhangyu 12# cursor.execute 执行真正的sql语句, DEFAULT 指定默认值cursor.execute(&quot;CREATE DATABASE zhangyu DEFAULT CHARACTER SET utf8&quot;) 在zhangyu库中创建tianshan2_datas的数据表 指定在zhangyu这个数据库中运行 12db = pymysql.connect(host = &apos;localhost&apos;, user = &apos;root&apos;, port = 3306, db=&apos;zhangyu&apos;)cursor = db.cursor() 用sql语句创建名为tianshan2_datas的表 12345sql = &apos;CREATE TABLE IF NOT EXISTS tianshan2_datas (name VARCHAR(255) NOT NULL, pirce VARCHAR(255) NOT NULL,numbers VARCHAR(255), PRIMARY KEY (name))&apos;curosr.exectute(sql)db.close() 爬取天善智能网站的数据12345678910111213141516171819202122232425262728import reimport requestsfor i in range(1,5): # 观察天善课程链接， 找出规律 thisurl = &quot;https://edu.hellobi.com/course/&quot; + str(i+1) # 用requests库抓取数据 hd =&#123;&quot;user-agent&quot;: &quot;Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Mobile Safari/537.36&quot;&#125; data = requests.get(thisurl, headers = hd) #用正则表达式进行解析 title_pat = &apos;&lt;li class=&quot;active&quot;&gt;(.*?)&lt;/li&gt;&apos; price_pat = &apos;class=&quot;price-expense&quot;&gt;&lt;sub&gt;￥&lt;/sub&gt;(.*?)&lt;/span&gt;&apos; numb_pat = &apos;class=&quot;course-view&quot;&gt;(.*?)&lt;/span&gt;&apos; title = re.compile(title_pat, re.S).findall(data.text) if(len(title)&gt;0): title = title[0] else: continue price = re.compile(price_pat, re.S).findall(data.text) if(len(price)&gt;0): price = price[0] else: price = &apos;免费&apos; numb = re.compile(numb_pat, re.S).findall(data.text) if(len(numb)&gt;0): numb = numb[0] else: numb = &apos;缺失&apos; 将爬取的数据存储到名为zhangyu数据库的tianshan2_datas表中1234567891011con = pymysql.connect(host = &apos;localhost&apos;, user = &apos;root&apos;, port = 3306, db = &apos;zhangyu&apos;)cursor = con.cursor()sql = &apos;insert into tianshan2_datas(name, pirce, numbers) values(%s,%s,%s)&apos;try: cursor.execute(sql, (title, price, numb)) con.commit()except: con.rollback()con.close() 这样，我们就成功的把爬取的数据保存到mysql数据库中，方便我们查询使用。 完整代码123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import pymysql# 因为本地mysql没有设置密码， 所以没有加password参数db = pymysql.connect(host = &apos;localhost&apos;, user = &apos;root&apos;, port = 3306)# 用cursor()方法获取MYSQL的操作游标， 利用游标来执行SQL语句cursor = db.cursor()# cursor.execute 执行真正的sql语句, DEFAULT 指定默认值cursor.execute(&quot;CREATE DATABASE zhangyu DEFAULT CHARACTER SET utf8&quot;)db = pymysql.connect(host = &apos;localhost&apos;, user = &apos;root&apos;, port = 3306, db=&apos;zhangyu&apos;)cursor = db.cursor()sql = &apos;CREATE TABLE IF NOT EXISTS tianshan2_datas (name VARCHAR(255) NOT NULL, pirce VARCHAR(255) NOT NULL,numbers VARCHAR(255), PRIMARY KEY (name))&apos;cursor.execute(sql)db.close()import reimport pymysqlimport requestsfor i in range(0,284): thisurl = &quot;https://edu.hellobi.com/course/&quot; + str(i+1) hd =&#123;&quot;user-agent&quot;: &quot;Mozilla/5.0 AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 Mobile Safari/537.36&quot;&#125; data = requests.get(thisurl, headers = hd) title_pat = &apos;&lt;li class=&quot;active&quot;&gt;(.*?)&lt;/li&gt;&apos; price_pat = &apos;class=&quot;price-expense&quot;&gt;&lt;sub&gt;￥&lt;/sub&gt;(.*?)&lt;/span&gt;&apos; numb_pat = &apos;class=&quot;course-view&quot;&gt;(.*?)&lt;/span&gt;&apos; title = re.compile(title_pat, re.S).findall(data.text) if(len(title)&gt;0): title = title[0] else: continue price = re.compile(price_pat, re.S).findall(data.text) if(len(price)&gt;0): price = price[0] else: price = &apos;免费&apos; numb = re.compile(numb_pat, re.S).findall(data.text) if(len(numb)&gt;0): numb = numb[0] else: numb = &apos;缺失&apos; con = pymysql.connect(host = &apos;localhost&apos;, user = &apos;root&apos;, port = 3306, db = &apos;zhangyu&apos;) cursor = con.cursor() sql = &apos;insert into tianshan2_datas(name, pirce, numbers) values(%s,%s,%s)&apos; try: cursor.execute(sql, (title, price, numb)) con.commit() except: con.rollback() con.close()]]></content>
      <categories>
        <category>编程语言</category>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>爬虫</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[刻意练习]]></title>
    <url>%2F2018%2F04%2F16%2F%E3%80%8A%E5%88%BB%E6%84%8F%E7%BB%83%E4%B9%A0%E3%80%8B%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[1. 什么是刻意练习 刻意练习是一个在已经有明确方法论的行业内， 个人通过制定一系列明确的目标， 不断进行刚好超出他们能力范围的练习， 并通过检验反馈不断地对练习进行调整， 从而创建有效的知识晶体， 保存在长时记忆中， 以便以后遇到问题及时响应。 2. 刻意练习背后的原理是什么 利用身体偏爱稳定的倾向，进行刚好超出能力范围的练习我们人类的身体天生偏爱稳定性， 我们的身体通过各种各样的反馈机制来保持身体各项指标的稳定性。当身体系统长时间的感受到压力，原来的平衡再无法保持时， 身体便会开始响应那些变化，让那些变化更加容易， 进而达到重新的平衡。但在过长的时间内过分的逼迫自己， 可能导致倦怠和低效。 例如：对于跑步锻炼来说， 如果你短时间剧烈的运动，导致身体中的能量与氧气下降， 身体就会通过心跳加速以提高氧气供给，并将储存在不同部位的能量拿来给肌肉供给，以达到平衡状态。只要体育锻炼并未让身体平衡机制无法正常运转，就很难引起身体上的生理变化。 因此， 你需要足够努力的锻炼并保持足够长的时间，才能让身体形成新的平衡。要想要改变不断地进行下去， 你就需要不断地加码：跑的更远，更快，负重跑等。 一旦你不给自己在跑步方面施加压力， 你将停止改进的脚步，停留在新的平衡内。 但如果你一上来玩命的跑， 可能只让自己受伤。 同理，对大脑进行长时间的锻炼， 大脑也会以各种不同的方式来重新布置神经元之间的连接，以达到快速地相应。 为了创建有效的知识晶体 知识晶体就是我们思考某件事物时心理所创建的知识结构。 刻意练习的目的之一就是创建有效的知识晶体。 信息预先存在这些晶体中，并长时间保存在记忆之中，当生活中遇到类似的情况可快速地进行响应。 行业内的杰出人物正式由于他们经过多年的积累，针对行业中可能遇到的不同局面，创建了高度复杂和精密的知识晶体。反过来这些知识晶体让他们更好地在一系列事物中找到规律，更好地理解信息，指定计划，高效的学习。 比如： 你听到‘猫’这个词就会想到毛茸茸可爱的猫，它的样子，叫声等具体的内容。你在生活中对‘猫’这个词创建了包括图像， 气味，声音等一系列的晶体结构。 同理，我们要想更好地创建对某一动物的知识晶体，最好的方法就是花一点点的时间来了解它们，摸摸它的毛发，和它玩耍，并且细心地观察它的一举一动。 3. 如何在一个行业中进行刻意练习 找到一位好的导师 如果可以的话， 找到一个好的导师能够让自己的练习事半功倍。 好的导师能够了解什么样的行为会带来进步，能带来及时的反馈。 找行业中的大牛 我们在现实中很难找到一个好的导师， 但我们在互联网中可以很容易找到行业中的大牛。 我们首先确定大牛的指标都有哪些， 然后调查思考谁符合这些指标，算的上是真正的大牛。 在数据分析行业， 称为大牛的特征有：有多年的行业积累， 有大厂的工作经历。 有较大的行业影响力， 愿意传播教授 技能。 通过搜索，我们可以知道：数据挖掘与数据分析博主-邓凯是数据分析里的大牛， 他在数据分析行业工作多年， 并在京东这样的大厂担任数据负责人， 微信公众号有数十万粉丝，现在成立了爱数圈这样的学习团体。 观察大牛都做了什么 观察他们是做了什么让他们如此的杰出，运用了哪些方法让他们如此的卓越。 不断的通过工作业务磨练自己的数据分析思维，建立了良好的互联网分析能力。 他不断地写数据分析的相关文章，总结输出，让自己不断扩大影响力。这些方法途径，也是自己在进行技能学习时可以学习借鉴的方法。 学会分解目标 找到一种适合自己的练习方法，并将漫长的目标分解成一个一个的小目标，每次练习都只专注于这一个目标，当达到目标时， 给自己一个小小的奖赏 最重要的是盯紧自己的目标。 找到自己的练习规律 保证自己在短的时间内能够集中全部的注意力去练习。 一旦自己发现自己不能够保持专注力，就停下来休息。 经过自己这段时间的统计发现，自己能够保持专注学习的时间为一个小时， 超过一个小时自己就看不进去了。 这个时候，自己停下来放松10分钟再看，效果会好的多。 在工作中需要必要的反馈 给自己设计某种必要的反馈， 让自己能够随着时间的推移， 不断的纠正错误和精进技巧。 如何创建反馈， 我觉得可以通过写作来给自己提供反馈，通过输出来倒逼输入， 在写作中发现自己的问题， 比如自己在写这篇读书笔记时就发现自己有很多的概念没有理解。 创建自己对于这个技能的知识晶体 将工作中的项目经验和学到的知识相结合，构成强大的晶体结构。培养自己能够遇到问题迅速的响应能力。多培养自己遇到问题的解决思路。 隐形知识 寻求建议 当遇到停滞阶段时， 稍微给自己加强练习的强度，找出到底是在哪里让你停滞不前， 然后尝试换一种方法专门针对这个缺点来进行练习，或向大牛寻求建议。 自己的打字速度现在停滞不前 在练习的过程中保持动机 给自己制定一个专门的时间点来进行练习，并想办法把干扰你的事物控制到最小。坚定自己可以通过刻意练习可以进步的信念。有可能的话， 加入一个社区进行学习比自己单独学习更容易坚持 比如：把手机调静音，去图书馆学习防止网络对自己的影响。 保持充足的睡眠，加入一个数据分析的圈子进行学习。 保证错误是低风险的。这样自己才能敢于犯错。 练习， 试错， 反馈，修正 应用 当自己在进行学习时， 自己总是会想到刻意练习里的一个观点， 就是在学习过程中必须脱离自己的舒适区，让学习的内容稍微难一点， 这样的学习才会让自己进步。 每当这样想， 自己就不会抵触学习的过程了， 也让自己能够长久的坚持。 –2018-12-08 刻意练习区别与其他练习的特征是什么 有定义明确的特定目标把大目标分解成每一个小目标，制定计划，在达成每一个小目标的过程中，纠正自己的行为方式，解决面临的问题。 具有专注的练习状态尽力保持专注，集中精力，不会走神 练习包含反馈你必须知道自己做的对不对，如果不对，又错在哪里。 走出舒适区 刻意练习的原则专注、反馈、纠正、足够的重复次数 延伸： TED演讲： 《how to get better at the things you care about》 刻意练习的本质-阳志平公众号：心智工具箱网站：http://www.yangzhiping.com/ 论文：the role of Deliberate Practice in the Acquisition of Expert Performance]]></content>
      <categories>
        <category>个人系统</category>
        <category>学习</category>
      </categories>
      <tags>
        <tag>学习方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[学习双拼输入法的心得]]></title>
    <url>%2F2017%2F09%2F17%2F%E5%8F%8C%E6%8B%BC%E8%BE%93%E5%85%A5%E6%B3%95%2F</url>
    <content type="text"><![CDATA[1. 怎么接触到双拼的？自己第一次接触双拼，是看到李笑来老师的《把时间当朋友》第二章中的“盲打究竟是否值得学会”,里面提到了盲打与双拼帮助他快速进行记录笔记和文字。 于是自己就试着在网上找双拼的学习方法。 2. 什么是双拼？ 维基百科：双拼是汉语拼音输入法的一种编码方案。相对于全拼而言，使用双拼输入汉字时只需输入一个代表声母的字母，一个代表韵母的字母，就可以打出任意一个中文单字了。 理解起来也很简单，比如你要拼写 张 红 这两个字， 用全拼的话可能你得输入 zhang hong， 而用小鹤双拼的话， 你只需要输入vh hs 就可以显示。 v 代表zh , h 代表ang , s代表ong 双拼的语法也有很多种，比如小鹤双拼、自然码双拼、紫光拼音双拼、搜狗拼音双拼、微软拼音双拼、智能ABC双拼。 每种双拼对应的语法也都不一样。 自己学习的是小鹤双拼，语法图是这样的： 在搜狗输入法上点击 设置—— 属性设置 —— 常用 —— 特殊习惯—— 双拼 就可以使用了。 3. 学习的过程刚开始学习小鹤双拼的时候， 自己是完全不习惯的， 因为想要使用小鹤双拼进行文字输入，就得记住每个字母对应的韵母， 自己每输入一个字， 就得想一下这个字的韵母是什么， 对应到按键上的那个字母又是什么。 再去输入，说实话当时输入字的效率低下到令人发指，而且往往记不住，自己只好打印了一张语法表贴在自己的电脑旁， 忘了就在表上找。 好几次忍不住偷偷换成了全拼， 特别是在工作着急的情况下。 就这样别别扭扭用了一周之后， 才发现自己已经能够慢慢不看语法表了。 （这让我都有点怀疑自己的智商，因为网上说基本一周就可以很熟练了）一个月过后自己才做到了输入基本不卡壳，但如果旁边有人一紧张还是会忘掉如何输入了。 现在用了一年多， 自己已经能够无意识的使用双拼了。 如果你现在问我键盘上的字母在双拼中代表哪个韵母，自己可能真的答不上来，但只要自己在键盘上打字，自己就能够无意识的打出来。 4. 学习双拼的优点与缺点( 1 ). 优点 简洁，同样一个词全拼要五六下，双拼只需要两下 感觉节省了时间，更喜欢在键盘上敲字了。 （至于是否真正节省了时间，自己没有做过对比） ( 2 ). 缺点 全拼不会用了， 有时在别人的电脑上输入文字总是很别扭，老出错，总想着把输入法改成双拼 有时大脑短路会想不起来双拼的语法 5. 感悟 任何学习都是不可逆的，当你学了到了一项技能，你就不可能再像从前没学过一样生活。 最可怕的不是自己知道自己不知道，而是不知道自己不知道。 比如自己学习双拼，自己以前根本不知道还有双拼这么一种输入法，就更不会产生要学习这种输入法的冲动。 如何解决自己不知道自己不知道的知识，自己目前能够想到的方法是：多读书，多关注大牛，多了解别人是怎么工作、生活。 有些东西只有自己亲身经历过后才能有所体会，哪怕是坏的体验。 如果只是看别人推荐而不去坚持使用双拼， 我就不能体会到大脑下意识使用双拼输入的快感。当然，也许会出现这种情况， 你付出了时间，付出了精力，而这项技能对你的生活影响并不大。这就需要你前期做一些搜索调查。 延伸到生活上，要是我不来北京生活，不来北京工作，我就没有机会知道来北京到底会面临什么困难，到底对自己的职业发展是否有益。也许最后自己会失败，可那又怎么样，自己的人生自己做主。 学会一项技能，不是只是了解它，而是能够在生活中无意识的使用它 一项技能，只是了解是远远不够的， 你要去不断的磨练，打磨，直到它成为你大脑的一部分。 学会的标准就是：你能否不需要专门思考就能够调用它。 要学习那些你通过短时间学会,就能够一辈子用的上的知识。 6. 延伸 总是听很多的牛人说，写作是非常重要的一项技能，对一个人清晰思考问题是非常有帮助的，然而自己却迟迟没有行动，主要还是觉得自己语言词汇匮乏， 缺乏独立思考，怕自己语无伦次。 其实又想想，写作这东西这就和自己刚开始学习双拼时一样，开始你觉得自己没有可能学会，也许过一段时间你就能够发现自己的进步， 你不去坚持写又怎么能够证明自己一定学不会呢？ 自己认为学习是一个自我验证的过程：你认为自己不可能学会，你就不会坚持去学；你不坚持去学，你就不会有进步， 从而你就不会看到到自己能够学会的结果，也就证明了自己确实学不会。相反， 你认为自己能学会，你就坚持去学，看到自己的进步，最终的确学会了，也证明了自己确实能够学会。 7. 未解决问题 如何运用心智的力量在还没有机会亲身体验的情况下，仅凭心智就可以像真实经历过一样深刻体会？ 如何解决 害怕自己付出了时间，付出了精力，而没有一个好的结果 参考资料： 《把时间当朋友》]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>双拼</tag>
      </tags>
  </entry>
</search>
